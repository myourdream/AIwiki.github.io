



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="AI Wiki 是一个编程竞赛知识整合站点，提供有趣又实用的编程竞赛知识以及其他有帮助的内容，帮助广大编程竞赛爱好者更快更深入地学习编程竞赛">
      
      
        <link rel="canonical" href="https://hai5g.cn/aiwiki/lihang/CH09/">
      
      
        <meta name="author" content="AI Wiki Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>EM算法及其推广 - AI Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,400i,700|Fira+Mono">
        <style>body,input{font-family:"Fira Sans","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Fira Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
      <link rel="manifest" href="../../manifest.webmanifest">
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/ah@1.5.0/han.min.css">
    
      <link rel="stylesheet" href="../../_static/css/extra.css?v=11">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#ch09-em" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-header-nav__button md-logo">
          
            <i class="md-icon">school</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              AI Wiki
            </span>
            <span class="md-header-nav__topic">
              EM算法及其推广
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../.." title="简介" class="md-tabs__link">
          简介
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../Coursera_ML_AndrewNg/" title="机器学习" class="md-tabs__link">
          机器学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../qa500/" title="深度学习500问" class="md-tabs__link">
          深度学习500问
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../" title="统计学习" class="md-tabs__link md-tabs__link--active">
          统计学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../hands-on-ml-zh/README.old/" title="Sklearn与TensorFlow" class="md-tabs__link">
          Sklearn与TensorFlow
        </a>
      
    </li>
  

      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-nav__button md-logo">
      
        <i class="md-icon">school</i>
      
    </a>
    AI Wiki
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      简介
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        简介
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../.." title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/AI学习路线/" title="AI学习路线1" class="md-nav__link">
      AI学习路线1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/ai-union-201904/" title="AI学习路线2" class="md-nav__link">
      AI学习路线2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/v0.1/" title="AI 路线图v0.1" class="md-nav__link">
      AI 路线图v0.1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/v0.2/" title="AI 路线图v0.2" class="md-nav__link">
      AI 路线图v0.2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/v1.0/" title="ApacheCN 人工智能知识树" class="md-nav__link">
      ApacheCN 人工智能知识树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7" type="checkbox" id="nav-1-7">
    
    <label class="md-nav__link" for="nav-1-7">
      工具软件
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-1-7">
        工具软件
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/judgers/" title="评测工具" class="md-nav__link">
      评测工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/editors/" title="编辑工具" class="md-nav__link">
      编辑工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/wsl/" title="WSL (Windows 10)" class="md-nav__link">
      WSL (Windows 10)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/spj/" title="Special Judge" class="md-nav__link">
      Special Judge
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7-5" type="checkbox" id="nav-1-7-5">
    
    <label class="md-nav__link" for="nav-1-7-5">
      Testlib
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-1-7-5">
        Testlib
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/" title="Testlib 简介" class="md-nav__link">
      Testlib 简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/general/" title="通用" class="md-nav__link">
      通用
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/generator/" title="Generator" class="md-nav__link">
      Generator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/validator/" title="Validator" class="md-nav__link">
      Validator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/interactor/" title="Interactor" class="md-nav__link">
      Interactor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/checker/" title="Checker" class="md-nav__link">
      Checker
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/docker-deploy/" title="Docker 部署" class="md-nav__link">
      Docker 部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/about/" title="关于本项目" class="md-nav__link">
      关于本项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/faq/" title="F.A.Q." class="md-nav__link">
      F.A.Q.
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      机器学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        机器学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/SUMMARY/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/math/" title="数学基础" class="md-nav__link">
      数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week1/" title="week1" class="md-nav__link">
      week1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week2/" title="week2" class="md-nav__link">
      week2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week3/" title="week3" class="md-nav__link">
      week3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week4/" title="week4" class="md-nav__link">
      week4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week5/" title="week5" class="md-nav__link">
      week5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week6/" title="week6" class="md-nav__link">
      week6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week7/" title="week7" class="md-nav__link">
      week7
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week8/" title="week8" class="md-nav__link">
      week8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week9/" title="week9" class="md-nav__link">
      week9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week10/" title="week10" class="md-nav__link">
      week10
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      深度学习500问
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        深度学习500问
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/content/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch01_math/ch01_math/" title="第一章_数学基础" class="md-nav__link">
      第一章_数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch02_机器学习基础/第二章_机器学习基础/" title="第二章_机器学习基础" class="md-nav__link">
      第二章_机器学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch03_深度学习基础/第三章_深度学习基础/" title="第三章_深度学习基础" class="md-nav__link">
      第三章_深度学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch04_经典网络/第四章_经典网络/" title="第四章_经典网络" class="md-nav__link">
      第四章_经典网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch05_卷积神经网络(CNN)/第五章 卷积神经网络（CNN）/" title="第五章 卷积神经网络（CNN）" class="md-nav__link">
      第五章 卷积神经网络（CNN）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch06_循环神经网络(RNN)/第六章_循环神经网络(RNN)/" title="第六章_循环神经网络(RNN)" class="md-nav__link">
      第六章_循环神经网络(RNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch07_生成对抗网络(GAN)/ch7/" title="第七章_生成对抗网络(GAN)" class="md-nav__link">
      第七章_生成对抗网络(GAN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch08_目标检测/第八章_目标检测/" title="第八章_目标检测" class="md-nav__link">
      第八章_目标检测
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch09_图像分割/第九章_图像分割/" title="第九章_图像分割" class="md-nav__link">
      第九章_图像分割
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch10_强化学习/第十章_强化学习/" title="第十章_强化学习" class="md-nav__link">
      第十章_强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch11_迁移学习/第十一章_迁移学习/" title="第十一章_迁移学习" class="md-nav__link">
      第十一章_迁移学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch12_网络搭建及训练/第十二章_网络搭建及训练/" title="第十二章_网络搭建及训练" class="md-nav__link">
      第十二章_网络搭建及训练
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch13_优化算法/第十三章_优化算法/" title="第十三章_优化算法" class="md-nav__link">
      第十三章_优化算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch14_超参数调整/第十四章_超参数调整/" title="第十四章_超参数调整" class="md-nav__link">
      第十四章_超参数调整
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch15_GPU和框架选型/第十五章_异构运算、GPU及框架选型/" title="第十五章_异构运算、GPU及框架选型" class="md-nav__link">
      第十五章_异构运算、GPU及框架选型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch16_自然语言处理(NLP)/第十六章_NLP/" title="第十六章_NLP" class="md-nav__link">
      第十六章_NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署/" title="第十七章_模型压缩、加速及移动端部署" class="md-nav__link">
      第十七章_模型压缩、加速及移动端部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch18_后端架构选型、离线及实时计算/第十八章_后端架构选型、离线及实时计算/" title="第十八章_后端架构选型、离线及实时计算" class="md-nav__link">
      第十八章_后端架构选型、离线及实时计算
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch18_后端架构选型及应用场景/第十八章_后端架构选型及应用场景/" title="第十八章_后端架构选型及应用场景" class="md-nav__link">
      第十八章_后端架构选型及应用场景
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      统计学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        统计学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH01/" title="统计学习及监督学习概论" class="md-nav__link">
      统计学习及监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH02/" title="感知机" class="md-nav__link">
      感知机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH03/" title="K近邻法" class="md-nav__link">
      K近邻法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH04/" title="朴素贝叶斯法" class="md-nav__link">
      朴素贝叶斯法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH05/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH06/" title="逻辑斯蒂回归与最大熵模型" class="md-nav__link">
      逻辑斯蒂回归与最大熵模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH07/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH08/" title="提升方法" class="md-nav__link">
      提升方法
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        EM算法及其推广
      </label>
    
    <a href="./" title="EM算法及其推广" class="md-nav__link md-nav__link--active">
      EM算法及其推广
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="前言" class="md-nav__link">
    前言
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" title="章节目录" class="md-nav__link">
    章节目录
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="导读" class="md-nav__link">
    导读
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="符号说明" class="md-nav__link">
    符号说明
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" title="混合模型" class="md-nav__link">
    混合模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" title="伯努利混合模型(三硬币模型)" class="md-nav__link">
    伯努利混合模型(三硬币模型)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" title="问题描述" class="md-nav__link">
    问题描述
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#em" title="三硬币模型的EM算法" class="md-nav__link">
    三硬币模型的EM算法
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" title="1.初值" class="md-nav__link">
    1.初值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2e" title="2.E步" class="md-nav__link">
    2.E步
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3m" title="3.M步" class="md-nav__link">
    3.M步
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" title="初值影响" class="md-nav__link">
    初值影响
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pq" title="p,q 含义" class="md-nav__link">
    p,q 含义
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#em_1" title="EM算法另外视角" class="md-nav__link">
    EM算法另外视角
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q" title="Q 函数" class="md-nav__link">
    Q 函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bmmem" title="BMM的EM算法" class="md-nav__link">
    BMM的EM算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l" title="目标函数L" class="md-nav__link">
    目标函数L
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#em_2" title="EM算法导出" class="md-nav__link">
    EM算法导出
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" title="高斯混合模型" class="md-nav__link">
    高斯混合模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gmm" title="GMM的图模型" class="md-nav__link">
    GMM的图模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gmmem" title="GMM的EM算法" class="md-nav__link">
    GMM的EM算法
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" title="1. 明确隐变量, 初值" class="md-nav__link">
    1. 明确隐变量, 初值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-eq" title="2. E步,确定Q函数" class="md-nav__link">
    2. E步,确定Q函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-m" title="3. M步" class="md-nav__link">
    3. M步
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" title="4. 停止条件" class="md-nav__link">
    4. 停止条件
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92" title="算法9.2" class="md-nav__link">
    算法9.2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gmmcv" title="GMM在CV中的应用" class="md-nav__link">
    GMM在CV中的应用
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kmeans" title="Kmeans" class="md-nav__link">
    Kmeans
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#k" title="K怎么定" class="md-nav__link">
    K怎么定
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="广义期望极大" class="md-nav__link">
    广义期望极大
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" title="其他" class="md-nav__link">
    其他
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#93" title="习题9.3" class="md-nav__link">
    习题9.3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#94" title="习题9.4" class="md-nav__link">
    习题9.4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" title="参考" class="md-nav__link">
    参考
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH10/" title="隐马尔可夫模型" class="md-nav__link">
      隐马尔可夫模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH11/" title="条件随机场" class="md-nav__link">
      条件随机场
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH12/" title="监督学习方法总结" class="md-nav__link">
      监督学习方法总结
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH13/" title="无监督学习概论" class="md-nav__link">
      无监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH14/" title="聚类方法" class="md-nav__link">
      聚类方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH22/" title="无监督学习方法总结" class="md-nav__link">
      无监督学习方法总结
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Sklearn与TensorFlow
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Sklearn与TensorFlow
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/README.old/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/0.前言/" title="前言" class="md-nav__link">
      前言
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/1.机器学习概览/" title="机器学习概览" class="md-nav__link">
      机器学习概览
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/2.一个完整的机器学习项目/" title="一个完整的机器学习项目" class="md-nav__link">
      一个完整的机器学习项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/3.分类/" title="分类" class="md-nav__link">
      分类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/4.训练模型/" title="训练模型" class="md-nav__link">
      训练模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/5.支持向量机/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/6.决策树/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/7.集成学习和随机森林/" title="集成学习和随机森林" class="md-nav__link">
      集成学习和随机森林
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/9.启动并运行_TensorFlow/" title="启动并运行_TensorFlow" class="md-nav__link">
      启动并运行_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/10.人工神经网络介绍/" title="人工神经网络介绍" class="md-nav__link">
      人工神经网络介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/11.训练深层神经网络/" title="训练深层神经网络" class="md-nav__link">
      训练深层神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/12.设备和服务器上的分布式_TensorFlow/" title="设备和服务器上的分布式_TensorFlow" class="md-nav__link">
      设备和服务器上的分布式_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/13.卷积神经网络/" title="卷积神经网络" class="md-nav__link">
      卷积神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/14.循环神经网络/" title="循环神经网络" class="md-nav__link">
      循环神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/15.自编码器/" title="自编码器" class="md-nav__link">
      自编码器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/16.强化学习/" title="强化学习" class="md-nav__link">
      强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/B.机器学习项目清单/" title="机器学习项目清单" class="md-nav__link">
      机器学习项目清单
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/C.SVM_对偶问题/" title="SVM_对偶问题" class="md-nav__link">
      SVM_对偶问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/D.自动微分/" title="自动微分" class="md-nav__link">
      自动微分
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../about/" title="关于" class="md-nav__link">
      关于
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="前言" class="md-nav__link">
    前言
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" title="章节目录" class="md-nav__link">
    章节目录
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="导读" class="md-nav__link">
    导读
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="符号说明" class="md-nav__link">
    符号说明
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" title="混合模型" class="md-nav__link">
    混合模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" title="伯努利混合模型(三硬币模型)" class="md-nav__link">
    伯努利混合模型(三硬币模型)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" title="问题描述" class="md-nav__link">
    问题描述
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#em" title="三硬币模型的EM算法" class="md-nav__link">
    三硬币模型的EM算法
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" title="1.初值" class="md-nav__link">
    1.初值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2e" title="2.E步" class="md-nav__link">
    2.E步
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3m" title="3.M步" class="md-nav__link">
    3.M步
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" title="初值影响" class="md-nav__link">
    初值影响
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pq" title="p,q 含义" class="md-nav__link">
    p,q 含义
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#em_1" title="EM算法另外视角" class="md-nav__link">
    EM算法另外视角
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q" title="Q 函数" class="md-nav__link">
    Q 函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bmmem" title="BMM的EM算法" class="md-nav__link">
    BMM的EM算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l" title="目标函数L" class="md-nav__link">
    目标函数L
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#em_2" title="EM算法导出" class="md-nav__link">
    EM算法导出
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" title="高斯混合模型" class="md-nav__link">
    高斯混合模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gmm" title="GMM的图模型" class="md-nav__link">
    GMM的图模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gmmem" title="GMM的EM算法" class="md-nav__link">
    GMM的EM算法
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" title="1. 明确隐变量, 初值" class="md-nav__link">
    1. 明确隐变量, 初值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-eq" title="2. E步,确定Q函数" class="md-nav__link">
    2. E步,确定Q函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-m" title="3. M步" class="md-nav__link">
    3. M步
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" title="4. 停止条件" class="md-nav__link">
    4. 停止条件
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92" title="算法9.2" class="md-nav__link">
    算法9.2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gmmcv" title="GMM在CV中的应用" class="md-nav__link">
    GMM在CV中的应用
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kmeans" title="Kmeans" class="md-nav__link">
    Kmeans
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#k" title="K怎么定" class="md-nav__link">
    K怎么定
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="广义期望极大" class="md-nav__link">
    广义期望极大
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" title="其他" class="md-nav__link">
    其他
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#93" title="习题9.3" class="md-nav__link">
    习题9.3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#94" title="习题9.4" class="md-nav__link">
    习题9.4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" title="参考" class="md-nav__link">
    参考
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/myourdream/aiwiki/blob/master/docs/lihang/CH09/README.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="ch09-em">CH09 EM算法及其推广<a class="headerlink" href="#ch09-em" title="Permanent link">&para;</a></h1>
<div class="toc">
<ul>
<li><a href="#ch09-em">CH09 EM算法及其推广</a><ul>
<li><a href="#_1">前言</a><ul>
<li><a href="#_2">章节目录</a></li>
<li><a href="#_3">导读</a></li>
<li><a href="#_4">符号说明</a></li>
</ul>
</li>
<li><a href="#_5">混合模型</a><ul>
<li><a href="#_6">伯努利混合模型(三硬币模型)</a><ul>
<li><a href="#_7">问题描述</a></li>
<li><a href="#em">三硬币模型的EM算法</a><ul>
<li><a href="#1">1.初值</a></li>
<li><a href="#2e">2.E步</a></li>
<li><a href="#3m">3.M步</a></li>
<li><a href="#_8">初值影响</a></li>
<li><a href="#pq">p,q 含义</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#em_1">EM算法另外视角</a><ul>
<li><a href="#q">Q 函数</a></li>
<li><a href="#bmmem">BMM的EM算法</a></li>
<li><a href="#l">目标函数L</a></li>
<li><a href="#em_2">EM算法导出</a></li>
</ul>
</li>
<li><a href="#_9">高斯混合模型</a><ul>
<li><a href="#gmm">GMM的图模型</a></li>
<li><a href="#gmmem">GMM的EM算法</a><ul>
<li><a href="#1_1">1. 明确隐变量, 初值</a></li>
<li><a href="#2-eq">2. E步,确定Q函数</a></li>
<li><a href="#3-m">3. M步</a></li>
<li><a href="#4">4. 停止条件</a></li>
</ul>
</li>
<li><a href="#92">算法9.2</a></li>
<li><a href="#gmmcv">GMM在CV中的应用</a></li>
</ul>
</li>
<li><a href="#kmeans">Kmeans</a><ul>
<li><a href="#k">K怎么定</a></li>
</ul>
</li>
<li><a href="#_10">广义期望极大</a></li>
</ul>
</li>
<li><a href="#_11">其他</a><ul>
<li><a href="#93">习题9.3</a></li>
<li><a href="#94">习题9.4</a></li>
</ul>
</li>
<li><a href="#_12">参考</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="_1">前言<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<h3 id="_2">章节目录<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<ol>
<li>EM算法的引入</li>
<li>EM算法</li>
<li>EM算法的导出</li>
<li>EM算法在非监督学习中的应用</li>
<li>EM算法的收敛性</li>
<li>EM算法在高斯混合模型学习中的应用</li>
<li>高斯混合模型</li>
<li>高斯混合模型参数估计的EM算法</li>
<li>EM算法的推广</li>
<li>F函数的极大极大算法</li>
</ol>
<h3 id="_3">导读<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>概率模型</strong>有时既含有观测变量，又含有隐变量或潜在变量。这句很重要。</p>
</li>
<li>
<p>这章如果看三硬币有疑问，可以往后继续看，看到高斯混合模型，然后再回头理解三硬币。有不理解的地方，可以重新看对应问题的定义，重新理解各个符号的意义，因为这章开始，需要分析的问题和之前的分类问题有差异，任务不同了要理解需求。希望对学习有帮助。</p>
</li>
<li>
<p>EM算法可以用于<strong>生成模型</strong>的非监督学习，EM算法是个一般方法，不具有具体模型。</p>
</li>
</ul>
<blockquote>
<p>EM算法是一种迭代算法，用于含有隐变量的概率模型的极大似然估计，或极大后验概率估计。</p>
</blockquote>
<p>本书<a href="../CH12/">CH12</a>在对比各种模型的策略的时候，从这章开始，学习策略都是MLE，损失函数都是对数似然损失。体现了这一类问题的共性与联系。</p>
<ul>
<li>
<p>这里面注意体会不同变量的大小以及对应的取值范围。</p>
</li>
<li>
<p>一个<span><span class="MathJax_Preview">m\times n\times k</span><script type="math/tex">m\times n\times k</script></span>的矩阵可能可以划分成<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个<span><span class="MathJax_Preview">m\times k</span><script type="math/tex">m\times k</script></span>的形式，这点理解下。</p>
</li>
<li>
<p>涉及混合模型的部分推导有很多求和，注意体会是按照<strong>样本</strong>做的，还是按照<strong>模型</strong>做的，也就是操作的域</p>
</li>
<li>
<p>如果对PDF，高斯分布，边缘概率分布，协方差矩阵不清楚，可以在这个章节从GMM的角度扩展阅读下，一定会有收获。</p>
</li>
<li>
<p>似然和概率的关系可以推广了解，这章关于概率和似然的符号表示，可能会有点看不懂，比如<span><span class="MathJax_Preview">P_{157}</span><script type="math/tex">P_{157}</script></span>中的部分表述。可以参考引用内容[^3], 概率和似然是同样的形式描述的都是<strong>可能性</strong>，<span><span class="MathJax_Preview">P(Y|\theta)</span><script type="math/tex">P(Y|\theta)</script></span>是一个两变量的函数，似然是给定结果，求参数可能性；概率是给定参数求结果可能性。</p>
</li>
</ul>
<blockquote>
<p>Suppose you have a probability model with parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.
<span><span class="MathJax_Preview">p(x|\theta)</span><script type="math/tex">p(x|\theta)</script></span> has two names.
It can be called the <strong>probability of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></strong> (given <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>),
or the <strong>likelihood of <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></strong> (given that <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>  was observed).</p>
</blockquote>
<ul>
<li>
<p>学习过程中注意<strong>观测数据</strong>在EM算法每次迭代中的意义。</p>
</li>
<li>
<p>GMM中注意区分<span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>和<span><span class="MathJax_Preview">\gamma_{jk}</span><script type="math/tex">\gamma_{jk}</script></span>的差异，直觉上都有一种归属的感觉，<span><span class="MathJax_Preview">\gamma_{jk}</span><script type="math/tex">\gamma_{jk}</script></span>是二值函数，<span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>是一种概率的表示。<span><span class="MathJax_Preview">\gamma_{jk}</span><script type="math/tex">\gamma_{jk}</script></span>是one-hot encoding(also: 1-of-K representation)，还有<span><span class="MathJax_Preview">\hat\gamma_{jk}</span><script type="math/tex">\hat\gamma_{jk}</script></span>这个是个估计注意和<span><span class="MathJax_Preview">\gamma_{jk}</span><script type="math/tex">\gamma_{jk}</script></span>的关系</p>
</li>
<li>
<p>GMM这里面实际上还涉及到一个概念叫做凸组合(Convex Combination)[^4]，是凸几何领域的一个概念，点的线性组合，所有系数都非负且和为1。点集的凸包等价于该点集的凸组合。</p>
</li>
<li>
<p>无论是三硬币还是GMM，采样的过程都是如下:</p>
</li>
</ul>
<blockquote>
<ol>
<li>Sample <span><span class="MathJax_Preview">z_i \sim p(z|\pi)</span><script type="math/tex">z_i \sim p(z|\pi)</script></span></li>
<li>Sample <span><span class="MathJax_Preview">x_i \sim p(x|\pi)</span><script type="math/tex">x_i \sim p(x|\pi)</script></span></li>
</ol>
</blockquote>
<p>注意，这里用到了<span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>，在强化学习中，随机性策略<span><span class="MathJax_Preview">\pi(x,a)</span><script type="math/tex">\pi(x,a)</script></span>表示为状态<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>下选择动作<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>的概率。</p>
<ul>
<li>
<p>关于EM算法的解释
  注意这里EM不是模型，是个一般方法，不具有具体的模型，这点前面已经提到</p>
</li>
<li>
<p>PRML
     <span><span class="MathJax_Preview">kmeans \rightarrow GMM \rightarrow EM</span><script type="math/tex">kmeans \rightarrow GMM \rightarrow EM</script></span>
     所以，EM应用举例子为kmeans也OK。而且，西瓜书<span><span class="MathJax_Preview">P_{165}</span><script type="math/tex">P_{165}</script></span>上有说， <code>k均值聚类算法就是一个典型的EM算法</code></p>
</li>
<li>
<p>统计学习方法</p>
<ol>
<li><span><span class="MathJax_Preview">MLE \rightarrow B</span><script type="math/tex">MLE \rightarrow B</script></span></li>
<li><span><span class="MathJax_Preview">F</span><script type="math/tex">F</script></span>函数的极大-极大算法</li>
</ol>
</li>
<li>
<p>这个repo里面实现了BMM算法和GMM算法两种混合模型</p>
</li>
<li>
<p>HMM也是Discrete <strong>Dynamic Model</strong>，从图模型角度考虑，可以发现HMM和卡尔曼滤波以及粒子滤波深层之间的联系。这部分内容在PRML中有讨论。</p>
</li>
<li>
<p>书中图9.1说一下，可以参考<a href="../CH08/">CH08</a>的部分内容，关于Bregman distance的那部分说明。</p>
</li>
<li>
<p>HMM作了两个基本假设，实际上是在说在图模型中，存在哪些<strong>边</strong>。</p>
</li>
</ul>
<h3 id="_4">符号说明<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<blockquote>
<p>一般地，用<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>表示观测随机变量的数据，<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>表示隐随机变量的数据。<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>和<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>一起称为<strong>完全数据</strong>(complete-data)，观测数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>又称为<strong>不完全数据</strong>(incomplete-data)</p>
</blockquote>
<p>上面这个概念很重要，Dempster在1977年提出EM算法的时候文章题目就是《Maximum likelihood from incomplete data via the EM algorithm》，具体看书中本章参考文献[^3]</p>
<blockquote>
<p>假设给定观测数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>，其概率分布是<span><span class="MathJax_Preview">P(Y|\theta)</span><script type="math/tex">P(Y|\theta)</script></span>，其中<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>是需要估计的模型参数
那么不完全数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>的似然函数是<span><span class="MathJax_Preview">P(Y|\theta)</span><script type="math/tex">P(Y|\theta)</script></span>，对数似然函数是<span><span class="MathJax_Preview">L(\theta)=\log P(Y|\theta)</span><script type="math/tex">L(\theta)=\log P(Y|\theta)</script></span></p>
<p>假设<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>和<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>的联合概率分布是<span><span class="MathJax_Preview">P(Y,Z|\theta)</span><script type="math/tex">P(Y,Z|\theta)</script></span>，那么完全数据的对数似然函数是<span><span class="MathJax_Preview">\log P(Y,Z|\theta)</span><script type="math/tex">\log P(Y,Z|\theta)</script></span></p>
</blockquote>
<p>上面这部分简单对应一下，这里说明一下，你看到下面概率分布和似然函数形式看起来一样。在概率中，<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>已知, 求<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>，在似然函数中通过已知的Y去求<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>观测数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span></td>
<td>不完全数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span></td>
<td></td>
</tr>
<tr>
<td>不完全数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span></td>
<td>概率分布$P(Y</td>
<td>\theta)$</td>
<td>似然函数$P(Y</td>
</tr>
<tr>
<td>完全数据 <span><span class="MathJax_Preview">(Y, Z)</span><script type="math/tex">(Y, Z)</script></span></td>
<td><span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>和<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>的联合概率分布$P(Y,Z</td>
<td>\theta )$</td>
<td>似然函数$P(Y,Z</td>
</tr>
</tbody>
</table>
<p>7hb 观测数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span></p>
<p>有一点要注意下, 这里没有出现<span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>, 在<strong>9.1.3</strong>节中有提到一种理解</p>
<blockquote>
<ul>
<li>有时训练数据只有输入没有对应的输出<span><span class="MathJax_Preview">{(x_1,\cdot),(x_2,\cdot),\dots,(x_N,\cdot)}</span><script type="math/tex">{(x_1,\cdot),(x_2,\cdot),\dots,(x_N,\cdot)}</script></span>，从这样的数据学习模型称为非监督学习问题。</li>
<li>EM算法可以用于生成模型的非监督学习。</li>
<li>生成模型由联合概率分布<span><span class="MathJax_Preview">P(X,Y)</span><script type="math/tex">P(X,Y)</script></span>表示，可以认为非监督学习训练数据是联合概率分布产生的数据。<span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>为观测数据， <span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>为未观测数据。</li>
</ul>
</blockquote>
<p>有时候，只观测显变量看不到关系，就需要把隐变量引进来。</p>
<h2 id="_5">混合模型<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h2>
<p>书中用三硬币模型做为引子，在学习这部分内容的时候，注意体会观测数据的作用。</p>
<h3 id="_6">伯努利混合模型(三硬币模型)<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<h4 id="_7">问题描述<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<p>书中用例子来介绍EM算法的问题，并给出了EM算法迭代求解的过程，具体例子描述见<strong>例9.1</strong>，这块如果不懂，可以跳过，看完后面高斯混合模型再回来看。</p>
<p>问题的描述过程中有这样一句：独立的重复<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>次实验(这里<span><span class="MathJax_Preview">n=10</span><script type="math/tex">n=10</script></span>)，观测结果如下:</p>
<p><code>1,1,0,1,0,0,1,0,1,1</code></p>
<p>思考上面这个观测和<code>1,1,1,1,1,1,0,0,0,0</code>有区别么?</p>
<p>没有任何信息的前提下，我们得到上面的观测数据可以假定是一个<strong>二项分布</strong>的形式，参数<span><span class="MathJax_Preview">n=10, p=0.6</span><script type="math/tex">n=10, p=0.6</script></span></p>
<p>把<span><span class="MathJax_Preview">k=6</span><script type="math/tex">k=6</script></span>次成功分布在<span><span class="MathJax_Preview">n=10</span><script type="math/tex">n=10</script></span>次试验中有<span><span class="MathJax_Preview">C(10,6)</span><script type="math/tex">C(10,6)</script></span>种可能.</p>
<p>所以上面两个观测序列，可能出自同一个模型。在这个问题的求解上是没有区别的，测试案例<span><span class="MathJax_Preview">test\_t91</span><script type="math/tex">test\_t91</script></span>做了这个说明，可以参考。</p>
<p>我们通过一段代码来生成这个数据</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="nn">Engine</span>

<span class="n">p</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># np.random.seed(2018)</span>
<span class="n">flag_a</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">flag_b</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">flag_a</span> <span class="ow">or</span> <span class="n">flag_b</span><span class="p">:</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tmp</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="n">flag_a</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;[1,1,1,1,1,1,0,0,0,0] at </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">cnt</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tmp</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="n">flag_b</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;[1,1,0,1,0,0,1,0,1,1] at </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">cnt</span><span class="p">)</span>
    <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</td></tr></table>

<p>实际上题目的描述中说明了观测数据生成的过程，这些参数是未知的，所以需要对这些参数进行估计。</p>
<p>解的过程记录在这里。</p>
<p>三硬币模型可以写作
$$
\begin{equation}
        \begin{aligned}
            P(y|\theta)&amp;=\sum_z P(y,z|\theta) \
            &amp;=\sum_z P(z|\theta)P(y|z,\theta) \
            &amp;=\pi p^y (1-p)^{1-y} + (1-\pi)q<sup>y(1-q)</sup>{1-y}
        \end{aligned}
\end{equation}
$$
以上</p>
<ol>
<li>随机变量<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>是观测变量，表示一次试验观测的结果是<strong>1或0</strong></li>
<li>随机变量<span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>是隐变量，表示未观测到的掷硬币<span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>的结果</li>
<li><span><span class="MathJax_Preview">\theta=(\pi,p,q)</span><script type="math/tex">\theta=(\pi,p,q)</script></span>是模型参数</li>
<li>这个模型是<strong>以上数据</strong>(1,1,0,1,0,0,1,0,1,1)的生成模型</li>
</ol>
<p>观测数据表示为<span><span class="MathJax_Preview">Y=(Y_1, Y_2, Y_3, \dots, Y_n)^T</span><script type="math/tex">Y=(Y_1, Y_2, Y_3, \dots, Y_n)^T</script></span>, 未观测数据表示为<span><span class="MathJax_Preview">Z=(Z_1,Z_2, Z_3,\dots, Z_n)^T</span><script type="math/tex">Z=(Z_1,Z_2, Z_3,\dots, Z_n)^T</script></span>, 则观测数据的似然函数为</p>
<blockquote>
<p>其实觉得这里应该是小写的<span><span class="MathJax_Preview">y=(y_1,y_2,\dots,y_n), z=(z_1, z_2, \dots,z_n)</span><script type="math/tex">y=(y_1,y_2,\dots,y_n), z=(z_1, z_2, \dots,z_n)</script></span>
$$
P(Y|\theta) = \sum\limits_{Z}P(Z|\theta)P(Y|Z,\theta)
$$
注意这里的求和是下面的"+"描述的部分</p>
</blockquote>
<p>即
$$
P(Y|\theta)=\prod\limits^{n}_{j=1}[\pi p<sup>{y_j}(1-p)</sup>{1-y_j}+(1-\pi)q<sup>{y_j}(1-q)</sup>{1-y_j}]
$$
注意这里连乘是<span><span class="MathJax_Preview">Y\rightarrow y_j</span><script type="math/tex">Y\rightarrow y_j</script></span>出来的, 不理解看似然定义.</p>
<p>考虑求模型参数<span><span class="MathJax_Preview">\theta=(\pi,p,q)</span><script type="math/tex">\theta=(\pi,p,q)</script></span>的极大似然估计, 即
$$
\hat \theta = \arg\max\limits_{\theta}\log P(Y|\theta)
$$
这个题目的标准答案实际上也是未知的，因为可能生成这样的观测的假设空间太大。</p>
<h4 id="em">三硬币模型的EM算法<a class="headerlink" href="#em" title="Permanent link">&para;</a></h4>
<h5 id="1">1.初值<a class="headerlink" href="#1" title="Permanent link">&para;</a></h5>
<p>EM算法首选参数初值，记作<span><span class="MathJax_Preview">\theta^{(0)}=(\pi^{(0)},p^{(0)}, q^{(0)})</span><script type="math/tex">\theta^{(0)}=(\pi^{(0)},p^{(0)}, q^{(0)})</script></span>, 然后迭代计算参数的估计值。</p>
<p>如果第<span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>次迭代的模型参数估计值为<span><span class="MathJax_Preview">\theta^{(i)}=(\pi^{(i)}, p^{(i)}, q^{(i)})</span><script type="math/tex">\theta^{(i)}=(\pi^{(i)}, p^{(i)}, q^{(i)})</script></span></p>
<h5 id="2e">2.E步<a class="headerlink" href="#2e" title="Permanent link">&para;</a></h5>
<p>那么第<span><span class="MathJax_Preview">i+1</span><script type="math/tex">i+1</script></span> 次迭代的模型参数估计值表示为
$$
\mu_j<sup>{i+1} = \frac{\pi</sup>{(i)}(p<sup>{(i)})</sup>{y_j}(1-p<sup>{(i)})</sup>{1-y_j}}{\pi<sup>{(i)}(p</sup>{(i)})<sup>{y_j}(1-p</sup>{(i)})^{1-y_j} + (1-\pi<sup>{(i)})(q</sup>{(i)})<sup>{y_j}(1-q</sup>{(i)})^{1-y_j}}
$$
因为是硬币，只有0，1两种可能，所有有上面的表达。</p>
<p>这个表达方式还可以拆成如下形式
$$
\mu_j^{i+1} =
\begin{cases}
\frac{\pi<sup>{(i)}p</sup>{(i)}}{\pi<sup>{(i)}p</sup>{(i)} + (1-\pi<sup>{(i)})q</sup>{(i)}}&amp;, y_j = 1\
\frac{\pi<sup>{(i)}(1-p</sup>{(i)})}{\pi<sup>{(i)}(1-p</sup>{(i)}) + (1-\pi<sup>{(i)})(1-q</sup>{(i)})}&amp;, y_j = 0\
\end{cases}
$$
所以, 这步(求<span><span class="MathJax_Preview">\mu_j</span><script type="math/tex">\mu_j</script></span>)干了什么，样本起到了什么作用？</p>
<p>这一步，通过假设的参数，计算了不同的样本对假设模型的响应(<span><span class="MathJax_Preview">\mu_j</span><script type="math/tex">\mu_j</script></span>)，注意这里因为样本(<span><span class="MathJax_Preview">y_j</span><script type="math/tex">y_j</script></span>)是二值的，所以，用<span><span class="MathJax_Preview">\{y_j, 1-y_j\}</span><script type="math/tex">\{y_j, 1-y_j\}</script></span> 构成了one-hot的编码，用来表示样本归属的假设。</p>
<p>以上，有点绕。</p>
<p>这一步是什么的期望？书中有写，<strong>观测数据来自硬币<span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>的概率, 在二项分布的情况下, 响应度和概率是一个概念. </strong>这个说明，有助于后面M步公式的理解。</p>
<h5 id="3m">3.M步<a class="headerlink" href="#3m" title="Permanent link">&para;</a></h5>
<p>$$
\begin{align}
\pi^{(i+1)} &amp;= \frac{1}{n}\sum_{j=1}<sup>{n}\mu_j</sup>{(i+1)}\
\color{red}
p^{(i+1)} &amp;= \frac{\sum_{j=1}<sup>{n}\mu_j</sup>{(i+1)}y_j}{\sum_{j=1}<sup>{n}\mu_j</sup>{(i+1)}}\
\color{red}
q^{(i+1)} &amp;= \frac{\sum_{j=1}<sup>{n}(1-\mu_j</sup>{(i+1)})y_j}{\sum_{j=1}<sup>{n}(1-\mu_j</sup>{(i+1)})}
\end{align}
$$
上面，红色部分的公式从<code>观测数据是来自硬币B的概率</code>这句来理解。</p>
<h5 id="_8">初值影响<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h5>
<p>这个例子里面0.5是个合理又牛逼的初值。迭代收敛的最后结果是(0.5, 0.6, 0.6)</p>
<p>这个结果说明，如果A是均匀的，那么一个合理的解就是B，C是同质的。他们的分布情况和观测的分布一致。</p>
<p>在测试案例<span><span class="MathJax_Preview">test\_e91</span><script type="math/tex">test\_e91</script></span>中有计算这部分的结果，注意看，这种简单的模型其实收敛的很快。</p>
<h5 id="pq">p,q 含义<a class="headerlink" href="#pq" title="Permanent link">&para;</a></h5>
<p>这里面p对应了A =1，B=1，q对应了A=0，C=1</p>
<p>这三个公式可以改写成如下形式:
$$
\begin{align}
\pi^{(i+1)} &amp;= \frac{1}{n}\sum_{j=1}<sup>{n}\mu_j</sup>{(i+1)}\
\color{red}
p^{(i+1)} &amp;= \frac{\sum_{j=1}<sup>{n}\mu_j</sup>{(i+1)}y_j}{\sum_{j=1}<sup>{n}(\mu_j</sup>{(i+1)}y_j+\mu_j<sup>{(i+1)}(1-y_j)}\
\color{red}
q</sup>{(i+1)} &amp;= \frac{\sum_{j=1}<sup>{n}(1-\mu_j</sup>{(i+1)})y_j}{\sum_{j=1}<sup>{n}((1-\mu_j</sup>{(i+1)})y_j+(1-\mu_j^{(i+1)})(1-y_j))}
\end{align}
$$
<span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>的表达式回答这样一个问题:  刷了这么多样本，拿到一堆数，那么<span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>应该是多少，均值十个比较好的选择。</p>
<p><span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>的表达式回答这样一个问题:  如果我知道每个结果<span><span class="MathJax_Preview">y_j</span><script type="math/tex">y_j</script></span>以<span><span class="MathJax_Preview">\mu_j</span><script type="math/tex">\mu_j</script></span>的可能来自硬币B(A=1)，那么用这些数据刷出来他可能是正面的概率。这里面<span><span class="MathJax_Preview">\mu_j</span><script type="math/tex">\mu_j</script></span>对应了<span><span class="MathJax_Preview">A=1</span><script type="math/tex">A=1</script></span></p>
<p><span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span>的表达式同理，其中<span><span class="MathJax_Preview">1-\mu_j</span><script type="math/tex">1-\mu_j</script></span>对应了<span><span class="MathJax_Preview">A=0</span><script type="math/tex">A=0</script></span></p>
<p>到后面讲高斯混合模型的时候，可以重新审视这里
$$
\begin{aligned}
\alpha_0&amp; \leftrightarrow \pi \
\mu_0&amp; \leftrightarrow p<sup>{y_j}(1-p)</sup>{1-y_j}\
\alpha_1&amp; \leftrightarrow 1-\pi\
\mu_1&amp; \leftrightarrow q<sup>{y_j}(1-q)</sup>{1-y_j}
\end{aligned}
$$
以上对应了包含两个分量的伯努利混合模型, BMM, 包含四个参数, 因为<span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>满足等式约束, 所以通常会有三个参数, 另外参见习题<span><span class="MathJax_Preview">9.3</span><script type="math/tex">9.3</script></span>中有提到<code>两个分量的高斯混合模型的五个参数</code>实际上也是因为等式约束.</p>
<p><a href="bmm.py">bmm.py</a>对伯努利混合模型做了实现, 有几点说明一下:</p>
<ol>
<li>
<p><span><span class="MathJax_Preview">(p^{(i)})^{y_i}(1-p^{(i)})^{1-y_i}</span><script type="math/tex">(p^{(i)})^{y_i}(1-p^{(i)})^{1-y_i}</script></span>这个表达式对应了伯努利分布的概率密度, 可以表示成矩阵乘法, 尽量不要用for, 效率会差</p>
</li>
<li>
<p>书中<span><span class="MathJax_Preview">e_{91}</span><script type="math/tex">e_{91}</script></span>的表达中, 采用了<span><span class="MathJax_Preview">\pi, p, q</span><script type="math/tex">\pi, p, q</script></span>来表示, 注意在题目的说明部分有说明三个符号的含义</p>
</li>
<li>
<p>实际上不怎么抛硬币, 但是0-1的伯努利分布很多, 在书中算法9.4部分, 有这样一个说明:</p>
</li>
</ol>
<blockquote>
<p>当参数<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>的维数为<span><span class="MathJax_Preview">d(d\ge2 )</span><script type="math/tex">d(d\ge2 )</script></span>的时候, 可以采用一种特殊的GEM算法, 它将算法的M步分解成d次条件极大化, 每次只改变参数向量的一个分量,其余量不改变.</p>
</blockquote>
<h3 id="em_1">EM算法另外视角<a class="headerlink" href="#em_1" title="Permanent link">&para;</a></h3>
<blockquote>
<p>输入: 观测变量数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>，隐变量数据<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>，联合分布<span><span class="MathJax_Preview">P(Y,Z|\theta)</span><script type="math/tex">P(Y,Z|\theta)</script></span>，条件分布<span><span class="MathJax_Preview">P(Z|Y,\theta)</span><script type="math/tex">P(Z|Y,\theta)</script></span></p>
<p>输出: 模型参数<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></p>
<ol>
<li>
<p>选择参数的初值<span><span class="MathJax_Preview">\theta^{(0)}</span><script type="math/tex">\theta^{(0)}</script></span>，开始迭代</p>
</li>
<li>
<p>E步：记<span><span class="MathJax_Preview">\theta^{(i)}</span><script type="math/tex">\theta^{(i)}</script></span>为第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 次迭代参数<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>的估计值，在第<span><span class="MathJax_Preview">i+1</span><script type="math/tex">i+1</script></span>次迭代的<span><span class="MathJax_Preview">E</span><script type="math/tex">E</script></span>步，计算
$$
\begin{align}
Q(\theta, \theta^{(i)}) =&amp; E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]\
=&amp;\sum_Z\color{red}\log P(Y,Z|\theta)\color{green}P(Z|Y, \theta^{(i)})
\end{align}
$$</p>
</li>
<li>
<p>M步
   求使<span><span class="MathJax_Preview">Q(\theta, \theta^{(i)})</span><script type="math/tex">Q(\theta, \theta^{(i)})</script></span>最大化的<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>，确定第<span><span class="MathJax_Preview">i+1</span><script type="math/tex">i+1</script></span>次迭代的参数估计值</p>
</li>
</ol>
<div>
<div class="MathJax_Preview">
\theta^{(i+1)}=\arg\max_\theta Q(\theta, \theta^{(i)})
</div>
<script type="math/tex; mode=display">
\theta^{(i+1)}=\arg\max_\theta Q(\theta, \theta^{(i)})
</script>
</div>
</blockquote>
<h4 id="q">Q 函数<a class="headerlink" href="#q" title="Permanent link">&para;</a></h4>
<p>注意Q函数的定义，可以帮助理解上面E步中的求和表达式</p>
<p>完全数据的<strong>对数似然函数<span><span class="MathJax_Preview">\log P(Y, Z|\theta)</span><script type="math/tex">\log P(Y, Z|\theta)</script></span>关于</strong>给定观测数据<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>的当前参数<span><span class="MathJax_Preview">\theta^{(i)}</span><script type="math/tex">\theta^{(i)}</script></span>下对为观测数据<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>的<strong>条件概率分布<span><span class="MathJax_Preview">P(Z|Y,\theta^{(i)})</span><script type="math/tex">P(Z|Y,\theta^{(i)})</script></span>的期望</strong>称为Q函数。</p>
<h4 id="bmmem">BMM的EM算法<a class="headerlink" href="#bmmem" title="Permanent link">&para;</a></h4>
<blockquote>
<p>输入: 观测变量数据<span><span class="MathJax_Preview">y_1, y_2, \dots, y_N</span><script type="math/tex">y_1, y_2, \dots, y_N</script></span>, 伯努利混合模型</p>
<p>输出: 伯努利混合模型参数</p>
<ol>
<li>选择参数的初始值开始迭代, <span><span class="MathJax_Preview">2K</span><script type="math/tex">2K</script></span> 个参数</li>
<li>E步:
<span><span class="MathJax_Preview"><span><span class="MathJax_Preview">\hat\gamma_{jk}=\frac{\alpha_kBern(y_j|\theta_k)}{\sum_{k=1}^K\alpha_kBern(y_j|\theta_k)}=\frac{\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}{\sum_{k=1}^K\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}, j=1,2,\dots,N; k=1,2,\dots,K</span><script type="math/tex">\hat\gamma_{jk}=\frac{\alpha_kBern(y_j|\theta_k)}{\sum_{k=1}^K\alpha_kBern(y_j|\theta_k)}=\frac{\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}{\sum_{k=1}^K\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}, j=1,2,\dots,N; k=1,2,\dots,K</script></span></span><script type="math/tex"><span><span class="MathJax_Preview">\hat\gamma_{jk}=\frac{\alpha_kBern(y_j|\theta_k)}{\sum_{k=1}^K\alpha_kBern(y_j|\theta_k)}=\frac{\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}{\sum_{k=1}^K\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}, j=1,2,\dots,N; k=1,2,\dots,K</span><script type="math/tex">\hat\gamma_{jk}=\frac{\alpha_kBern(y_j|\theta_k)}{\sum_{k=1}^K\alpha_kBern(y_j|\theta_k)}=\frac{\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}{\sum_{k=1}^K\alpha_k\mu_k^{y_j}(1-\mu_k)^{1-y_j}}, j=1,2,\dots,N; k=1,2,\dots,K</script></span></script></span></li>
<li>M步:
<span><span class="MathJax_Preview"><span><span class="MathJax_Preview">\hat\mu_k=\frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}}\\
\hat\alpha_k=\frac{n_k}{N}</span><script type="math/tex">\hat\mu_k=\frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}}\\
\hat\alpha_k=\frac{n_k}{N}</script></span></span><script type="math/tex"><span><span class="MathJax_Preview">\hat\mu_k=\frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}}\\
\hat\alpha_k=\frac{n_k}{N}</span><script type="math/tex">\hat\mu_k=\frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}}\\
\hat\alpha_k=\frac{n_k}{N}</script></span></script></span></li>
</ol>
</blockquote>
<h4 id="l">目标函数L<a class="headerlink" href="#l" title="Permanent link">&para;</a></h4>
<div>
<div class="MathJax_Preview">
L(\theta)=\log P(Y|\theta)=\log \sum_Z P(Y,Z|\theta)=\log(\sum_Z P(Y|Z,\theta)P(Z|\theta))
</div>
<script type="math/tex; mode=display">
L(\theta)=\log P(Y|\theta)=\log \sum_Z P(Y,Z|\theta)=\log(\sum_Z P(Y|Z,\theta)P(Z|\theta))
</script>
</div>
<p>目标函数是不完全数据的对数似然</p>
<h4 id="em_2">EM算法导出<a class="headerlink" href="#em_2" title="Permanent link">&para;</a></h4>
<p>书中这部分内容回答为什么EM算法能近似实现对观测数据的极大似然估计？
$$
\begin{align}
L(\theta)-L(\theta^{(i)})&amp;=\log \left(\sum_Z\color{green}P(Y|Z,\theta<sup>{(i)})\color{black}\frac{P(Y|Z,\theta)P(Z|\theta)}{\color{green}P(Y|Z,\theta</sup>{(i)})}\color{black}\right)-\log P(Y|\theta^{(i)})\
&amp;\ge\sum_Z \color{green}P(Z|Y,\theta^{(i)})\color{black}\log \frac{P(Y|Z,\theta)P(Z|\theta)}{\color{green}P(Z|Y,\theta^{(i)})\color{black}}-\log P(Y|\theta^{(i)})\
&amp;=\sum_Z P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta<sup>{(i)})}-\color{red}\sum_ZP(Z|Y,\theta</sup>{(i)})\color{black}\log P(Y|\theta<sup>{(i)})\
&amp;=\sum_ZP(Z|Y,\theta</sup>{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta<sup>{(i)})P(Y|\theta</sup>{(i)})}
\end{align}
$$</p>
<p>以上用于推导迭代过程中两次<span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>会变大， 这里面红色部分是后加的方便理解前后两步之间的推导。绿色部分是为了构造期望， 进而应用琴声不等式。在这里凑项应该是凑<span><span class="MathJax_Preview">P(Z|Y,\theta^{(i)})</span><script type="math/tex">P(Z|Y,\theta^{(i)})</script></span>，书中这部分可能是笔误。</p>
<p>这里也写一下琴声不等式
$$
\log \sum_j \lambda_j y_j \ge \sum_j \lambda_j \log y_j, s.t., \lambda_j \ge 0, \sum_j \lambda_j = 1
$$
所以，这里的这一项不是随便凑的。</p>
<p>TODO:更新下这个图</p>
<h3 id="_9">高斯混合模型<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h3>
<p><strong>混合模型</strong>，有多种，高斯混合模型是最常用的。</p>
<p>高斯混合模型(Gaussian Mixture Model)是具有如下<strong>概率分布</strong>的模型:
$$
P(y|\theta)=\sum\limits^{K}_{k=1}\alpha_k\phi(y|\theta_k)
$$
其中，<span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>是系数，<span><span class="MathJax_Preview">\alpha_k\ge0</span><script type="math/tex">\alpha_k\ge0</script></span>，<span><span class="MathJax_Preview">\sum\limits^{K}_{k=1}\alpha_k=1</span><script type="math/tex">\sum\limits^{K}_{k=1}\alpha_k=1</script></span>, <span><span class="MathJax_Preview">\phi(y|\theta_k)</span><script type="math/tex">\phi(y|\theta_k)</script></span> 是<strong>高斯分布密度</strong>，<span><span class="MathJax_Preview">\theta_k=(\mu,\sigma^2)</span><script type="math/tex">\theta_k=(\mu,\sigma^2)</script></span>
$$
\phi(y|\theta_k)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\frac{(y-\mu_k)<sup>2}{2\sigma_k</sup>2}\right)
$$
上式表示第k个<strong>分</strong>模型。</p>
<p>以上, 注意几点：</p>
<ol>
<li>GMM的描述是概率分布，形式上可以看成是加权求和</li>
<li>
<p>加权求和的权重<span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>满足<span><span class="MathJax_Preview">\sum_{k=1}^K\alpha_k=1</span><script type="math/tex">\sum_{k=1}^K\alpha_k=1</script></span>的约束</p>
</li>
<li>
<p>求和符号中除去权重的部分，是高斯分布密度(PDF)。高斯混合模型是一种<span><span class="MathJax_Preview">\sum(权重\times 分布密度)=分布</span><script type="math/tex">\sum(权重\times 分布密度)=分布</script></span>的表达
    高斯混合模型的参数估计是EM算法的一个重要应用，隐马尔科夫模型的非监督学习也是EM算法的一个重要应用。</p>
</li>
<li>
<p>书中描述的是一维的高斯混合模型，d维的形式如下[<sup>2]，被称作多元正态分布，也叫多元高斯分布
$$
\phi(y|\theta_k)=\frac{1}{\sqrt{(2\pi)</sup>d|\Sigma|}}\exp\left(-\frac{(y-\mu_k)<sup>T\Sigma</sup>{-1}(y-\mu_k)}{2}\right)
$$
其中，协方差矩阵<span><span class="MathJax_Preview">\Sigma\in \R^{n\times n}</span><script type="math/tex">\Sigma\in \R^{n\times n}</script></span></p>
</li>
<li>
<p>另外，关于高斯模型的混合，还有另外一种混合的方式，沿着时间轴的方向做混合。可以理解为滤波器，典型的算法就是Kalman Filter，对应了时域与频域之间的关系，两个高斯的混合依然是高斯，混合的方法是卷积，而不是简单的加法，考虑到的是概率密度的混合，也是一种线性的加权。</p>
</li>
</ol>
<h4 id="gmm">GMM的图模型<a class="headerlink" href="#gmm" title="Permanent link">&para;</a></h4>
<p>这个弄的不咋好看，plate notation</p>
<p><img alt="图模型" src="assets/gmm_graph_model.png" /></p>
<h4 id="gmmem">GMM的EM算法<a class="headerlink" href="#gmmem" title="Permanent link">&para;</a></h4>
<p>问题描述:</p>
<p>已知观测数据<span><span class="MathJax_Preview">y_1, y_2, \dots , y_N</span><script type="math/tex">y_1, y_2, \dots , y_N</script></span>，由高斯混合模型生成
$$
P(y|\theta)=\sum_{k=1}^K\alpha_k\phi(y|\theta_k)
$$
其中， <span><span class="MathJax_Preview">\theta=(\alpha_1,\alpha_2,\dots,\alpha_K;\theta_1,\theta_2,\dots,\theta_K)</span><script type="math/tex">\theta=(\alpha_1,\alpha_2,\dots,\alpha_K;\theta_1,\theta_2,\dots,\theta_K)</script></span></p>
<p>补充下，不完全数据的似然函数应该是
$$
\begin{align}
P(y|\theta)=&amp;\prod_{j=1}<sup>NP(y_j|\theta)\
=&amp;\prod_{j=1}</sup>N\sum_{k=1}^K\alpha_k\phi(y|\theta_k)
\end{align}
$$
使用EM算法估计GMM的参数<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></p>
<h5 id="1_1">1. 明确隐变量, 初值<a class="headerlink" href="#1_1" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p>观测数据<span><span class="MathJax_Preview">y_j, j=1,2,\dots,N</span><script type="math/tex">y_j, j=1,2,\dots,N</script></span>这样产生, 是<strong>已知的</strong>:</p>
</li>
<li>
<p>依概率<span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span><strong>选择第<span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>个</strong>高斯分布分模型<span><span class="MathJax_Preview">\phi(y|\theta_k)</span><script type="math/tex">\phi(y|\theta_k)</script></span>;</p>
</li>
<li>依第<span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>个分模型的概率分布<span><span class="MathJax_Preview">\phi(y|\theta_k)</span><script type="math/tex">\phi(y|\theta_k)</script></span>生成观测数据<span><span class="MathJax_Preview">y_j</span><script type="math/tex">y_j</script></span></li>
<li>
<p>反映观测数据<span><span class="MathJax_Preview">y_j</span><script type="math/tex">y_j</script></span>来自第<span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>个分模型的数据是<strong>未知的</strong>, <span><span class="MathJax_Preview">k=1,2,\dots,K</span><script type="math/tex">k=1,2,\dots,K</script></span> 以<strong>隐变量<span><span class="MathJax_Preview">\gamma_{jk}</span><script type="math/tex">\gamma_{jk}</script></span></strong>表示
     <strong>注意这里<span><span class="MathJax_Preview">\gamma_{jk}</span><script type="math/tex">\gamma_{jk}</script></span>的维度<span><span class="MathJax_Preview">(j\times k)</span><script type="math/tex">(j\times k)</script></span></strong>
$$
  \gamma_{jk}=
  \begin{cases}
  1, &amp;第j个观测来自第k个分模型\
  0, &amp;否则
  \end{cases}\
  j=1,2,\dots,N; k=1,2,\dots,K; \gamma_{jk}\in{0,1}
$$
  注意, 以上说明有几个假设:</p>
</li>
<li>
<p>隐变量和观测变量的数据对应, 每个观测数据，对应了一个隐变量，<span><span class="MathJax_Preview">\gamma_{jk}</span><script type="math/tex">\gamma_{jk}</script></span>是一种one-hot的形式。</p>
</li>
<li>
<p>具体的单一观测数据是混合模型中的某一个模型产生的</p>
</li>
<li>
<p>完全数据为<span><span class="MathJax_Preview">(y_j,\gamma_{j1},\gamma_{j2},\dots,\gamma_{jK},k=1,2,\dots,N)</span><script type="math/tex">(y_j,\gamma_{j1},\gamma_{j2},\dots,\gamma_{jK},k=1,2,\dots,N)</script></span></p>
</li>
<li>
<p>完全数据似然函数
$$
\begin{aligned}
  P(y,\gamma|\theta)=&amp;\prod_{j=1}^NP(y_j,\gamma_{j1},\gamma_{j2},\dots,\gamma_{jK}|\theta)\
  =&amp;\prod_{k=1}<sup>K\prod_{j=1}</sup>N\left[\alpha_k\phi(y_j|\theta_k)\right]^{\gamma_{jk}}\
  =&amp;\prod_{k=1}<sup>K\alpha_k</sup>{n_k}\prod_{j=1}<sup>N\left[\phi(y_j|\theta_k)\right]</sup>{\gamma_{jk}}\
  =&amp;\prod_{k=1}<sup>K\alpha_k</sup>{n_k}\prod_{j=1}<sup>N\left[\frac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\frac{(y_j-\mu_k)</sup>2}{2\sigma<sup>2}\right)\right]</sup>{\gamma_{jk}}\
  \end{aligned}
$$
  其中<span><span class="MathJax_Preview">n_k=\sum_{j=1}^N\gamma_{jk}, \sum_{k=1}^Kn_k=N</span><script type="math/tex">n_k=\sum_{j=1}^N\gamma_{jk}, \sum_{k=1}^Kn_k=N</script></span></p>
</li>
<li>
<p>完全数据对数似然函数
$$
  \log P(y,\gamma|\theta)=\sum_{k=1}^K\left{n_k\log \alpha_k+\sum_{j=1}^N\gamma_{jk}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k -\frac{1}{2\sigma<sup>2}(y_j-\mu_k)</sup>2\right]\right}
$$</p>
</li>
</ul>
<h5 id="2-eq">2. E步,确定Q函数<a class="headerlink" href="#2-eq" title="Permanent link">&para;</a></h5>
<p>把<span><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> 函数表示成参数形式</p>
<div>
<div class="MathJax_Preview">\begin{aligned}Q(\theta,\theta^{(i)})=&amp;E[\log P(y,\gamma|\theta)|y,\theta^{(i)}]\\=&amp;\color{green}E\color{black}\left\{\sum_{k=1}^K\left\{\color{red}n_k\color{black}\log \alpha_k+\color{blue}\sum_{j=1}^N\gamma _{jk}\color{black}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma _k-\frac{1}{2\sigma^2(y_j-\mu_k)^2}\right]\right\}\right\}\\=&amp;\color{green}E\color{black}\left\{\sum_{k=1}^K\left\{\color{red}\sum_{j=1}^N\gamma_{jk}\color{black}\log \alpha_k+\color{blue}\sum_{j=1}^N\gamma _{jk}\color{black}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma _k-\frac{1}{2\sigma^2(y_j-\mu_k)^2}\right]\right\}\right\}\\=&amp;\sum_{k=1}^K\left\{\color{red}\sum_{j=1}^{N}(\color{green}E\color{red}\gamma_{jk})\color{black}\log \alpha_k+\color{blue}\sum_{j=1}^N(\color{green}E\color{blue}\gamma _{jk})\color{black}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma _k-\frac{1}{2\sigma^2(y_j-\mu_k)^2}\right]\right\}\\\end{aligned}</div>
<script type="math/tex; mode=display">\begin{aligned}Q(\theta,\theta^{(i)})=&E[\log P(y,\gamma|\theta)|y,\theta^{(i)}]\\=&\color{green}E\color{black}\left\{\sum_{k=1}^K\left\{\color{red}n_k\color{black}\log \alpha_k+\color{blue}\sum_{j=1}^N\gamma _{jk}\color{black}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma _k-\frac{1}{2\sigma^2(y_j-\mu_k)^2}\right]\right\}\right\}\\=&\color{green}E\color{black}\left\{\sum_{k=1}^K\left\{\color{red}\sum_{j=1}^N\gamma_{jk}\color{black}\log \alpha_k+\color{blue}\sum_{j=1}^N\gamma _{jk}\color{black}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma _k-\frac{1}{2\sigma^2(y_j-\mu_k)^2}\right]\right\}\right\}\\=&\sum_{k=1}^K\left\{\color{red}\sum_{j=1}^{N}(\color{green}E\color{red}\gamma_{jk})\color{black}\log \alpha_k+\color{blue}\sum_{j=1}^N(\color{green}E\color{blue}\gamma _{jk})\color{black}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma _k-\frac{1}{2\sigma^2(y_j-\mu_k)^2}\right]\right\}\\\end{aligned}</script>
</div>
<div>
<div class="MathJax_Preview">\begin{aligned}\hat \gamma _{jk}= &amp;\color{purple}E(\gamma_{jk}|y,\theta)=P(\gamma_{jk}=1|y,\theta)\\=&amp;\frac{P(\gamma_{jk}=1,y_j|\theta)}{\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta)}\\=&amp;\frac{P(y_j|\color{red}\gamma_{jk}=1,\theta\color{black})\color{green}P(\gamma_{jk}=1|\theta)}{\sum_{k=1}^KP(y_j|\gamma_{jk}=1,\theta)P(\gamma_{jk}=1|\theta)}\\=&amp;\frac{\color{green}\alpha_k\color{black}\phi(y_j|\color{red}\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)}\end{aligned}</div>
<script type="math/tex; mode=display">\begin{aligned}\hat \gamma _{jk}= &\color{purple}E(\gamma_{jk}|y,\theta)=P(\gamma_{jk}=1|y,\theta)\\=&\frac{P(\gamma_{jk}=1,y_j|\theta)}{\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta)}\\=&\frac{P(y_j|\color{red}\gamma_{jk}=1,\theta\color{black})\color{green}P(\gamma_{jk}=1|\theta)}{\sum_{k=1}^KP(y_j|\gamma_{jk}=1,\theta)P(\gamma_{jk}=1|\theta)}\\=&\frac{\color{green}\alpha_k\color{black}\phi(y_j|\color{red}\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)}\end{aligned}</script>
</div>
<p>这部分内容就是搬运了书上的公式，有几点说明:</p>
<ol>
<li>注意这里<span><span class="MathJax_Preview">E(\gamma_{jk}|y,\theta)</span><script type="math/tex">E(\gamma_{jk}|y,\theta)</script></span>，记为<span><span class="MathJax_Preview">\hat\gamma_{jk}</span><script type="math/tex">\hat\gamma_{jk}</script></span>， 对应了E步求的<strong>期望</strong>中的一部分。</li>
<li>对应理解一下上面公式中的红色，蓝色和绿色部分，以及<span><span class="MathJax_Preview">\hat\gamma_{jk}</span><script type="math/tex">\hat\gamma_{jk}</script></span>中红色和绿色的对应关系</li>
<li>这里用到了<span><span class="MathJax_Preview">n_k=\sum_{j=1}^N\gamma_{jk}</span><script type="math/tex">n_k=\sum_{j=1}^N\gamma_{jk}</script></span></li>
<li><span><span class="MathJax_Preview">\hat \gamma_{jk}</span><script type="math/tex">\hat \gamma_{jk}</script></span>为分模型<span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>对观测数据<span><span class="MathJax_Preview">y_j</span><script type="math/tex">y_j</script></span>的响应度。这里，紫色标记的第一行参考伯努利分布的期望。</li>
</ol>
<p><span><span class="MathJax_Preview"><span><span class="MathJax_Preview">Q(\theta,\theta^{(i)})=\sum_{k=1}^Kn_k\log \alpha_k+\sum_{j=1}^N\hat \gamma_{jk}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]</span><script type="math/tex">Q(\theta,\theta^{(i)})=\sum_{k=1}^Kn_k\log \alpha_k+\sum_{j=1}^N\hat \gamma_{jk}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]</script></span></span><script type="math/tex"><span><span class="MathJax_Preview">Q(\theta,\theta^{(i)})=\sum_{k=1}^Kn_k\log \alpha_k+\sum_{j=1}^N\hat \gamma_{jk}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]</span><script type="math/tex">Q(\theta,\theta^{(i)})=\sum_{k=1}^Kn_k\log \alpha_k+\sum_{j=1}^N\hat \gamma_{jk}\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]</script></span></script></span>
其中<span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>表示第<span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>步迭代</p>
<ol>
<li>写出<span><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> 函数在推导的时候有用，但是在程序计算的时候，E步需要计算的就是<span><span class="MathJax_Preview">\hat\gamma_{jk}</span><script type="math/tex">\hat\gamma_{jk}</script></span>，M步用到了这个结果。其实抄公式没有什么意义，主要是能放慢看公式的速度。和图表一样，公式简洁的表达了很多信息，公式中也许更能体会到数学之美。</li>
</ol>
<h5 id="3-m">3. M步<a class="headerlink" href="#3-m" title="Permanent link">&para;</a></h5>
<p>求函数<span><span class="MathJax_Preview">Q(\theta,\theta^{(i)})</span><script type="math/tex">Q(\theta,\theta^{(i)})</script></span>对<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>的极大值，分别求<span><span class="MathJax_Preview">\sigma, \mu, \alpha</span><script type="math/tex">\sigma, \mu, \alpha</script></span>
$$
\theta^{(i+1)}=\arg\max_\theta Q(\theta,\theta^{(i)})
$$</p>
<ul>
<li><span><span class="MathJax_Preview">\arg\max</span><script type="math/tex">\arg\max</script></span> 就是求Q的极值对应的参数<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>，如说是离散的，遍历所有值，最大查找，如果是连续的，偏导为零求极值。</li>
<li><span><span class="MathJax_Preview">\frac {\partial Q}{\partial \mu_k}=0, \frac {\partial{Q}}{\partial{\sigma^2}}= 0</span><script type="math/tex">\frac {\partial Q}{\partial \mu_k}=0, \frac {\partial{Q}}{\partial{\sigma^2}}= 0</script></span>  得到<span><span class="MathJax_Preview">\hat\mu_k, \hat \sigma_k^2</span><script type="math/tex">\hat\mu_k, \hat \sigma_k^2</script></span></li>
<li><span><span class="MathJax_Preview">\sum_{k=1}^K\alpha_k=1, \frac{\partial{Q}}{\partial{\alpha_k}}=0</span><script type="math/tex">\sum_{k=1}^K\alpha_k=1, \frac{\partial{Q}}{\partial{\alpha_k}}=0</script></span> 得到<span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span></li>
</ul>
<h5 id="4">4. 停止条件<a class="headerlink" href="#4" title="Permanent link">&para;</a></h5>
<p>重复以上计算，直到对数似然函数值不再有明显的变化为止。</p>
<h4 id="92">算法9.2<a class="headerlink" href="#92" title="Permanent link">&para;</a></h4>
<p>这部分摘要总结了前面的公式。</p>
<p>因为公式比较集中，方便对比，注意体会以下两个方面:</p>
<ol>
<li>这几个公式中待求的变量的维度和角标的关系。</li>
<li>这里面有求和，前面提到过要注意体会每一步刷的是模型，还是样本</li>
</ol>
<h4 id="gmmcv">GMM在CV中的应用<a class="headerlink" href="#gmmcv" title="Permanent link">&para;</a></h4>
<p>其实CV中用到了很多统计的方法，GMM在GrabCut方法中用于前景和背景建模。</p>
<h3 id="kmeans">Kmeans<a class="headerlink" href="#kmeans" title="Permanent link">&para;</a></h3>
<p>另外，直觉上看，GMM最直观的想法就是Kmeans，那么:</p>
<ol>
<li>在Kmeans常见的描述中都有距离的概念，对应在算法9.2 的描述中，该如何理解?
   这里面距离对应了方差，二范数平方。</li>
<li>那么又是怎么在每轮刷过距离之后，重新划分样本的分类呢?
   这里对应了响应度，响应度对应了一个<span><span class="MathJax_Preview">j \times k</span><script type="math/tex">j \times k</script></span>的矩阵，记录了每一个<span><span class="MathJax_Preview">y_j</span><script type="math/tex">y_j</script></span> 对第<span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>个模型的响应度，可以理解为划分了类别。</li>
</ol>
<h4 id="k">K怎么定<a class="headerlink" href="#k" title="Permanent link">&para;</a></h4>
<ul>
<li>手肘法</li>
<li>Gap Statistics[^1]</li>
</ul>
<h3 id="_10">广义期望极大<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h3>
<p>广义期望极大(generalized expectation maximization, <span><span class="MathJax_Preview">GEM</span><script type="math/tex">GEM</script></span>)</p>
<p>广义期望极大是为了解决什么问题？</p>
<p>看名字是为了通用解决方案吧</p>
<h2 id="_11">其他<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h2>
<h3 id="93">习题9.3<a class="headerlink" href="#93" title="Permanent link">&para;</a></h3>
<p>GMM模型的参数(<span><span class="MathJax_Preview">\alpha _k, \mu _k, \sigma^2_k <span><span class="MathJax_Preview">)应该是</span><script type="math/tex">)应该是</script></span>3k3k</span><script type="math/tex">\alpha _k, \mu _k, \sigma^2_k <span><span class="MathJax_Preview">)应该是</span><script type="math/tex">)应该是</script></span>3k3k</script></span>个，题目9.3中提出两个分量的高斯混合模型的5个参数，是因为参数<span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span>满足<span><span class="MathJax_Preview">\sum_{k=1}^K\alpha _k=1</span><script type="math/tex">\sum_{k=1}^K\alpha _k=1</script></span></p>
<h3 id="94">习题9.4<a class="headerlink" href="#94" title="Permanent link">&para;</a></h3>
<p>EM算法用到朴素贝叶斯的非监督学习，就是说没有标注的数据。   </p>
<p>这个题目可以参考https://ttic.uchicago.edu/~suriya/website-intromlss2018/course_material/Day10a.pdf</p>
<h2 id="_12">参考<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p>[^3]: <a href="-">Maximum-likelihood from incomplete data via the EM algorithm</a></p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM Algorithm</a></p>
</li>
<li>
<p><a href="http://scikit-learn.org/stable/modules/mixture.html">Sklearn Gaussian Mixed Model</a></p>
</li>
<li>
<p>[^1]: <a href="https://web.stanford.edu/~hastie/Papers/gap.pdf">Gap Statistics</a></p>
</li>
<li>
<p>[^2]: <a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83">多元正态分布</a></p>
</li>
<li>
<p><a href="[https://mml-book.com](https://mml-book.com/)">mml</a></p>
</li>
<li>
<p>[^3 ]: <a href="https://www.quora.com/What-is-the-difference-between-probability-and-likelihood-1/answer/Jason-Eisner">probability and likelihood</a></p>
</li>
<li>
<p>[^4]: <a href="https://en.wikipedia.org/wiki/Convex_combination">Convex Combination</a></p>
</li>
</ol>
<p><strong><a href="#导读">⬆ top</a></strong></p>
                
                  
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://hai5g.cn/aiwiki/lihang/CH09/";
      this.page.identifier =
        "/lihang/CH09/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//AI-Wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../CH08/" title="提升方法" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                提升方法
              </span>
            </div>
          </a>
        
        
          <a href="../CH10/" title="隐马尔可夫模型" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                隐马尔可夫模型
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 AI Wiki Team
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.39abc4af.js"></script>
      
        
        
          
          <script src="../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdn.jsdelivr.net/gh/ethantw/Han@3.3.0/dist/han.min.js"></script>
      
        <script src="../../_static/js/extra.js?v=10"></script>
      
        <script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>