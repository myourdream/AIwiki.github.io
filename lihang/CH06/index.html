



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="AI Wiki 是一个编程竞赛知识整合站点，提供有趣又实用的编程竞赛知识以及其他有帮助的内容，帮助广大编程竞赛爱好者更快更深入地学习编程竞赛">
      
      
        <link rel="canonical" href="https://hai5g.cn/aiwiki/lihang/CH06/">
      
      
        <meta name="author" content="AI Wiki Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>逻辑斯蒂回归与最大熵模型 - AI Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,400i,700|Fira+Mono">
        <style>body,input{font-family:"Fira Sans","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Fira Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
      <link rel="manifest" href="../../manifest.webmanifest">
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/ah@1.5.0/han.min.css">
    
      <link rel="stylesheet" href="../../_static/css/extra.css?v=11">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#ch06" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-header-nav__button md-logo">
          
            <i class="md-icon">school</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              AI Wiki
            </span>
            <span class="md-header-nav__topic">
              逻辑斯蒂回归与最大熵模型
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../.." title="简介" class="md-tabs__link">
          简介
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../Coursera_ML_AndrewNg/" title="机器学习" class="md-tabs__link">
          机器学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../qa500/" title="深度学习500问" class="md-tabs__link">
          深度学习500问
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../" title="统计学习" class="md-tabs__link md-tabs__link--active">
          统计学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../hands-on-ml-zh/README.old/" title="Sklearn与TensorFlow" class="md-tabs__link">
          Sklearn与TensorFlow
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-nav__button md-logo">
      
        <i class="md-icon">school</i>
      
    </a>
    AI Wiki
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      简介
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        简介
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../.." title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/AI学习路线/" title="AI学习路线1" class="md-nav__link">
      AI学习路线1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/ai-union-201904/" title="AI学习路线2" class="md-nav__link">
      AI学习路线2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/v0.1/" title="AI 路线图v0.1" class="md-nav__link">
      AI 路线图v0.1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/v0.2/" title="AI 路线图v0.2" class="md-nav__link">
      AI 路线图v0.2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/ai-roadmap/v1.0/" title="ApacheCN 人工智能知识树" class="md-nav__link">
      ApacheCN 人工智能知识树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7" type="checkbox" id="nav-1-7">
    
    <label class="md-nav__link" for="nav-1-7">
      工具软件
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-1-7">
        工具软件
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/judgers/" title="评测工具" class="md-nav__link">
      评测工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/editors/" title="编辑工具" class="md-nav__link">
      编辑工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/wsl/" title="WSL (Windows 10)" class="md-nav__link">
      WSL (Windows 10)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/spj/" title="Special Judge" class="md-nav__link">
      Special Judge
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7-5" type="checkbox" id="nav-1-7-5">
    
    <label class="md-nav__link" for="nav-1-7-5">
      Testlib
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-1-7-5">
        Testlib
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/" title="Testlib 简介" class="md-nav__link">
      Testlib 简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/general/" title="通用" class="md-nav__link">
      通用
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/generator/" title="Generator" class="md-nav__link">
      Generator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/validator/" title="Validator" class="md-nav__link">
      Validator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/interactor/" title="Interactor" class="md-nav__link">
      Interactor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/testlib/checker/" title="Checker" class="md-nav__link">
      Checker
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/docker-deploy/" title="Docker 部署" class="md-nav__link">
      Docker 部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/about/" title="关于本项目" class="md-nav__link">
      关于本项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/faq/" title="F.A.Q." class="md-nav__link">
      F.A.Q.
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      机器学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        机器学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/SUMMARY/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/math/" title="数学基础" class="md-nav__link">
      数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week1/" title="week1" class="md-nav__link">
      week1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week2/" title="week2" class="md-nav__link">
      week2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week3/" title="week3" class="md-nav__link">
      week3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week4/" title="week4" class="md-nav__link">
      week4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week5/" title="week5" class="md-nav__link">
      week5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week6/" title="week6" class="md-nav__link">
      week6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week7/" title="week7" class="md-nav__link">
      week7
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week8/" title="week8" class="md-nav__link">
      week8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week9/" title="week9" class="md-nav__link">
      week9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Coursera_ML_AndrewNg/markdown/week10/" title="week10" class="md-nav__link">
      week10
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      深度学习500问
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        深度学习500问
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/content/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch01_math/ch01_math/" title="第一章_数学基础" class="md-nav__link">
      第一章_数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch02_机器学习基础/第二章_机器学习基础/" title="第二章_机器学习基础" class="md-nav__link">
      第二章_机器学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch03_深度学习基础/第三章_深度学习基础/" title="第三章_深度学习基础" class="md-nav__link">
      第三章_深度学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch04_经典网络/第四章_经典网络/" title="第四章_经典网络" class="md-nav__link">
      第四章_经典网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch05_卷积神经网络(CNN)/第五章 卷积神经网络（CNN）/" title="第五章 卷积神经网络（CNN）" class="md-nav__link">
      第五章 卷积神经网络（CNN）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch06_循环神经网络(RNN)/第六章_循环神经网络(RNN)/" title="第六章_循环神经网络(RNN)" class="md-nav__link">
      第六章_循环神经网络(RNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch07_生成对抗网络(GAN)/ch7/" title="第七章_生成对抗网络(GAN)" class="md-nav__link">
      第七章_生成对抗网络(GAN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch08_目标检测/第八章_目标检测/" title="第八章_目标检测" class="md-nav__link">
      第八章_目标检测
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch09_图像分割/第九章_图像分割/" title="第九章_图像分割" class="md-nav__link">
      第九章_图像分割
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch10_强化学习/第十章_强化学习/" title="第十章_强化学习" class="md-nav__link">
      第十章_强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch11_迁移学习/第十一章_迁移学习/" title="第十一章_迁移学习" class="md-nav__link">
      第十一章_迁移学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch12_网络搭建及训练/第十二章_网络搭建及训练/" title="第十二章_网络搭建及训练" class="md-nav__link">
      第十二章_网络搭建及训练
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch13_优化算法/第十三章_优化算法/" title="第十三章_优化算法" class="md-nav__link">
      第十三章_优化算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch14_超参数调整/第十四章_超参数调整/" title="第十四章_超参数调整" class="md-nav__link">
      第十四章_超参数调整
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch15_GPU和框架选型/第十五章_异构运算、GPU及框架选型/" title="第十五章_异构运算、GPU及框架选型" class="md-nav__link">
      第十五章_异构运算、GPU及框架选型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch16_自然语言处理(NLP)/第十六章_NLP/" title="第十六章_NLP" class="md-nav__link">
      第十六章_NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署/" title="第十七章_模型压缩、加速及移动端部署" class="md-nav__link">
      第十七章_模型压缩、加速及移动端部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch18_后端架构选型、离线及实时计算/第十八章_后端架构选型、离线及实时计算/" title="第十八章_后端架构选型、离线及实时计算" class="md-nav__link">
      第十八章_后端架构选型、离线及实时计算
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../qa500/ch18_后端架构选型及应用场景/第十八章_后端架构选型及应用场景/" title="第十八章_后端架构选型及应用场景" class="md-nav__link">
      第十八章_后端架构选型及应用场景
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      统计学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        统计学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH01/" title="统计学习及监督学习概论" class="md-nav__link">
      统计学习及监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH02/" title="感知机" class="md-nav__link">
      感知机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH03/" title="K近邻法" class="md-nav__link">
      K近邻法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH04/" title="朴素贝叶斯法" class="md-nav__link">
      朴素贝叶斯法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH05/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        逻辑斯蒂回归与最大熵模型
      </label>
    
    <a href="./" title="逻辑斯蒂回归与最大熵模型" class="md-nav__link md-nav__link--active">
      逻辑斯蒂回归与最大熵模型
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="前言" class="md-nav__link">
    前言
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" title="章节目录" class="md-nav__link">
    章节目录
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="导读" class="md-nav__link">
    导读
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" title="模型" class="md-nav__link">
    模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" title="逻辑斯谛回归模型" class="md-nav__link">
    逻辑斯谛回归模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" title="逻辑斯谛分布" class="md-nav__link">
    逻辑斯谛分布
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" title="二项逻辑斯谛回归模型" class="md-nav__link">
    二项逻辑斯谛回归模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" title="多项逻辑斯谛回归" class="md-nav__link">
    多项逻辑斯谛回归
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" title="二元推广" class="md-nav__link">
    二元推广
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="对数线性模型" class="md-nav__link">
    对数线性模型
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" title="模型参数估计" class="md-nav__link">
    模型参数估计
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logistic-regression" title="Logistic Regression" class="md-nav__link">
    Logistic Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-regression" title="Softmax Regression" class="md-nav__link">
    Softmax Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" title="最大熵模型" class="md-nav__link">
    最大熵模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" title="概念" class="md-nav__link">
    概念
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" title="信息量" class="md-nav__link">
    信息量
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" title="熵和概率" class="md-nav__link">
    熵和概率
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" title="最大熵原理" class="md-nav__link">
    最大熵原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" title="最大熵原理几何解释" class="md-nav__link">
    最大熵原理几何解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" title="特征与约束条件" class="md-nav__link">
    特征与约束条件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" title="模型" class="md-nav__link">
    模型
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" title="算法实现" class="md-nav__link">
    算法实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_21" title="特征提取原理" class="md-nav__link">
    特征提取原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" title="预测分类原理" class="md-nav__link">
    预测分类原理
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" title="最大熵模型的学习" class="md-nav__link">
    最大熵模型的学习
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#62" title="例6.2" class="md-nav__link">
    例6.2
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_24" title="一个约束条件" class="md-nav__link">
    一个约束条件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_25" title="两个约束条件" class="md-nav__link">
    两个约束条件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_26" title="三个约束条件" class="md-nav__link">
    三个约束条件
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_27" title="模型学习" class="md-nav__link">
    模型学习
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_28" title="目标函数" class="md-nav__link">
    目标函数
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_29" title="逻辑斯谛回归模型" class="md-nav__link">
    逻辑斯谛回归模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_30" title="最大熵模型" class="md-nav__link">
    最大熵模型
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_31" title="其他" class="md-nav__link">
    其他
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_32" title="代码实现" class="md-nav__link">
    代码实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demo" title="Demo" class="md-nav__link">
    Demo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxent" title="Maxent" class="md-nav__link">
    Maxent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mnist" title="Mnist" class="md-nav__link">
    Mnist
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_33" title="参考" class="md-nav__link">
    参考
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH07/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH08/" title="提升方法" class="md-nav__link">
      提升方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH09/" title="EM算法及其推广" class="md-nav__link">
      EM算法及其推广
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH10/" title="隐马尔可夫模型" class="md-nav__link">
      隐马尔可夫模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH11/" title="条件随机场" class="md-nav__link">
      条件随机场
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH12/" title="监督学习方法总结" class="md-nav__link">
      监督学习方法总结
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH13/" title="无监督学习概论" class="md-nav__link">
      无监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH14/" title="聚类方法" class="md-nav__link">
      聚类方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CH22/" title="无监督学习方法总结" class="md-nav__link">
      无监督学习方法总结
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Sklearn与TensorFlow
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Sklearn与TensorFlow
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/README.old/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/0.前言/" title="前言" class="md-nav__link">
      前言
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/1.机器学习概览/" title="机器学习概览" class="md-nav__link">
      机器学习概览
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/2.一个完整的机器学习项目/" title="一个完整的机器学习项目" class="md-nav__link">
      一个完整的机器学习项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/3.分类/" title="分类" class="md-nav__link">
      分类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/4.训练模型/" title="训练模型" class="md-nav__link">
      训练模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/5.支持向量机/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/6.决策树/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/7.集成学习和随机森林/" title="集成学习和随机森林" class="md-nav__link">
      集成学习和随机森林
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/9.启动并运行_TensorFlow/" title="启动并运行_TensorFlow" class="md-nav__link">
      启动并运行_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/10.人工神经网络介绍/" title="人工神经网络介绍" class="md-nav__link">
      人工神经网络介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/11.训练深层神经网络/" title="训练深层神经网络" class="md-nav__link">
      训练深层神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/12.设备和服务器上的分布式_TensorFlow/" title="设备和服务器上的分布式_TensorFlow" class="md-nav__link">
      设备和服务器上的分布式_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/13.卷积神经网络/" title="卷积神经网络" class="md-nav__link">
      卷积神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/14.循环神经网络/" title="循环神经网络" class="md-nav__link">
      循环神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/15.自编码器/" title="自编码器" class="md-nav__link">
      自编码器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/16.强化学习/" title="强化学习" class="md-nav__link">
      强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/B.机器学习项目清单/" title="B.机器学习项目清单" class="md-nav__link">
      B.机器学习项目清单
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/C.SVM_对偶问题/" title="C.SVM_对偶问题" class="md-nav__link">
      C.SVM_对偶问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../hands-on-ml-zh/docs/D.自动微分/" title="D.自动微分" class="md-nav__link">
      D.自动微分
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="前言" class="md-nav__link">
    前言
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" title="章节目录" class="md-nav__link">
    章节目录
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="导读" class="md-nav__link">
    导读
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" title="模型" class="md-nav__link">
    模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" title="逻辑斯谛回归模型" class="md-nav__link">
    逻辑斯谛回归模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" title="逻辑斯谛分布" class="md-nav__link">
    逻辑斯谛分布
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" title="二项逻辑斯谛回归模型" class="md-nav__link">
    二项逻辑斯谛回归模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" title="多项逻辑斯谛回归" class="md-nav__link">
    多项逻辑斯谛回归
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" title="二元推广" class="md-nav__link">
    二元推广
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="对数线性模型" class="md-nav__link">
    对数线性模型
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" title="模型参数估计" class="md-nav__link">
    模型参数估计
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#logistic-regression" title="Logistic Regression" class="md-nav__link">
    Logistic Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-regression" title="Softmax Regression" class="md-nav__link">
    Softmax Regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" title="最大熵模型" class="md-nav__link">
    最大熵模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" title="概念" class="md-nav__link">
    概念
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" title="信息量" class="md-nav__link">
    信息量
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" title="熵和概率" class="md-nav__link">
    熵和概率
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" title="最大熵原理" class="md-nav__link">
    最大熵原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" title="最大熵原理几何解释" class="md-nav__link">
    最大熵原理几何解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" title="特征与约束条件" class="md-nav__link">
    特征与约束条件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" title="模型" class="md-nav__link">
    模型
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" title="算法实现" class="md-nav__link">
    算法实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_21" title="特征提取原理" class="md-nav__link">
    特征提取原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" title="预测分类原理" class="md-nav__link">
    预测分类原理
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" title="最大熵模型的学习" class="md-nav__link">
    最大熵模型的学习
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#62" title="例6.2" class="md-nav__link">
    例6.2
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_24" title="一个约束条件" class="md-nav__link">
    一个约束条件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_25" title="两个约束条件" class="md-nav__link">
    两个约束条件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_26" title="三个约束条件" class="md-nav__link">
    三个约束条件
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_27" title="模型学习" class="md-nav__link">
    模型学习
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_28" title="目标函数" class="md-nav__link">
    目标函数
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_29" title="逻辑斯谛回归模型" class="md-nav__link">
    逻辑斯谛回归模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_30" title="最大熵模型" class="md-nav__link">
    最大熵模型
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_31" title="其他" class="md-nav__link">
    其他
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_32" title="代码实现" class="md-nav__link">
    代码实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demo" title="Demo" class="md-nav__link">
    Demo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxent" title="Maxent" class="md-nav__link">
    Maxent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mnist" title="Mnist" class="md-nav__link">
    Mnist
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_33" title="参考" class="md-nav__link">
    参考
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/myourdream/aiwiki/blob/master/docs/lihang/CH06/README.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="ch06">CH06 逻辑斯谛回归与最大熵模型<a class="headerlink" href="#ch06" title="Permanent link">&para;</a></h1>
<div class="toc">
<ul>
<li><a href="#ch06">CH06 逻辑斯谛回归与最大熵模型</a><ul>
<li><a href="#_1">前言</a><ul>
<li><a href="#_2">章节目录</a></li>
<li><a href="#_3">导读</a></li>
</ul>
</li>
<li><a href="#_4">模型</a><ul>
<li><a href="#_5">逻辑斯谛回归模型</a><ul>
<li><a href="#_6">逻辑斯谛分布</a></li>
<li><a href="#_7">二项逻辑斯谛回归模型</a></li>
<li><a href="#_8">多项逻辑斯谛回归</a><ul>
<li><a href="#_9">二元推广</a></li>
<li><a href="#_10">对数线性模型</a></li>
</ul>
</li>
<li><a href="#_11">模型参数估计</a><ul>
<li><a href="#logistic-regression">Logistic Regression</a></li>
<li><a href="#softmax-regression">Softmax Regression</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_12">最大熵模型</a><ul>
<li><a href="#_13">概念</a><ul>
<li><a href="#_14">信息量</a></li>
<li><a href="#_15">熵和概率</a></li>
<li><a href="#_16">最大熵原理</a></li>
<li><a href="#_17">最大熵原理几何解释</a></li>
<li><a href="#_18">特征与约束条件</a></li>
<li><a href="#_19">模型</a></li>
</ul>
</li>
<li><a href="#_20">算法实现</a><ul>
<li><a href="#_21">特征提取原理</a></li>
<li><a href="#_22">预测分类原理</a></li>
</ul>
</li>
<li><a href="#_23">最大熵模型的学习</a><ul>
<li><a href="#62">例6.2</a><ul>
<li><a href="#_24">一个约束条件</a></li>
<li><a href="#_25">两个约束条件</a></li>
<li><a href="#_26">三个约束条件</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#_27">模型学习</a><ul>
<li><a href="#_28">目标函数</a><ul>
<li><a href="#_29">逻辑斯谛回归模型</a></li>
<li><a href="#_30">最大熵模型</a></li>
</ul>
</li>
<li><a href="#_31">其他</a></li>
</ul>
</li>
<li><a href="#_32">代码实现</a><ul>
<li><a href="#demo">Demo</a></li>
<li><a href="#maxent">Maxent</a></li>
<li><a href="#mnist">Mnist</a></li>
</ul>
</li>
<li><a href="#_33">参考</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="_1">前言<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<h3 id="_2">章节目录<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<ol>
<li>逻辑斯谛回归模型</li>
<li>逻辑斯谛分布</li>
<li>二项逻辑斯谛回归模型</li>
<li>模型参数估计</li>
<li>多项逻辑斯蒂回归<strong>模型</strong></li>
<li>最大熵模型</li>
<li>最大熵原理</li>
<li>最大熵模型定义</li>
<li>最大熵模型学习</li>
<li>极大似然估计</li>
<li>模型学习的最优化算法</li>
<li>改进的迭代尺度法</li>
<li>拟牛顿法</li>
</ol>
<h3 id="_3">导读<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<blockquote>
<p>在最大熵的通用迭代算法GIS中, E过程就是跟着现有的模型计算每一个特征的数学期望值, M过程就是根据这些特征的数学期望值和实际观测值的比值, 调整模型参数. 这里最大化的目标函数是<strong>熵函数</strong>.</p>
<p>--吴军, 数学之美 <span><span class="MathJax_Preview">P_{244}</span><script type="math/tex">P_{244}</script></span> </p>
</blockquote>
<p>这里的<strong>熵函数</strong>是条件熵.</p>
<ul>
<li>
<p>这一章放在决策树后面，可能就因为熵的概念，对比前面<a href="../CH05/">CH05</a>部分理解对应的损失函数，发现其中的区别和联系。</p>
</li>
<li>
<p>LR做二分类的时候， <span><span class="MathJax_Preview">\cal{Y}=\{0,1\}</span><script type="math/tex">\cal{Y}=\{0,1\}</script></span>，在涉及到综合考虑各种模型损失函数作为0-1损失上界存在的情况中， <span><span class="MathJax_Preview">\cal{Y}=\{-1,+1\}</span><script type="math/tex">\cal{Y}=\{-1,+1\}</script></span>，这时方便使用函数间隔<span><span class="MathJax_Preview">yf(x)</span><script type="math/tex">yf(x)</script></span></p>
</li>
<li>
<p>本章从逻辑斯谛分布开始，在<a href="../CH04/">CH04</a>的时候，应该熟悉狄利克雷分布和高斯分布，对于离散和连续型的情况应该熟悉这两个分布，这样在这一章看到逻辑斯谛分布的时候会更适应。在书上有这样一句</p>
</li>
</ul>
<blockquote>
<p>二项逻辑斯谛回归模型是一种分类模型，由条件概率分布<span><span class="MathJax_Preview">P(Y|X)</span><script type="math/tex">P(Y|X)</script></span>表示，<strong>形式为参数化的逻辑斯谛分布</strong>。</p>
</blockquote>
<p><del>这一句是这两小节唯一的联系，可能不是很好理解。</del> 分类问题，可以表示成one-hot的形式，而one-hot可以认为是概率的一种表达，只是很确定的一种概率表达。而最大熵模型，是一种不确定的概率表达，其中这个概率，是一个条件概率，是构建的特征函数生成的概率。他们之间的关系有点像hard 和 soft，类似的思想还有kmeans和GMM之间的关系。
  因为书中第四章并没有讲到高斯朴素贝叶斯(GNB)，有GNB做类比，这里可能更容易理解一点，这里重新推荐一下第四章的参考文献1[^1]，配合理解NB和LR的关系。</p>
<ul>
<li>
<p>在模型参数估计的部分用到了<span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>，这个应该联想到狄利克雷分布</p>
</li>
<li>
<p>关于NB和LR的对比，Ng也有一篇文章[^2]</p>
</li>
<li>
<p>平方误差经过Sigmoid之后得到的是非凸函数</p>
</li>
<li>
<p>书中LR篇幅不大， 注意这样一句，<code>在逻辑斯谛回归模型中，输出Y=1的对数几率是输入x的线性函数。或者说，输出Y=1的对数几率是由输入x的线性函数表示的模型，及逻辑斯谛回归模型。</code></p>
</li>
<li>
<p>LR 和 Maxent什么关系？有人说明了这两个是等价的。另外也有说在NLP里LR叫做Maxent。Maxent更多的是从信息的角度来分析这个算法。</p>
</li>
<li>
<p>书中没有直接给出LR的损失函数，在<a href="../CH12/">CH12</a>中有提到LR的损失函数是<strong>逻辑斯谛损失函数</strong>。如果采用损失函数上界的那类分析方法，定义<span><span class="MathJax_Preview">\cal{Y}=\{+1,-1\}</span><script type="math/tex">\cal{Y}=\{+1,-1\}</script></span>， 有<span><span class="MathJax_Preview">\log_2(1+\exp(yf(x)))=\log_2(1+\exp(y(w\cdot x)))</span><script type="math/tex">\log_2(1+\exp(yf(x)))=\log_2(1+\exp(y(w\cdot x)))</script></span></p>
</li>
<li>
<p>概率的表示方法之一是采用[0,1]之间的数字，也可以使用odds，概率较低的时候用赔率。书中逻辑斯谛分布的定义给的是CDF。</p>
</li>
<li>
<p>LR和CRF也可以对比着看，在CRF中， 势函数是严格正的，通常定义为指数函数，概率无向图模型的联合概率分布<span><span class="MathJax_Preview">P(Y)</span><script type="math/tex">P(Y)</script></span>可以表示为如下形式：
  $$
  P(Y)=\frac{1}{Z}\prod_C\Psi_C(Y_C)\
  Z=\sum_Y\prod_C\Psi_C(Y_C)\
  $$
其实LR 也可以看成这样的形式，对应的<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>有如下的表示形式
$$
Z=1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)\
$$
注意， 定义中的<span><span class="MathJax_Preview">x\in \R^{n+1},w_k\in\R^{n+1}</span><script type="math/tex">x\in \R^{n+1},w_k\in\R^{n+1}</script></span>，这样，上面常数1是考虑到偏置项归一化了
实际上可以写成下面的形式
$$
P(Y=k|x)=\frac{\exp(w_k\cdot x)}{\sum_{k=1}^{K}\exp(w_k\cdot x)}
$$</p>
</li>
</ul>
<p>其中第<span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>项对应了偏置，对应的<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>=1，所以是一个常数<span><span class="MathJax_Preview">w_0</span><script type="math/tex">w_0</script></span>，将分子归一化就得到了书中的表达方式，这就是出现个1的原因。</p>
<ul>
<li>对于问题，什么情况下需要归一化，可以考虑下模型是不是要求这个特征要构成一个概率分布。</li>
<li>单纯形法是求解线性规划问题的有效方法，最初是为了解决空军军事规划问题，之后成为了解决线性规划问题的有效方法。这个在运筹学中有介绍，比较经典的参考是胡运权的《运筹学》。</li>
</ul>
<h2 id="_4">模型<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Logistic regression is a special case of maximum entropy with two labels +1 and −1.</p>
</blockquote>
<h3 id="_5">逻辑斯谛回归模型<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>这一章的这个部分，可以认为是对第四章的一个补充与延续，只是第四章最后没有说高斯朴素贝叶斯。在《机器学习，周志华》上把这个叫做对数几率回归。</p>
<h4 id="_6">逻辑斯谛分布<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<p>注意分布函数中关于位置参数，形状参数的说明，可以大致的和高斯对应理解。
$$
F(x)=P(X\leqslant x)=\frac{1}{1+\exp(-(x-\mu)/\gamma)}
$$</p>
<p>关于逻辑斯谛， 更常见的一种表达是Logistic function
$$
\sigma(z)=\frac{1}{1+\exp(-z)}
$$
这个函数把实数域映射到(0, 1)区间，这个范围正好是概率的范围， 而且可导，对于0输入， 得到的是0.5，可以用来表示等可能性。</p>
<h4 id="_7">二项逻辑斯谛回归模型<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<p>书中给出的定义：</p>
<p>二项逻辑斯谛回归模型是如下的条件概率分布：
$$
\begin{aligned}
P(Y=1|x)&amp;=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\
&amp;=\frac{\exp(w\cdot x)/\exp(w\cdot x)}{(1+\exp(w\cdot x))/(\exp(w\cdot x))}\
&amp;=\frac{1}{e^{-(w\cdot x)}+1}\
P(Y=0|x)&amp;=\frac{1}{1+\exp(w\cdot x)}\
&amp;=1-\frac{1}{1+e^{-(w\cdot x)}}\
&amp;=\frac{e^{-(w\cdot x)}}{1+e^{-(w\cdot x)}}
\end{aligned}
$$</p>
<p>这部分提到了几率，但是怎么就想到几率了。</p>
<p>之前一直不清楚为什么就联想到几率了, 从哪里建立了这种联系. 直到看了Think Bayes[^3].</p>
<blockquote>
<p>One way to represent a probability is with a number between 0 and 1, <strong>but
that’s not the only way</strong>. If you have ever bet on a <strong>football game or a horse
race</strong>, you have probably encountered another representation of probability,
called odds</p>
</blockquote>
<p>这本书有中文版,  希望这部分内容的补充能增加一些博彩业的直觉...</p>
<p>写到这里，突然想到一个人: 吴军博士。不记得数学之美中关于LR是如何描述的, 但是觉得能外延阐述几率和概率的这种联系的内容也许会出现在他的某部作品里。于是翻了数学之美。但，并没有。</p>
<p>数学之美中有这样一个公式
$$
f(z)=\color{red}\frac{e<sup>z}{e</sup>z+1}\color{black}=\frac{1}{1+e^{-z}}
$$
然后几率和概率之间的关系有这样一种表达
$$
o=\frac{p}{1-p} \nonumber\
\color{red}p=\frac{o}{1+o}
$$
看上面红色部分, <strong>逻辑斯谛分布</strong>对应了一种<strong>概率</strong>, <strong>几率</strong>为指数形式 <span><span class="MathJax_Preview">e^z</span><script type="math/tex">e^z</script></span>,  <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> 为<strong>对数几率</strong><span><span class="MathJax_Preview">logit</span><script type="math/tex">logit</script></span>.</p>
<p>$$
logit(p)=\log(o)=\log\frac{p}{1-p}
$$
上面是对数几率的定义， 这里对应了事件， 要么发生， 要么不发生。所以逻辑斯谛回归模型就表示成
$$
\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=\color{red}\log\frac{P(Y=1|x)}{P(Y=0|x)}\color{black}=w\cdot x
$$
上面红色部分留一下，后面推广到多类时候用到。</p>
<h4 id="_8">多项逻辑斯谛回归<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<p>假设离散型随机变量<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>的取值集合是<span><span class="MathJax_Preview">{1,2,\dots,K}</span><script type="math/tex">{1,2,\dots,K}</script></span>, 多项逻辑斯谛回归模型是
$$
\begin{aligned}
P(Y=k|x)&amp;=\frac{\exp(w_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}, k=1,2,\dots,K-1\
P(Y=K|x)&amp;=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}\
\end{aligned}
$$
下面看这个多分类模型怎么来的[^4]。</p>
<h5 id="_9">二元推广<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h5>
<p>计算<span><span class="MathJax_Preview">K-1</span><script type="math/tex">K-1</script></span>种可能的取值发生的概率相对取值<span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>发生的概率的比值， 假设其取对数的结果是<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的线性模型， 有
$$
\begin{aligned}
\ln\frac{P(Y=1|x)}{P(Y=K|x)}&amp;=w_1\cdot x\
\ln\frac{P(Y=2|x)}{P(Y=K|x)}&amp;=w_2\cdot x\
\cdots\
\ln\frac{P(Y=K-1|x)}{P(Y=K|x)}&amp;=w_{K-1}\cdot x\
\end{aligned}
$$
得到取值<span><span class="MathJax_Preview">{1,2，\dots,K-1}</span><script type="math/tex">{1,2，\dots,K-1}</script></span>的概率表示
$$
\begin{aligned}
{P(Y=1|x)}&amp;={P(Y=K|x)}\exp(w_1\cdot x)\
{P(Y=2|x)}&amp;={P(Y=K|x)}\exp(w_2\cdot x)\
\cdots\
{P(Y=K-1|x)}&amp;={P(Y=K|x)}\exp(w_{K-1}\cdot x)\
\color{red}{P(Y=k|x)}&amp;\color{red}={P(Y=K|x)}\exp(w_k\cdot x), k=1,2,\dots,K-1
\end{aligned}
$$
上面红色部分有点像书上的(6.7)， 又有<span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>种可能取值概率和为1，可以得到下面推导
$$
\begin{aligned}
P(Y=K|x)&amp;=1-\sum_{j=1}<sup>{K-1}P(Y=j|x)\
&amp;=1-P(Y=K|x)\sum_{j=1}</sup>{K-1}\exp(w_j\cdot x)\
&amp;=\frac{1}{1+\sum_{j=1}^{K-1}\exp(w_j\cdot x)}
\end{aligned}
$$
所以之前红色部分的表达可以表示为
$$
\begin{aligned}
\color{red}{P(Y=k|x)}&amp;\color{red}={P(Y=K|x)}\exp(w_k\cdot x), k=1,2,\dots,K-1\
&amp;=\frac{1}{1+\sum_{j=1}^{K-1}\exp(w_j\cdot x)}\exp(w_k\cdot x), k=1,2,\dots,K-1\
&amp;=\frac{\exp(w_k\cdot x)}{1+\sum_{j=1}^{K-1}\exp(w_j\cdot x)}, k=1,2,\dots,K-1\
\end{aligned}
$$</p>
<p>这里公式和书上有点区别， 求和的用了<span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>表示， 感觉不太容易造成误解。</p>
<h5 id="_10">对数线性模型<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h5>
<p>假设归一化因子<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>， 有如下关系
$$
\begin{aligned}
\ln (ZP(Y=k|x))&amp;=w_k\cdot x, k=1,2,\dots,K\
P(Y=k|x)&amp;=\frac{1}{Z}\exp(w_k\cdot x), k=1,2,\dots,K
\end{aligned}
$$
又对所有的<span><span class="MathJax_Preview">P(Y=k|x)</span><script type="math/tex">P(Y=k|x)</script></span>可以形成概率分布， 有
$$
\begin{aligned}
\sum_{k=1}<sup>KP(Y=k|x)&amp;=1\
&amp;=\sum_{k=1}</sup>K\frac{1}{Z}\exp(w_k\cdot x)\
&amp;=\frac{1}{Z}\sum_{k=1}^K\exp(w_k\cdot x)
\end{aligned}
$$
得到
$$
Z=\sum_{k=1}^K\exp(w_k\cdot x)
$$
所以
$$
P(Y=k|x)=\frac{1}{Z}\exp(w_k\cdot x)=\frac{\exp(w_k\cdot x)}{\sum_{k=1}^K\exp(w_k\cdot x)}, k=1,2,\dots,K
$$
上面这个叫Softmax，针对多项的情况也叫Softmax Regression。</p>
<h4 id="_11">模型参数估计<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h4>
<p>通过监督学习的方法来估计模型参数[这部分不完整]。</p>
<h5 id="logistic-regression">Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permanent link">&para;</a></h5>
<p>参数估计这里， 似然函数书中的表达
$$
\prod<sup>N_{i=1}[\pi(x_i)]</sup>{y_i}[1-\pi(x_i)]^{1-y_i}
$$
这里利用了<span><span class="MathJax_Preview">y_i\in\{0,1\}</span><script type="math/tex">y_i\in\{0,1\}</script></span>这个特点</p>
<p>更一般的表达
$$
\prod_{i=1}^NP(y_i|x_i,W)
$$
使用对数似然会更简单， 会将上面表达式的连乘形式会转换成求和形式。对数函数为单调递增函数， 最大化对数似然等价于最大化似然函数。
$$
\begin{aligned}
\log \prod_{i=1}<sup>N[\pi(x_i)]</sup>{y_i}[1-\pi(x_i)]<sup>{1-y_i}&amp;=\sum_{i=1}</sup>N[y_i\log(\pi(x_i))+(1-y_i)\log(1-\pi(x_i))]\
&amp;=\sum_{i=1}<sup>N[y_i\log(\frac{\pi(x_i)}{1-\pi(x_i)})+\log(1-\pi(x_i))]\
&amp;=\sum_{i=1}</sup>N[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i))]
\end{aligned}
$$
好像不用这样麻烦，似然函数表示为
$$
\prod\limits_{i=1}\limits<sup>NP(y_i|x_i,W)=\prod\limits_{i=1}\limits</sup>N\frac{(\exp(w\cdot x_i))^{y_i}}{1+\exp(w\cdot x_i)}
$$
使用对数技巧
$$
\sum_{i=1}^N\log\frac{\exp(w\cdot x_i)}{1+\exp(w\cdot x_i)}=\sum_{i=1}^N[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i))]
$$</p>
<h5 id="softmax-regression">Softmax Regression<a class="headerlink" href="#softmax-regression" title="Permanent link">&para;</a></h5>
<p>多类分类的情况这个表达式是什么样的？感觉不能用0，1这样的技巧了。
$$
\prod\limits_{i=1}\limits<sup>NP(y_i|x_i,W)=\prod\limits_{i=1}\limits</sup>N\prod\limits_{l=1}^K \left(\frac{\exp(w_k\cdot x_i)}{\sum_{k=1}^K\exp(w_k\cdot x_i)}\right)^{I(y_i=l)}
$$
但是可以用指示函数。</p>
<h3 id="_12">最大熵模型<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h3>
<h4 id="_13">概念<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h4>
<p>逻辑斯谛回归模型和最大熵模型，既可以看作是概率模型，又可以看作是非概率模型。</p>
<h5 id="_14">信息量<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h5>
<p>信息量是对信息的度量, PRML中有关于信息量的讨论,  信息是概率的单调函数.</p>
<p><span><span class="MathJax_Preview">h(x)=-\log_2{p(x)}</span><script type="math/tex">h(x)=-\log_2{p(x)}</script></span>, 符号保证了非负性. 低概率事件对应了高的信息量. 对数底选择是任意的, 信息论里面常用2, 单位是比特.</p>
<ul>
<li>信息和概率的关系参考PRML中1.6节信息论部分的描述.</li>
</ul>
<blockquote>
<p>如果我们知道某件事件一定会发生, 那么我们就不会接收到信息.
于是, 我们对于信息内容的度量将依赖于概率分布<span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span></p>
<p>如果我们有两个不相关的事件x, y, 那么我们观察到两个事件同时发生时获得的信息应该等于观察到事件各自发生时获得的信息之和, 即<span><span class="MathJax_Preview">h(x,y)=h(x)+h(y)</span><script type="math/tex">h(x,y)=h(x)+h(y)</script></span>,  这两个不相关的事件是独立的, 因此<span><span class="MathJax_Preview">p(x,y)=p(x)p(y)</span><script type="math/tex">p(x,y)=p(x)p(y)</script></span></p>
<p><strong>根据这两个关系</strong>, 很容易看出<span><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span>一定与<span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>的对数有关. 所以有</p>
<div>
<div class="MathJax_Preview">h(x)=-\log_2{p(x)}=\log_2{\frac{1}{p(x)}}</div>
<script type="math/tex; mode=display">h(x)=-\log_2{p(x)}=\log_2{\frac{1}{p(x)}}</script>
</div>
<ul>
<li>负号确保了信息非负</li>
<li>低概率事件<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>对应了高的信息.</li>
</ul>
</blockquote>
<h5 id="_15">熵和概率<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h5>
<p>熵可以从随机变量状态需要的平均信息量角度理解, 也可以从描述统计力学中无序程度的度量角度理解.</p>
<p>关于熵, 条件熵, 互信息, 这些内容在<a href="../CH05/">第五章</a>5.2节有对应的描述.</p>
<blockquote>
<p>下面看下信息熵在PRML中的表达</p>
<p>假设一个发送者想传输一个随机变量<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的值给接受者. 在这个过程中, 他们传输的平均信息量可以通过求<strong>信息<span><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span>关于概率分布<span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>的期望</strong>得到.</p>
<p>这个重要的量叫做随机变量<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的熵</p>
</blockquote>
<p><strong>Venn图</strong>辅助理解和记忆, 这个暂时不画, 下面考虑下为什么Venn图能帮助理解和记忆?</p>
<p>因为熵的定义把连乘变成了求和, 对数的贡献. 这样可以通过集合的交并来实现熵之间关系的理解.</p>
<ol>
<li>
<p><strong>概率</strong> <span><span class="MathJax_Preview">\sum _{i=1}^{n}{p_i=1}</span><script type="math/tex">\sum _{i=1}^{n}{p_i=1}</script></span> <span><span class="MathJax_Preview">p \in [0,1]</span><script type="math/tex">p \in [0,1]</script></span></p>
</li>
<li>
<p><strong>熵</strong><span><span class="MathJax_Preview">Ent(D) \in [0, \log_2{|\mathcal Y|}]</span><script type="math/tex">Ent(D) \in [0, \log_2{|\mathcal Y|}]</script></span>, 熵可以大于1。熵是传输一个随机变量状态值所需的比特位<strong>下界</strong>(信息论角度的理解)，也叫香农下界。</p>
</li>
<li>
<p><strong>信息熵</strong>是度量样本集合纯度最常用的一种指标。</p>
</li>
</ol>
<p><span><span class="MathJax_Preview">Ent(D)=-\sum \limits ^{|\mathcal Y|}_{k=1}p_k\log_2{p_k}</span><script type="math/tex">Ent(D)=-\sum \limits ^{|\mathcal Y|}_{k=1}p_k\log_2{p_k}</script></span></p>
<ul>
<li>if <span><span class="MathJax_Preview">p=0</span><script type="math/tex">p=0</script></span>，then <span><span class="MathJax_Preview">p\log_2{p}=0</span><script type="math/tex">p\log_2{p}=0</script></span></li>
<li><span><span class="MathJax_Preview">Ent(D)</span><script type="math/tex">Ent(D)</script></span>越小，D的纯度越高。非均匀分布比均匀分布熵要小。</li>
<li>
<p>熵衡量的是不确定性，概率描述的是确定性，其实确定性和不确定性差不多。</p>
</li>
<li>
<p><strong>联合熵(相当于并集)</strong></p>
</li>
</ul>
<p><span><span class="MathJax_Preview">H(X, Y) = H(X) + H(Y|X) = H(Y)+H(X|Y) = H(X|Y)+H(Y|X)+I(X;Y)</span><script type="math/tex">H(X, Y) = H(X) + H(Y|X) = H(Y)+H(X|Y) = H(X|Y)+H(Y|X)+I(X;Y)</script></span></p>
<p>这个通过Venn应该是相对容易记忆，是不是容易理解这个。</p>
<p>如果<span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>和<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>独立同分布，联合概率分布<span><span class="MathJax_Preview">P(X,Y)=P(X)P(Y)</span><script type="math/tex">P(X,Y)=P(X)P(Y)</script></span> </p>
<ol>
<li><strong>条件熵</strong></li>
</ol>
<p>条件熵是最大熵原理提出的基础，最大的是条件熵，这个在书中有写(定义6.3)</p>
<p>条件熵衡量了条件概率分布的均匀性</p>
<p>最大熵，就是最大这个<strong>条件熵</strong></p>
<p>find </p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>$$\begin{aligned}
</pre></div>
</td></tr></table>

<p>p^*&amp;=\arg\max\limits_{p\in \mathcal C}H(p)\
   &amp;=\arg \max\limits_{p\in \mathcal C}(-\sum\limits_{x,y} {\tilde p(x)p(y|x)\log p(y|x) })
   \end{aligned} $$</p>
<p>接下来的概念，把熵的思想应用在模式识别问题中。</p>
<ol>
<li><strong>互信息</strong></li>
</ol>
<p>互信息(mutual information)，对应熵里面的交集，常用来描述差异性</p>
<p>一般的，熵<span><span class="MathJax_Preview">H(Y)</span><script type="math/tex">H(Y)</script></span>与条件熵<span><span class="MathJax_Preview">H(Y|X)</span><script type="math/tex">H(Y|X)</script></span>之差称为互信息。注意一下，这里<a href="../CH05/">第五章</a>中用到了<span><span class="MathJax_Preview">H(D, A)</span><script type="math/tex">H(D, A)</script></span> 可以对应理解下。</p>
<ol>
<li>Feature Selection </li>
<li>
<p>Feature Correlation，刻画的是相互之间的关系。 <strong>相关性主要刻画线性，互信息刻画非线性</strong></p>
</li>
<li>
<p><strong>信息增益</strong></p>
</li>
</ol>
<p>这个对应的是第五章的内容，决策树学习应用信息增益准则选择特征。
   $$
   g(D,A)=H(D)-H(D|A)
   $$
   信息增益表示得知<span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>的信息而使类<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>的信息的不确定性减少的程度。</p>
<p>在决策树学习中，信息增益等价于训练数据集中类与特征的互信息。</p>
<ol>
<li><strong>相对熵 (KL 散度)</strong> </li>
</ol>
<p>相对熵(Relative Entropy)描述差异性，从分布的角度描述差异性，可用于度量两个概率分布之间的差异。</p>
<p>KL散度不是一个度量，度量要满足交换性。</p>
<p>KL散度满足非负性。</p>
<blockquote>
<p>考虑由<span><span class="MathJax_Preview">p(x,y)</span><script type="math/tex">p(x,y)</script></span>给出的两个变量<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>组成的数据集。如果变量的集合是独立的，那么他们的联合分布可以分解为边缘分布的乘积<span><span class="MathJax_Preview">p(x,y)=p(x)p(y)</span><script type="math/tex">p(x,y)=p(x)p(y)</script></span></p>
<p>如果变量不是独立的，那么我们可以通过考察<strong>联合分布</strong>与<strong>边缘分布乘积</strong>之间的KL散度来判断他们是否"接近"于相互独立。</p>
<div>
<div class="MathJax_Preview">I(x,y)=KL(p(x,y)|p(x)p(y))=-\iint p(x,y) \ln {\left( \frac{p(x)p(y)}{p(x,y)}\right)}</div>
<script type="math/tex; mode=display">I(x,y)=KL(p(x,y)|p(x)p(y))=-\iint p(x,y) \ln {\left( \frac{p(x)p(y)}{p(x,y)}\right)}</script>
</div>
<p>这被称为变量<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和变量<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>之间的互信息.</p>
<p>--PRML 1.6.1</p>
</blockquote>
<p>注意这里，参考下<a href="../CH05/">第五章</a>中关于互信息的描述</p>
<blockquote>
<p>决策树学习中的信息增益等价于训练数据集中<strong>类</strong>与<strong>特征</strong>的互信息</p>
</blockquote>
<p>注意这里面类<span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>， 特征<span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>。</p>
<p>互信息和条件熵之间的关系
   $$
   I(x,y)=H(X)-H(x|y)=H(y)-H(y|x)
   $$
   可以把互信息看成由于知道<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>值而造成的<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的不确定性的减小(反之亦然)。<em>这个就是信息增益那部分的解释</em>。</p>
<ol>
<li><strong>交叉熵</strong></li>
</ol>
<p>刻画两个分布之间的差异
   $$
   \begin{aligned}
   CH(p,q)&amp;=-\sum\limits_{i=1}^{n}p(x_i)\log{q(x_i)}\
   &amp;=-\sum\limits_{i=1}<sup>{n}p(x_i)\log{p(x_i)}+\sum\limits_{i=1}</sup>{n}p(x_i)\log{p(x_i)}-\sum\limits_{i=1}^{n}p(x_i)\log{q(x_i)}\
   &amp;=H(p)+\sum\limits_{i=1}^{n}p(x_i)\log{\frac{p(x_i)}{q(x_i)}}\
   &amp;=H(p)+KL(p||q)
   \end{aligned}
   $$</p>
<p>CNN时候常用</p>
<p>对于各种熵的理解，是构建后面的目标函数的基础。</p>
<h5 id="_16">最大熵原理<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h5>
<p>最大熵原理(Maxent principle)是<strong>概率模型</strong>学习的一个准则。</p>
<p>书中通过一个例子来介绍最大熵原理，下面引用一下文献中关于这个例子的总结。</p>
<blockquote>
<p>Model all that is known and assume nothing about that which is unknown. In other words, given a collection of facts, choose a model which is consistent with all the facts, but otherwise as uniform as possible.</p>
<p>-- Berger, 1996</p>
</blockquote>
<p>书中关于这部分的总结如下：<strong>满足约束条件下求等概率的方法估计概率分布</strong></p>
<p>关于最大熵原理有很多直观容易理解的解释，比如Berger的例子，比如吴军老师数学之美中的例子。</p>
<p>最大熵原理很常<strong>见</strong>，很多原理我们都一直在用，只是没有上升到理论的高度。</p>
<p>等概率表示了对事实的无知，因为没有更多的信息，这种判断是合理的。</p>
<p>最大熵原理认为要选择的概率模型首先必须满足<strong>已有的事实</strong>，即<strong>约束条件</strong></p>
<p>最大熵原理根据已有的信息（<strong>约束条件</strong>），选择适当的概率模型。</p>
<p>最大熵原理认为不确定的部分都是等可能的，通过熵的最大化来表示<strong>等可能性</strong>。</p>
<p>最大熵的原则，承认已有的，且对未知无偏</p>
<p>最大熵原理并不直接关心特征选择，但是特征选择是非常重要的，因为约束可能是成千上万的。</p>
<h5 id="_17">最大熵原理几何解释<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h5>
<p>这部分书中只描述了模型空间<span><span class="MathJax_Preview">\mathcal P</span><script type="math/tex">\mathcal P</script></span>，两个约束<span><span class="MathJax_Preview">C_1</span><script type="math/tex">C_1</script></span>和<span><span class="MathJax_Preview">C_2</span><script type="math/tex">C_2</script></span>是<strong>一致性</strong>约束的情况。</p>
<p>在Berger 1996里面有展开这部分，分了四个图，分别讨论了</p>
<ol>
<li>概率模型空间<span><span class="MathJax_Preview">\mathcal {P}</span><script type="math/tex">\mathcal {P}</script></span></li>
<li>单一约束<span><span class="MathJax_Preview">C_1</span><script type="math/tex">C_1</script></span></li>
<li>一致性(consistent)约束<span><span class="MathJax_Preview">C_1</span><script type="math/tex">C_1</script></span>和<span><span class="MathJax_Preview">C_2</span><script type="math/tex">C_2</script></span>，这种情况下模型唯一确定<span><span class="MathJax_Preview">p=C_1\bigcap C_2</span><script type="math/tex">p=C_1\bigcap C_2</script></span></li>
<li>非一致性(inconsistent)约束<span><span class="MathJax_Preview">C_1</span><script type="math/tex">C_1</script></span>和<span><span class="MathJax_Preview">C_3</span><script type="math/tex">C_3</script></span>，这种情况下没有满足约束条件的模型。</li>
</ol>
<h5 id="_18">特征与约束条件<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h5>
<p>关于特征和约束，Berger有他的阐述</p>
<blockquote>
<p>指示函数</p>
<p><span><span class="MathJax_Preview">f(x,y)=\begin{cases}
1 &amp; if\ y=en\ and\ April\ follows\ in\\
0 &amp; otherwise
\end{cases}</span><script type="math/tex">f(x,y)=\begin{cases}
1 & if\ y=en\ and\ April\ follows\ in\\
0 & otherwise
\end{cases}</script></span></p>
</blockquote>
<p>上面这个<span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>直接引用自Berger的说明，原来的例子是英语in到法语的翻译。</p>
<p>这里面f就是<strong>特征函数</strong>，或者<strong>特征</strong>。</p>
<p>定义一个期望，如果是二值函数的话，就相当于计数。通过样本得到的这个统计。但是样本是有限的，并不是一个真实的分布， 所以叫经验分布，如果我们拿到的这个模型能够表示实际的分布，那么就可以假设经验分布和真实分布是相等的。这个，就是<strong>约束方程</strong>，或者<strong>约束</strong>。</p>
<p>一般模型的<strong>特征</strong>是关于<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的函数，最大熵模型中的特征函数，是关于<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>的函数。注意理解<span><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span>与<span><span class="MathJax_Preview">f(x, y)</span><script type="math/tex">f(x, y)</script></span>的区别。 关于特征函数可以参考<a href="../CH11/">条件随机场</a>中例11.1关于特征部分的内容增强理解。</p>
<h5 id="_19">模型<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h5>
<p>假设分类模型是一个条件概率分布，<span><span class="MathJax_Preview">P(Y|X)</span><script type="math/tex">P(Y|X)</script></span>, <span><span class="MathJax_Preview">X\in \mathcal {X} \sube \mathbf R^n</span><script type="math/tex">X\in \mathcal {X} \sube \mathbf R^n</script></span></p>
<p>给定一个训练集 <span><span class="MathJax_Preview">T=\{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}</span><script type="math/tex">T=\{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}</script></span></p>
<p><span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>是训练样本容量，<span><span class="MathJax_Preview">x \in \mathbf R^n</span><script type="math/tex">x \in \mathbf R^n</script></span> </p>
<p>联合分布<span><span class="MathJax_Preview">P(X,Y)</span><script type="math/tex">P(X,Y)</script></span>与边缘分布P(X)的经验分布分别为<span><span class="MathJax_Preview">\widetilde P(X, Y)和\widetilde P(X)</span><script type="math/tex">\widetilde P(X, Y)和\widetilde P(X)</script></span></p>
<p>$$
\begin{aligned}
&amp;\widetilde P (X=x, Y=y)=\frac{\nu(X=x, Y=y)}{N} \
&amp;\widetilde P (X=x)=\frac {\nu (X=x)}{N}
\end{aligned}
$$
上面两个就是不同的数据样本，在训练数据集中的比例。</p>
<p>如果增加<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个<strong>特征函数</strong>, 就可以增加<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个<strong>约束条件</strong>，特征也对应增加了一列。</p>
<p>假设满足所有约束条件的模型集合为</p>
<p>$\mathcal {C} \equiv  {P \in \mathcal {P}|E_P(f_i)=E_{\widetilde {P}}(f_i) {, i=1,2,\dots,n}} $</p>
<p>定义在条件概率分布<span><span class="MathJax_Preview">P(Y|X)</span><script type="math/tex">P(Y|X)</script></span>上的条件熵为</p>
<p><span><span class="MathJax_Preview">H(P)=-\sum \limits _{x, y} \widetilde {P}(x)P(y|x)\log {P(y|x)}</span><script type="math/tex">H(P)=-\sum \limits _{x, y} \widetilde {P}(x)P(y|x)\log {P(y|x)}</script></span></p>
<p>则模型集合<span><span class="MathJax_Preview">\cal {C}</span><script type="math/tex">\cal {C}</script></span>中条件熵<span><span class="MathJax_Preview">H(P)</span><script type="math/tex">H(P)</script></span>最大的模型称为最大熵模型，上式中对数为自然对数。</p>
<p>特征函数<span><span class="MathJax_Preview">f(x,y)</span><script type="math/tex">f(x,y)</script></span>关于经验分布<span><span class="MathJax_Preview">\widetilde P (X, Y)</span><script type="math/tex">\widetilde P (X, Y)</script></span>的期望值用<span><span class="MathJax_Preview">E_{\widetilde P}(f)</span><script type="math/tex">E_{\widetilde P}(f)</script></span>表示</p>
<div>
<div class="MathJax_Preview">E_{\widetilde P}(f)=\sum\limits_{x,y}\widetilde P(x,y)f(x,y)</div>
<script type="math/tex; mode=display">E_{\widetilde P}(f)=\sum\limits_{x,y}\widetilde P(x,y)f(x,y)</script>
</div>
<p>特征函数<span><span class="MathJax_Preview">f(x,y)</span><script type="math/tex">f(x,y)</script></span>关于模型<span><span class="MathJax_Preview">P(Y|X)</span><script type="math/tex">P(Y|X)</script></span>与经验分布<span><span class="MathJax_Preview">\widetilde P (X)</span><script type="math/tex">\widetilde P (X)</script></span>的期望值, 用<span><span class="MathJax_Preview">E_{P}(f)</span><script type="math/tex">E_{P}(f)</script></span>表示</p>
<div>
<div class="MathJax_Preview">E_{P}(f)=\sum\limits_{x,y}{\widetilde P(x)P(y|x)f(x,y)}</div>
<script type="math/tex; mode=display">E_{P}(f)=\sum\limits_{x,y}{\widetilde P(x)P(y|x)f(x,y)}</script>
</div>
<p>如果模型能够获取训练数据中的信息，那么就有
$$
\widetilde{P}(x,y)=P(y|x)\widetilde{P}(x)
$$
就可以假设这两个期望值相等，即</p>
<div>
<div class="MathJax_Preview">E_P(f)=E_{\widetilde P}(f)</div>
<script type="math/tex; mode=display">E_P(f)=E_{\widetilde P}(f)</script>
</div>
<p>上面这个也是约束方程</p>
<h4 id="_20">算法实现<a class="headerlink" href="#_20" title="Permanent link">&para;</a></h4>
<h5 id="_21">特征提取原理<a class="headerlink" href="#_21" title="Permanent link">&para;</a></h5>
<p>通过对已知训练集数据的分析，能够拿到联合分布的经验分布和边缘分布的经验分布。</p>
<p>特征函数用来描述<span><span class="MathJax_Preview">f(x, y)</span><script type="math/tex">f(x, y)</script></span>描述输入<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和输出<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>之间的某一事实。
$$
f(x,y) = \begin{cases}
1 &amp; x与y满足某一事实\
0 &amp; 否则
\end{cases}
$$</p>
<p>这里，满足的事实，可以是in，显然，特征函数可以自己定义，可以定义多个，<del>这些就是约束</del></p>
<p>之前理解的不对，看前面有描述特征和约束的关系。</p>
<h5 id="_22">预测分类原理<a class="headerlink" href="#_22" title="Permanent link">&para;</a></h5>
<p>这里面重复一下书中的过程，在<span><span class="MathJax_Preview">L(P, w)</span><script type="math/tex">L(P, w)</script></span>对<span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span>求导并令其为零的情况下解方程能拿到下面公式
$$
P(y|x)=\exp{\left(\sum_{i=1}<sup>{n}w_if_i(x,y)+w_0-1\right)}=\frac{\exp{\left(\sum\limits_{i=1}</sup>{n}w_if_i(x,y)\right)}}{\exp{\left(1-w_0\right)}}
$$
书中有提到因为<span><span class="MathJax_Preview">\sum\limits{_y}P(y|x)=1</span><script type="math/tex">\sum\limits{_y}P(y|x)=1</script></span>，然后得到模型</p>
<div>
<div class="MathJax_Preview">
P_w(y|x)=\frac{1}{Z_w(x)}\exp{\sum\limits_{i=1}^{n}w_if_i(x,y)}\\
Z_w(x)=\sum_y\exp{\sum_{i=1}^{n}w_if_i(x,y)}
</div>
<script type="math/tex; mode=display">
P_w(y|x)=\frac{1}{Z_w(x)}\exp{\sum\limits_{i=1}^{n}w_if_i(x,y)}\\
Z_w(x)=\sum_y\exp{\sum_{i=1}^{n}w_if_i(x,y)}
</script>
</div>
<p>注意这里面<span><span class="MathJax_Preview">Z_w</span><script type="math/tex">Z_w</script></span>是归一化因子。</p>
<p>这里面并不是因为概率为1推导出了<span><span class="MathJax_Preview">Z_w</span><script type="math/tex">Z_w</script></span>的表达式，而是因为<span><span class="MathJax_Preview">Z_w</span><script type="math/tex">Z_w</script></span>的位置在分母，然后对应位置<span><span class="MathJax_Preview">\exp(1-w_0)</span><script type="math/tex">\exp(1-w_0)</script></span>也在分母，凑出来这样一个表达式，意思就是遍历<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>的所有取值，求分子表达式的占比。</p>
<p>综上，如果<span><span class="MathJax_Preview">f_i(x,y)</span><script type="math/tex">f_i(x,y)</script></span>只检测是不是存在这种组合，那么概率就是<em>归一化的</em>出现过的特征，系数求和再取e指数。</p>
<h4 id="_23">最大熵模型的学习<a class="headerlink" href="#_23" title="Permanent link">&para;</a></h4>
<p>最大熵模型的学习过程就是求解最大熵模型的过程。</p>
<p>最大熵模型的学习可以形式化为约束最优化问题。
$$
\begin{eqnarray<em>}
\min \limits_{P\in \mathcal {C}}-H(P)=\sum\limits_{x,y}\widetilde P(x)P(y|x)\log P(y|x)\tag{6.14}\
s.t. E_P(f_i)-E_{\widetilde P}(f_i)=0, i =1,2,\dots,n\tag{6.15}\
\sum \limits_y P(y|x)=1\tag{6.16}
\end{eqnarray</em>}
$$</p>
<p>可以通过例6.2 来理解最大熵模型学习的过程，例6.2 考虑了两种约束条件，这部分内容可以通过python符号推导实现，下面代码整理整个求解过程。</p>
<h5 id="62">例6.2<a class="headerlink" href="#62" title="Permanent link">&para;</a></h5>
<h6 id="_24">一个约束条件<a class="headerlink" href="#_24" title="Permanent link">&para;</a></h6>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sympy</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># 1 constrains</span>
<span class="n">P1</span><span class="p">,</span> <span class="n">P2</span><span class="p">,</span> <span class="n">P3</span><span class="p">,</span> <span class="n">P4</span><span class="p">,</span> <span class="n">P5</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">symbols</span><span class="p">(</span><span class="s2">&quot;P1, P2, P3, P4, P5, w0, w1, w2&quot;</span><span class="p">,</span> <span class="n">real</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">P1</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span> <span class="o">+</span> <span class="n">P2</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">P2</span><span class="p">)</span> <span class="o">+</span> <span class="n">P3</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">P3</span><span class="p">)</span> <span class="o">+</span> <span class="n">P4</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">P4</span><span class="p">)</span> <span class="o">+</span> <span class="n">P5</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">P5</span><span class="p">)</span> \
    <span class="o">+</span> <span class="n">w0</span> <span class="o">*</span> <span class="p">(</span><span class="n">P1</span> <span class="o">+</span> <span class="n">P2</span> <span class="o">+</span> <span class="n">P3</span> <span class="o">+</span> <span class="n">P4</span> <span class="o">+</span> <span class="n">P5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">P1_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">P1</span><span class="p">),</span> <span class="n">P1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P2_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">P2</span><span class="p">),</span> <span class="n">P2</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P3_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">P3</span><span class="p">),</span> <span class="n">P3</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P4_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">P4</span><span class="p">),</span> <span class="n">P4</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P5_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">P5</span><span class="p">),</span> <span class="n">P5</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">P1</span><span class="p">:</span> <span class="n">P1_e</span><span class="p">,</span> <span class="n">P2</span><span class="p">:</span> <span class="n">P2_e</span><span class="p">,</span> <span class="n">P3</span><span class="p">:</span> <span class="n">P3_e</span><span class="p">,</span> <span class="n">P4</span><span class="p">:</span> <span class="n">P4_e</span><span class="p">,</span> <span class="n">P5</span><span class="p">:</span> <span class="n">P5_e</span><span class="p">})</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">([</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">w0</span><span class="p">)],</span> <span class="p">[</span><span class="n">w0</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P</span> <span class="o">=</span> <span class="p">[</span><span class="n">P1_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]}),</span>
     <span class="n">P2_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]}),</span>
     <span class="n">P3_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]}),</span>
     <span class="n">P4_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]}),</span>
     <span class="n">P5_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]})]</span>
<span class="n">P</span>
</pre></div>
</td></tr></table>

<h6 id="_25">两个约束条件<a class="headerlink" href="#_25" title="Permanent link">&para;</a></h6>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># 2 constrains</span>
<span class="n">P1</span><span class="p">,</span> <span class="n">P2</span><span class="p">,</span> <span class="n">P3</span><span class="p">,</span> <span class="n">P4</span><span class="p">,</span> <span class="n">P5</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">symbols</span><span class="p">(</span><span class="s2">&quot;P1, P2, P3, P4, P5, w0, w1, w2&quot;</span><span class="p">,</span><span class="n">real</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">P1</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span> <span class="o">+</span> <span class="n">P2</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P2</span><span class="p">)</span><span class="o">+</span><span class="n">P3</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P3</span><span class="p">)</span><span class="o">+</span><span class="n">P4</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P4</span><span class="p">)</span><span class="o">+</span><span class="n">P5</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P5</span><span class="p">)</span>\
    <span class="o">+</span><span class="n">w1</span><span class="o">*</span><span class="p">(</span><span class="n">P1</span><span class="o">+</span><span class="n">P2</span><span class="o">-</span><span class="mi">3</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>\
    <span class="o">+</span><span class="n">w0</span><span class="o">*</span><span class="p">(</span><span class="n">P1</span><span class="o">+</span><span class="n">P2</span><span class="o">+</span><span class="n">P3</span><span class="o">+</span><span class="n">P4</span><span class="o">+</span><span class="n">P5</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">P1_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P1</span><span class="p">),</span><span class="n">P1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P2_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P2</span><span class="p">),</span><span class="n">P2</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P3_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P3</span><span class="p">),</span><span class="n">P3</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P4_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P4</span><span class="p">),</span><span class="n">P4</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P5_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P5</span><span class="p">),</span><span class="n">P5</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">P1</span><span class="p">:</span><span class="n">P1_e</span><span class="p">,</span> <span class="n">P2</span><span class="p">:</span><span class="n">P2_e</span><span class="p">,</span> <span class="n">P3</span><span class="p">:</span><span class="n">P3_e</span><span class="p">,</span> <span class="n">P4</span><span class="p">:</span><span class="n">P4_e</span><span class="p">,</span> <span class="n">P5</span><span class="p">:</span><span class="n">P5_e</span><span class="p">})</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">([</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">w1</span><span class="p">),</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">w0</span><span class="p">)],[</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P</span> <span class="o">=</span> <span class="p">[</span><span class="n">P1_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]}),</span>
     <span class="n">P2_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]}),</span>
     <span class="n">P3_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]}),</span>
     <span class="n">P4_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]}),</span>
     <span class="n">P5_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]})]</span>
<span class="n">P</span>
</pre></div>
</td></tr></table>

<h6 id="_26">三个约束条件<a class="headerlink" href="#_26" title="Permanent link">&para;</a></h6>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># 3 constrains</span>
<span class="n">P1</span><span class="p">,</span> <span class="n">P2</span><span class="p">,</span> <span class="n">P3</span><span class="p">,</span> <span class="n">P4</span><span class="p">,</span> <span class="n">P5</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">symbols</span><span class="p">(</span><span class="s2">&quot;P1, P2, P3, P4, P5, w0, w1, w2&quot;</span><span class="p">,</span><span class="n">real</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">P1</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span> <span class="o">+</span> <span class="n">P2</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P2</span><span class="p">)</span><span class="o">+</span><span class="n">P3</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P3</span><span class="p">)</span><span class="o">+</span><span class="n">P4</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P4</span><span class="p">)</span><span class="o">+</span><span class="n">P5</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">P5</span><span class="p">)</span>\
    <span class="o">+</span><span class="n">w2</span><span class="o">*</span><span class="p">(</span><span class="n">P1</span><span class="o">+</span><span class="n">P3</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>\
    <span class="o">+</span><span class="n">w1</span><span class="o">*</span><span class="p">(</span><span class="n">P1</span><span class="o">+</span><span class="n">P2</span><span class="o">-</span><span class="mi">3</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>\
    <span class="o">+</span><span class="n">w0</span><span class="o">*</span><span class="p">(</span><span class="n">P1</span><span class="o">+</span><span class="n">P2</span><span class="o">+</span><span class="n">P3</span><span class="o">+</span><span class="n">P4</span><span class="o">+</span><span class="n">P5</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">P1_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P1</span><span class="p">),</span><span class="n">P1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P2_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P2</span><span class="p">),</span><span class="n">P2</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P3_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P3</span><span class="p">),</span><span class="n">P3</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P4_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P4</span><span class="p">),</span><span class="n">P4</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P5_e</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">P5</span><span class="p">),</span><span class="n">P5</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">P1</span><span class="p">:</span><span class="n">P1_e</span><span class="p">,</span> <span class="n">P2</span><span class="p">:</span><span class="n">P2_e</span><span class="p">,</span> <span class="n">P3</span><span class="p">:</span><span class="n">P3_e</span><span class="p">,</span> <span class="n">P4</span><span class="p">:</span><span class="n">P4_e</span><span class="p">,</span> <span class="n">P5</span><span class="p">:</span><span class="n">P5_e</span><span class="p">})</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">(</span><span class="n">solve</span><span class="p">([</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">w2</span><span class="p">),</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">w1</span><span class="p">),</span><span class="n">diff</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">w0</span><span class="p">)],[</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">P</span> <span class="o">=</span> <span class="p">[</span><span class="n">P1_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">w2</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]}),</span>
     <span class="n">P2_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">w2</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]}),</span>
     <span class="n">P3_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">w2</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]}),</span>
     <span class="n">P4_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">w2</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]}),</span>
     <span class="n">P5_e</span><span class="o">.</span><span class="n">subs</span><span class="p">({</span><span class="n">w0</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w1</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">w2</span><span class="p">:</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]})]</span>
<span class="n">P</span>
</pre></div>
</td></tr></table>

<h2 id="_27">模型学习<a class="headerlink" href="#_27" title="Permanent link">&para;</a></h2>
<p>逻辑斯谛回归模型和最大熵模型学习归结为以<strong>似然函数</strong>为<strong>目标函数</strong>的最优化问题，通常通过迭代算法求解。</p>
<h3 id="_28">目标函数<a class="headerlink" href="#_28" title="Permanent link">&para;</a></h3>
<h4 id="_29">逻辑斯谛回归模型<a class="headerlink" href="#_29" title="Permanent link">&para;</a></h4>
<div>
<div class="MathJax_Preview">
\begin{aligned}
L(w)&amp;=\sum\limits^{N}_{i=1}[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
&amp;=\sum\limits^{N}_{i=1}[y_i\log{\frac{\pi(x_i)}{1-\pi(x_i)}}+\log(1-\pi(x_i))]\\
&amp;=\sum\limits^{N}_{i=1}[y_i(w\cdot x_i)-\log(1+\exp(w\cdot{x_i})]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
L(w)&=\sum\limits^{N}_{i=1}[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
&=\sum\limits^{N}_{i=1}[y_i\log{\frac{\pi(x_i)}{1-\pi(x_i)}}+\log(1-\pi(x_i))]\\
&=\sum\limits^{N}_{i=1}[y_i(w\cdot x_i)-\log(1+\exp(w\cdot{x_i})]
\end{aligned}
</script>
</div>
<h4 id="_30">最大熵模型<a class="headerlink" href="#_30" title="Permanent link">&para;</a></h4>
<div>
<div class="MathJax_Preview">
\begin{align}
L_{\widetilde {P}}(P_w)&amp;=\sum \limits_{x,y}\widetilde {P}(x,y)\log{P}(y|x)\\
&amp;=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x,y}\widetilde{P}(x,y)\log{(Z_w(x))}\\
&amp;=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x,y}\widetilde{P}(x)P(y|x)\log{(Z_w(x))}\\
&amp;=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x}\widetilde{P}(x)\log{(Z_w(x))}\sum_{y}P(y|x)\\
&amp;=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x}\widetilde{P}(x)\log{(Z_w(x))}
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
L_{\widetilde {P}}(P_w)&=\sum \limits_{x,y}\widetilde {P}(x,y)\log{P}(y|x)\\
&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x,y}\widetilde{P}(x,y)\log{(Z_w(x))}\\
&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x,y}\widetilde{P}(x)P(y|x)\log{(Z_w(x))}\\
&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x}\widetilde{P}(x)\log{(Z_w(x))}\sum_{y}P(y|x)\\
&=\sum \limits_{x,y}\widetilde {P}(x,y)\sum \limits_{i=1}^{n}w_if_i(x,y) -\sum \limits_{x}\widetilde{P}(x)\log{(Z_w(x))}
\end{align}
</script>
</div>
<p>以上推导用到了<span><span class="MathJax_Preview">\sum\limits_yP(y|x)=1</span><script type="math/tex">\sum\limits_yP(y|x)=1</script></span></p>
<ol>
<li>逻辑斯谛回归模型与朴素贝叶斯的关系</li>
</ol>
<p>这部分内容书中引用了参考文献[4]，这是Mitchell的那本《机器学习》，应该说的是第六章中相关的部分推导。</p>
<p>注意对应的部分还写了在神经网络中梯度搜索实现似然最大化，给出了结果与BP结果的差异，这部分可以看看。</p>
<ol>
<li>
<p>逻辑斯谛回归模型与AdaBoost的关系</p>
</li>
<li>
<p>逻辑斯谛回归模型与核函数的关系</p>
</li>
</ol>
<h3 id="_31">其他<a class="headerlink" href="#_31" title="Permanent link">&para;</a></h3>
<p>课后习题的第一个题目提到了<strong>指数族</strong>(Exponential family)分布，这个概念在PRML中有单独的章节进行阐述。</p>
<p>扩展一下指数族分布：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>正态分布</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>两点分布</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>二项分布</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>泊松分布</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>伽马分布</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>大部分的算法实现，其实都是在刷权重，记得有个同学的昵称就是“一切源于wx+b”。另外，<strong>其实算法能跑起来，能预测，并不能说明算法实现的正确的，或者说并不一定是最优的。</strong> 但是其实这种情况也比较常见， 有的时候没有条件去创造一个专门的工具来解决问题的时候，或者没有更好的工具解决问题的时候， 我们会选择能解决部分问题，或者能解决问题的工具来<strong>对付</strong></p>
<h2 id="_32">代码实现<a class="headerlink" href="#_32" title="Permanent link">&para;</a></h2>
<p>关于代码实现，网上看似众多的版本，应该基本上都源自最早15年的一份GIS的程序。 </p>
<p>无论怎样，这些代码的实现，都会有助于对Maxent的理解。推荐后面参考文献[1]</p>
<p>李航老师在本章给出的参考文献中[1, 2]是Berger的文章。</p>
<h3 id="demo">Demo<a class="headerlink" href="#demo" title="Permanent link">&para;</a></h3>
<p>这部分代码没有LR的说明。</p>
<blockquote>
<p>代码来源: <a href="https://vimsky.com/article/776.html">https://vimsky.com/article/776.html</a>
相关公式: <a href="https://vimsky.com/article/714.html">https://vimsky.com/article/714.html</a></p>
</blockquote>
<p>提几点:</p>
<ol>
<li>代码参考文献可以看berger的文章，公式编号基本对应。</li>
<li>这份代码用defaultdict实现了稀疏存储。</li>
<li>如果<span><span class="MathJax_Preview">f(x, y)</span><script type="math/tex">f(x, y)</script></span>只是判断<span><span class="MathJax_Preview">(x, y)</span><script type="math/tex">(x, y)</script></span>在特征中出现的指示函数，那么特征可以简单的表示为<span><span class="MathJax_Preview">(x, y)</span><script type="math/tex">(x, y)</script></span>，这样给定一份含有标签的数据集，特征的数量就是 <span><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> 其中<span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>是标签的数量，<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>是词表大小。注意书中注释过，<span><span class="MathJax_Preview">f(x,y)</span><script type="math/tex">f(x,y)</script></span>可以是任意实值函数。</li>
<li>这份代码思路很清晰，$(E_{\widetilde p}, Z_x \Rightarrow  P(y|x) \Rightarrow E_p) \Rightarrow \delta $，具体参考书中公式6.22，6.23，6.34</li>
<li>体会一下在做直方图的时候，对于同一个样本，同样的特征出现多次的贡献是一样的。</li>
<li>在未知的情况下，输出结果等概率。</li>
</ol>
<h3 id="maxent">Maxent<a class="headerlink" href="#maxent" title="Permanent link">&para;</a></h3>
<blockquote>
<p>参考链接: <a href="https://github.com/WenDesi/lihang_book_algorithm/tree/master/maxENT">https://github.com/WenDesi/lihang_book_algorithm/tree/master/maxENT</a></p>
</blockquote>
<p>本来是想在这个代码的基础上更改，但是代码分解的不是非常容易理解。改来改去基本上面目全非了。保留链接以示感谢。博主写了一系列代码，至少有个成体系的参考。</p>
<p>提几点:</p>
<ol>
<li>没有用稀疏存储，所以，矩阵中会有很多零。需要除零错误处理</li>
</ol>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
 <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">true_divide</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">EPxy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">EPx</span><span class="p">)</span>
    <span class="n">tmp</span><span class="p">[</span><span class="n">tmp</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>分子分母都是0对应nan，分母为0对应inf</p>
<ol>
<li>
<p>尝试了三种数据，可以通过命令行参数实现数据选择。</p>
</li>
<li>
<p>Demo中用到的data</p>
</li>
<li>train_binary.csv 这份数据的来源是参考链接中的，只考虑了0，1两种数据，标签少。</li>
<li>
<p>sklearn中的digits，标签全，但是8x8大小，比mnist少。其实8x8也不是一个非常小的图了，因为数字相对简单一点，用OpenCV做级联分类器的训练的时候，建议的图片大小是20x20，或者40x40，或者60x60不要太大</p>
</li>
<li>
<p>书中有一个地方还是不理解的，提到了<span><span class="MathJax_Preview">f^\#</span><script type="math/tex">f^\#</script></span>是不是常数的问题。</p>
</li>
<li>
<p>没有采用字典的方式实现稀疏存储，但是numpy的数组操作还是很便捷的，后面有空评估一下存储和计算资源的消耗情况。</p>
</li>
<li>
<p>大多数算法都是在刷权重，考虑哪些量(特征)可以用，哪些方法(算法)可以让权重刷的更合理，哪些方法(优化方法)能刷的更快。</p>
</li>
</ol>
<h3 id="mnist">Mnist<a class="headerlink" href="#mnist" title="Permanent link">&para;</a></h3>
<p>有同学问LR实现中的GD，才发现那段代码不是很好读。而且，用到的train.csv已不在。</p>
<p>加了一个mnist_sample.py从Lecun那里下载数据，并按照类别采样300条。用来完成LR的Demo。</p>
<p>有些程序的问题，配合数据来理解。通常用到label乘法都是利用了label的符号，或者one-hot之后为了取到对应的类别的值。</p>
<p>代码更新了下，建议运行logistic_regression.py的时候在注释的位置断点，看下各个数据的shape，希望对理解代码有帮助。</p>
<h2 id="_33">参考<a class="headerlink" href="#_33" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p><a href="https://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/tutorial.html">Berger,1995, A Brief Maxent Tutorial</a></p>
</li>
<li>
<p>[数学之美:信息的度量和作用]</p>
</li>
<li>
<p>[数学之美:不要把鸡蛋放在一个篮子里 谈谈最大熵模型]</p>
</li>
<li>
<p><a href="https://blog.csdn.net/tina_ttl/article/details/53542004">李航·统计学习方法笔记·第6章 logistic regression与最大熵模型（2）·最大熵模型</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/u014688145/article/details/55003910">最大熵模型与GIS ,IIS算法</a></p>
</li>
<li>
<p><a href="https://www.zhihu.com/question/49139674/answer/114670380">关于最大熵模型的严重困惑：为什么没有解析解？</a></p>
</li>
<li>
<p><a href="http://www.cnblogs.com/hexinuaa/p/3353479.html">最大熵模型介绍</a> 这个是Berger的文章的翻译.</p>
</li>
<li>
<p><a href="https://vimsky.com/article/714.html">理论简介</a>  <a href="https://vimsky.com/article/776.html">代码实现</a> </p>
</li>
<li>
<p><a href="https://github.com/WenDesi/lihang_book_algorithm/tree/master/maxENT">另外一份代码</a></p>
</li>
<li>
<p><a href="https://www.zhihu.com/question/24094554">如何理解最大熵模型里面的特征？</a></p>
</li>
<li>
<p><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_journal.pdf">Iterative Scaling and Coordinate Descent Methods for
    Maximum Entropy Models</a></p>
</li>
<li>
<p>[^1]: <a href="http://www.cs.cmu.edu/~tom/mlbook/NBayeslogReg.pdf">Generative and discriminative classifiers: Naive Bayes and logistic regression</a></p>
</li>
<li>
<p>[^2]: <a href="-">On Discriminative vs. Generative Classifiers: A comparison of Logistic Regression and Naive Bayes</a></p>
</li>
<li>
<p>[^3]: <a href="http://www.greenteapress.com/thinkbayes/thinkbayes.pdf">ThinkBayes</a></p>
</li>
<li>
<p>[^4]: <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">Multinomial logistic regression</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/u012328159/article/details/72155874">https://blog.csdn.net/u012328159/article/details/72155874</a></p>
</li>
</ol>
<p><strong><a href="#导读">⬆ top</a></strong></p>
                
                  
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://hai5g.cn/aiwiki/lihang/CH06/";
      this.page.identifier =
        "/lihang/CH06/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//AI-Wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../CH05/" title="决策树" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                决策树
              </span>
            </div>
          </a>
        
        
          <a href="../CH07/" title="支持向量机" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                支持向量机
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 AI Wiki Team
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.39abc4af.js"></script>
      
        
        
          
          <script src="../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdn.jsdelivr.net/gh/ethantw/Han@3.3.0/dist/han.min.js"></script>
      
        <script src="../../_static/js/extra.js?v=10"></script>
      
        <script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>