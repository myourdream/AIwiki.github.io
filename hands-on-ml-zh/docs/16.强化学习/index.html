



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="AI Wiki 是一个编程竞赛知识整合站点，提供有趣又实用的编程竞赛知识以及其他有帮助的内容，帮助广大编程竞赛爱好者更快更深入地学习编程竞赛">
      
      
        <link rel="canonical" href="https://hai5g.cn/aiwiki/hands-on-ml-zh/docs/16.强化学习/">
      
      
        <meta name="author" content="AI Wiki Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../../favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>强化学习 - AI Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,400i,700|Fira+Mono">
        <style>body,input{font-family:"Fira Sans","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Fira Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
      <link rel="manifest" href="../../../manifest.webmanifest">
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/ah@1.5.0/han.min.css">
    
      <link rel="stylesheet" href="../../../_static/css/extra.css?v=11">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#_1" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-header-nav__button md-logo">
          
            <i class="md-icon">school</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              AI Wiki
            </span>
            <span class="md-header-nav__topic">
              强化学习
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../.." title="简介" class="md-tabs__link">
          简介
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../Coursera_ML_AndrewNg/" title="机器学习" class="md-tabs__link">
          机器学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../qa500/" title="深度学习500问" class="md-tabs__link">
          深度学习500问
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../lihang/" title="统计学习" class="md-tabs__link">
          统计学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../README.old/" title="Sklearn与TensorFlow" class="md-tabs__link md-tabs__link--active">
          Sklearn与TensorFlow
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../ml-mastery-zh/" title="Mastery博客" class="md-tabs__link">
          Mastery博客
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-nav__button md-logo">
      
        <i class="md-icon">school</i>
      
    </a>
    AI Wiki
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      简介
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        简介
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../.." title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/AI学习路线/" title="AI学习路线1" class="md-nav__link">
      AI学习路线1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/ai-union-201904/" title="AI学习路线2" class="md-nav__link">
      AI学习路线2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v0.1/" title="AI 路线图v0.1" class="md-nav__link">
      AI 路线图v0.1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v0.2/" title="AI 路线图v0.2" class="md-nav__link">
      AI 路线图v0.2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v1.0/" title="ApacheCN 人工智能知识树" class="md-nav__link">
      ApacheCN 人工智能知识树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7" type="checkbox" id="nav-1-7">
    
    <label class="md-nav__link" for="nav-1-7">
      工具软件
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-1-7">
        工具软件
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/judgers/" title="评测工具" class="md-nav__link">
      评测工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/editors/" title="编辑工具" class="md-nav__link">
      编辑工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/wsl/" title="WSL (Windows 10)" class="md-nav__link">
      WSL (Windows 10)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/spj/" title="Special Judge" class="md-nav__link">
      Special Judge
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7-5" type="checkbox" id="nav-1-7-5">
    
    <label class="md-nav__link" for="nav-1-7-5">
      Testlib
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-1-7-5">
        Testlib
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/" title="Testlib 简介" class="md-nav__link">
      Testlib 简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/general/" title="通用" class="md-nav__link">
      通用
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/generator/" title="Generator" class="md-nav__link">
      Generator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/validator/" title="Validator" class="md-nav__link">
      Validator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/interactor/" title="Interactor" class="md-nav__link">
      Interactor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/checker/" title="Checker" class="md-nav__link">
      Checker
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/docker-deploy/" title="Docker 部署" class="md-nav__link">
      Docker 部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/about/" title="关于本项目" class="md-nav__link">
      关于本项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/faq/" title="F.A.Q." class="md-nav__link">
      F.A.Q.
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      机器学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        机器学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/SUMMARY/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/math/" title="数学基础" class="md-nav__link">
      数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week1/" title="week1" class="md-nav__link">
      week1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week2/" title="week2" class="md-nav__link">
      week2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week3/" title="week3" class="md-nav__link">
      week3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week4/" title="week4" class="md-nav__link">
      week4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week5/" title="week5" class="md-nav__link">
      week5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week6/" title="week6" class="md-nav__link">
      week6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week7/" title="week7" class="md-nav__link">
      week7
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week8/" title="week8" class="md-nav__link">
      week8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week9/" title="week9" class="md-nav__link">
      week9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week10/" title="week10" class="md-nav__link">
      week10
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      深度学习500问
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        深度学习500问
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/content/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch01_math/ch01_math/" title="第一章_数学基础" class="md-nav__link">
      第一章_数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch02_机器学习基础/第二章_机器学习基础/" title="第二章_机器学习基础" class="md-nav__link">
      第二章_机器学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch03_深度学习基础/第三章_深度学习基础/" title="第三章_深度学习基础" class="md-nav__link">
      第三章_深度学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch04_经典网络/第四章_经典网络/" title="第四章_经典网络" class="md-nav__link">
      第四章_经典网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch05_卷积神经网络(CNN)/第五章 卷积神经网络（CNN）/" title="第五章 卷积神经网络（CNN）" class="md-nav__link">
      第五章 卷积神经网络（CNN）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch06_循环神经网络(RNN)/第六章_循环神经网络(RNN)/" title="第六章_循环神经网络(RNN)" class="md-nav__link">
      第六章_循环神经网络(RNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch07_生成对抗网络(GAN)/ch7/" title="第七章_生成对抗网络(GAN)" class="md-nav__link">
      第七章_生成对抗网络(GAN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch08_目标检测/第八章_目标检测/" title="第八章_目标检测" class="md-nav__link">
      第八章_目标检测
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch09_图像分割/第九章_图像分割/" title="第九章_图像分割" class="md-nav__link">
      第九章_图像分割
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch10_强化学习/第十章_强化学习/" title="第十章_强化学习" class="md-nav__link">
      第十章_强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch11_迁移学习/第十一章_迁移学习/" title="第十一章_迁移学习" class="md-nav__link">
      第十一章_迁移学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch12_网络搭建及训练/第十二章_网络搭建及训练/" title="第十二章_网络搭建及训练" class="md-nav__link">
      第十二章_网络搭建及训练
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch13_优化算法/第十三章_优化算法/" title="第十三章_优化算法" class="md-nav__link">
      第十三章_优化算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch14_超参数调整/第十四章_超参数调整/" title="第十四章_超参数调整" class="md-nav__link">
      第十四章_超参数调整
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch15_GPU和框架选型/第十五章_异构运算、GPU及框架选型/" title="第十五章_异构运算、GPU及框架选型" class="md-nav__link">
      第十五章_异构运算、GPU及框架选型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch16_自然语言处理(NLP)/第十六章_NLP/" title="第十六章_NLP" class="md-nav__link">
      第十六章_NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署/" title="第十七章_模型压缩、加速及移动端部署" class="md-nav__link">
      第十七章_模型压缩、加速及移动端部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch18_后端架构选型、离线及实时计算/第十八章_后端架构选型、离线及实时计算/" title="第十八章_后端架构选型、离线及实时计算" class="md-nav__link">
      第十八章_后端架构选型、离线及实时计算
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../qa500/ch18_后端架构选型及应用场景/第十八章_后端架构选型及应用场景/" title="第十八章_后端架构选型及应用场景" class="md-nav__link">
      第十八章_后端架构选型及应用场景
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      统计学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        统计学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH01/" title="统计学习及监督学习概论" class="md-nav__link">
      统计学习及监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH02/" title="感知机" class="md-nav__link">
      感知机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH03/" title="K近邻法" class="md-nav__link">
      K近邻法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH04/" title="朴素贝叶斯法" class="md-nav__link">
      朴素贝叶斯法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH05/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH06/" title="逻辑斯蒂回归与最大熵模型" class="md-nav__link">
      逻辑斯蒂回归与最大熵模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH07/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH08/" title="提升方法" class="md-nav__link">
      提升方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH09/" title="EM算法及其推广" class="md-nav__link">
      EM算法及其推广
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH10/" title="隐马尔可夫模型" class="md-nav__link">
      隐马尔可夫模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH11/" title="条件随机场" class="md-nav__link">
      条件随机场
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH12/" title="监督学习方法总结" class="md-nav__link">
      监督学习方法总结
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH13/" title="无监督学习概论" class="md-nav__link">
      无监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH14/" title="聚类方法" class="md-nav__link">
      聚类方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH22/" title="无监督学习方法总结" class="md-nav__link">
      无监督学习方法总结
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" checked>
    
    <label class="md-nav__link" for="nav-5">
      Sklearn与TensorFlow
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Sklearn与TensorFlow
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../README.old/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../0.前言/" title="前言" class="md-nav__link">
      前言
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../1.机器学习概览/" title="机器学习概览" class="md-nav__link">
      机器学习概览
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2.一个完整的机器学习项目/" title="一个完整的机器学习项目" class="md-nav__link">
      一个完整的机器学习项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../3.分类/" title="分类" class="md-nav__link">
      分类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../4.训练模型/" title="训练模型" class="md-nav__link">
      训练模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../5.支持向量机/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../6.决策树/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../7.集成学习和随机森林/" title="集成学习和随机森林" class="md-nav__link">
      集成学习和随机森林
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../9.启动并运行_TensorFlow/" title="启动并运行_TensorFlow" class="md-nav__link">
      启动并运行_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../10.人工神经网络介绍/" title="人工神经网络介绍" class="md-nav__link">
      人工神经网络介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../11.训练深层神经网络/" title="训练深层神经网络" class="md-nav__link">
      训练深层神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../12.设备和服务器上的分布式_TensorFlow/" title="设备和服务器上的分布式_TensorFlow" class="md-nav__link">
      设备和服务器上的分布式_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../13.卷积神经网络/" title="卷积神经网络" class="md-nav__link">
      卷积神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../14.循环神经网络/" title="循环神经网络" class="md-nav__link">
      循环神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../15.自编码器/" title="自编码器" class="md-nav__link">
      自编码器
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        强化学习
      </label>
    
    <a href="./" title="强化学习" class="md-nav__link md-nav__link--active">
      强化学习
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" title="学习优化奖励" class="md-nav__link">
    学习优化奖励
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" title="策略搜索" class="md-nav__link">
    策略搜索
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#openai" title="OpenAI 的介绍" class="md-nav__link">
    OpenAI 的介绍
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" title="神经网络策略" class="md-nav__link">
    神经网络策略
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" title="评价行为：信用分配问题" class="md-nav__link">
    评价行为：信用分配问题
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_6" title="策略梯度" class="md-nav__link">
    策略梯度
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_7" title="马尔可夫决策过程" class="md-nav__link">
    马尔可夫决策过程
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q" title="时间差分学习与 Q 学习" class="md-nav__link">
    时间差分学习与 Q 学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" title="探索策略" class="md-nav__link">
    探索策略
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q_1" title="近似 Q 学习" class="md-nav__link">
    近似 Q 学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q-mspac-man" title="学习去使用深度 Q 学习来玩 Ms.Pac-Man" class="md-nav__link">
    学习去使用深度 Q 学习来玩 Ms.Pac-Man
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" title="练习" class="md-nav__link">
    练习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" title="感谢" class="md-nav__link">
    感谢
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../B.机器学习项目清单/" title="B.机器学习项目清单" class="md-nav__link">
      B.机器学习项目清单
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../C.SVM_对偶问题/" title="C.SVM_对偶问题" class="md-nav__link">
      C.SVM_对偶问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../D.自动微分/" title="D.自动微分" class="md-nav__link">
      D.自动微分
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Mastery博客
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        Mastery博客
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../ml-mastery-zh/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../ml-mastery-zh/docs/xgboost/SUMMARY/" title="XGBoost" class="md-nav__link">
      XGBoost
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../ml-mastery-zh/docs/dl-keras/SUMMARY/" title="深度学习（Keras）" class="md-nav__link">
      深度学习（Keras）
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" title="学习优化奖励" class="md-nav__link">
    学习优化奖励
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" title="策略搜索" class="md-nav__link">
    策略搜索
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#openai" title="OpenAI 的介绍" class="md-nav__link">
    OpenAI 的介绍
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" title="神经网络策略" class="md-nav__link">
    神经网络策略
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" title="评价行为：信用分配问题" class="md-nav__link">
    评价行为：信用分配问题
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_6" title="策略梯度" class="md-nav__link">
    策略梯度
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_7" title="马尔可夫决策过程" class="md-nav__link">
    马尔可夫决策过程
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q" title="时间差分学习与 Q 学习" class="md-nav__link">
    时间差分学习与 Q 学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" title="探索策略" class="md-nav__link">
    探索策略
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q_1" title="近似 Q 学习" class="md-nav__link">
    近似 Q 学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q-mspac-man" title="学习去使用深度 Q 学习来玩 Ms.Pac-Man" class="md-nav__link">
    学习去使用深度 Q 学习来玩 Ms.Pac-Man
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" title="练习" class="md-nav__link">
    练习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" title="感谢" class="md-nav__link">
    感谢
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/myourdream/aiwiki/blob/master/docs/hands-on-ml-zh/docs/16.强化学习.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="_1">十六、强化学习<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<blockquote>
<p>译者：<a href="https://github.com/friedhelm739">@friedhelm739</a></p>
<p>校对者：<a href="https://github.com/wizardforcel">@飞龙</a>、<a href="https://github.com/rickllyxu">@rickllyxu</a></p>
</blockquote>
<p>强化学习（RL）如今是机器学习的一大令人激动的领域，当然之前也是。自从 1950 年被发明出来后，它在这些年产生了一些有趣的应用，尤其是在游戏（例如 TD-Gammon，一个西洋双陆棋程序）和及其控制领域，但是从未弄出什么大新闻。直到 2013 年一个革命性的发展：来自英国的研究者发起了一项 Deepmind 项目，这个项目可以学习去玩任何从头开始的 Atari 游戏，甚至多数比人类玩的还要好，它仅使用像素作为输入而没有使用游戏规则的任何先验知识。这是一系列令人惊叹的壮举中的第一个，并在 2016 年 3 月以他们的系统阿尔法狗战胜了世界围棋冠军李世石而告终。从未有程序能勉强打败这个游戏的大师，更不用说世界冠军了。今天，RL 的整个领域正在沸腾着新的想法，其都具有广泛的应用范围。DeepMind 在 2014 被谷歌以超过 5 亿美元收购。</p>
<p>那么他们是怎么做到的呢？事后看来，原理似乎相当简单：他们将深度学习运用到强化学习领域，结果却超越了他们最疯狂的设想。在本章中，我们将首先解释强化学习是什么，以及它擅长于什么，然后我们将介绍两个在深度强化学习领域最重要的技术：策略梯度和深度 Q 网络（DQN），包括讨论马尔可夫决策过程（MDP）。我们将使用这些技术来训练一个模型来平衡移动车上的杆子，另一个玩 Atari 游戏。同样的技术可以用于各种各样的任务，从步行机器人到自动驾驶汽车。</p>
<h2 id="_2">学习优化奖励<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>在强化学习中，智能体在环境（environment）中观察（observation）并且做出决策（action），随后它会得到奖励（reward）。它的目标是去学习如何行动能最大化<strong>期望奖励</strong>。如果你不在意去拟人化的话，你可以认为正奖励是愉快，负奖励是痛苦（这样的话奖励一词就有点误导了）。简单来说，智能体在环境中行动，并且在实验和错误中去学习最大化它的愉快，最小化它的痛苦。</p>
<p>这是一个相当广泛的设置，可以适用于各种各样的任务。以下是几个例子（详见图 16-1）：</p>
<ol>
<li>智能体可以是控制一个机械狗的程序。在此例中，环境就是真实的世界，智能体通过许多的传感器例如摄像机或者传感器来观察，它可以通过给电机发送信号来行动。它可以被编程设置为如果到达了目的地就得到正奖励，如果浪费时间，或者走错方向，或摔倒了就得到负奖励。</li>
<li>智能体可以是控制 MS.Pac-Man 的程序。在此例中，环境是 Atari 游戏的仿真，行为是 9 个操纵杆位（上下左右中间等等），观察是屏幕，回报就是游戏点数。</li>
<li>相似地，智能体也可以是棋盘游戏的程序例如：围棋。</li>
<li>智能体也可以不用去控制一个实体（或虚拟的）去移动。例如它可以是一个智能程序，当它调整到目标温度以节能时会得到正奖励，当人们需要自己去调节温度时它会得到负奖励，所以智能体必须学会预见人们的需要。</li>
<li>智能体也可以去观测股票市场价格以实时决定买卖。奖励的依据显然为挣钱或者赔钱。</li>
</ol>
<p><img alt="图16-1" src="../../images/chapter_16/16-1.png" /></p>
<p>其实没有正奖励也是可以的，例如智能体在迷宫内移动，它每分每秒都得到一个负奖励，所以它要尽可能快的找到出口！还有很多适合强化学习的领域，例如自动驾驶汽车，在网页上放广告，或者控制一个图像分类系统让它明白它应该关注于什么。</p>
<h2 id="_3">策略搜索<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>被智能体使用去改变它行为的算法叫做策略。例如，策略可以是一个把观测当输入，行为当做输出的神经网络（见图16-2）。</p>
<p><img alt="图16-2" src="../../images/chapter_16/16-2.png" /></p>
<p>这个策略可以是你能想到的任何算法，它甚至可以不被确定。举个例子，例如，考虑一个真空吸尘器，它的奖励是在 30 分钟内捡起的灰尘数量。它的策略可以是每秒以概率<code>P</code>向前移动，或者以概率<code>1-P</code>随机地向左或向右旋转。旋转角度将是<code>-R</code>和<code>+R</code>之间的随机角度，因为该策略涉及一些随机性，所以称为随机策略。机器人将有一个不确定的轨迹，它保证它最终会到达任何可以到达的地方，并捡起所有的灰尘。问题是：30分钟后它会捡起多少灰尘？</p>
<p>你怎么训练这样的机器人？你可以调整两个策略参数：概率<code>P</code>和角度范围<code>R</code>。一个想法是这些参数尝试许多不同的值，并选择执行最佳的组合（见图 16-3）。这是一个策略搜索的例子，在这种情况下使用野蛮的方法。然而，当策略空间太大（通常情况下），以这样的方式找到一组好的参数就像是大海捞针。</p>
<p><img alt="图16-3" src="../../images/chapter_16/16-3.png" /></p>
<p>另一种搜寻策略空间的方法是遗传算法。例如你可以随机创造一个包含 100 个策略的第一代基因，随后杀死 80 个糟糕的策略，随后让 20 个幸存策略繁衍 4 代。一个后代只是它父辈基因的复制品加上一些随机变异。幸存的策略加上他们的后代共同构成了第二代。你可以继续以这种方式迭代代，直到找到一个好的策略。</p>
<p>另一种方法是使用优化技术，通过评估奖励关于策略参数的梯度，然后通过跟随梯度向更高的奖励（梯度上升）调整这些参数。这种方法被称为策略梯度（policy gradient, PG），我们将在本章后面详细讨论。例如，回到真空吸尘器机器人，你可以稍微增加概率P并评估这是否增加了机器人在 30 分钟内拾起的灰尘的量；如果确实增加了，就相对应增加<code>P</code>，否则减少<code>P</code>。我们将使用 Tensorflow 来实现 PG 算法，但是在这之前我们需要为智能体创造一个生存的环境，所以现在是介绍 OpenAI 的时候了。</p>
<h2 id="openai">OpenAI 的介绍<a class="headerlink" href="#openai" title="Permanent link">&para;</a></h2>
<p>强化学习的一个挑战是，为了训练智能体，首先需要有一个工作环境。如果你想设计一个可以学习 Atari 游戏的程序，你需要一个 Atari 游戏模拟器。如果你想设计一个步行机器人，那么环境就是真实的世界，你可以直接在这个环境中训练你的机器人，但是这有其局限性：如果机器人从悬崖上掉下来，你不能仅仅点击“撤消”。你也不能加快时间；增加更多的计算能力不会让机器人移动得更快。一般来说，同时训练 1000 个机器人是非常昂贵的。简而言之，训练在现实世界中是困难和缓慢的，所以你通常需要一个模拟环境，至少需要引导训练。</p>
<p>OpenAI gym 是一个工具包，它提供各种各样的模拟环境（Atari 游戏，棋盘游戏，2D 和 3D 物理模拟等等），所以你可以训练，比较，或开发新的 RL 算法。</p>
<p>让我们安装 OpenAI gym。可通过<code>pip</code>安装：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>$ pip install --upgrade gym
</pre></div>
</td></tr></table>

<p>接下来打开 Python shell 或 Jupyter 笔记本创建您的第一个环境：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">gym</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">)</span> 
<span class="p">[</span><span class="mi">2016</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">14</span> <span class="mi">16</span><span class="p">:</span><span class="mo">03</span><span class="p">:</span><span class="mi">23</span><span class="p">,</span><span class="mi">199</span><span class="p">]</span> <span class="n">Making</span> <span class="n">new</span> <span class="n">env</span><span class="p">:</span> <span class="n">MsPacman</span><span class="o">-</span><span class="n">v0</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">obs</span> 
<span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.03799846</span><span class="p">,</span><span class="o">-</span><span class="mf">0.03288115</span><span class="p">,</span><span class="mf">0.02337094</span><span class="p">,</span><span class="mf">0.00720711</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span> 
</pre></div>
</td></tr></table>

<p>使用<code>make()</code>函数创建一个环境，在此例中是 CartPole 环境。这是一个 2D 模拟，其中推车可以被左右加速，以平衡放置在它上面的平衡杆（见图 16-4）。在创建环境之后，我们需要使用<code>reset()</code>初始化。这会返回第一个观察结果。观察取决于环境的类型。对于 CartPole 环境，每个观测是包含四个浮点的 1D Numpy 向量：这些浮点数代表推车的水平位置（0 为中心）、其速度、杆的角度（0 维垂直）及其角速度。最后，<code>render()</code>方法显示如图 16-4 所示的环境。</p>
<p><img alt="图16-4" src="../../images/chapter_16/16-4.png" /></p>
<p>如果你想让<code>render()</code>让图像以一个 NUMPY 数组格式返回，可以将<code>mode</code>参数设置为<code>rgb_array</code>（注意其他环境可能支持不同的模式）：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">img</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;rgb_array&quot;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># height, width, channels (3=RGB) </span>
<span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>不幸的是，即使将<code>mode</code>参数设置为<code>rgb_array</code>，CartPole（和其他一些环境）还是会将将图像呈现到屏幕上。避免这种情况的唯一方式是使用一个 fake X 服务器，如 XVFB 或 XDimMy。例如，可以使用以下命令安装 XVFB 和启动 Python：<code>xvfb-run -s "screen 0 1400x900x24" python</code>。或者使用<code>xvfbwrapper</code>包。</p>
<p>让我们来询问环境什么动作是可能的：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span> 
<span class="n">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p><code>Discrete(2)</code>表示可能的动作是整数 0 和 1，表示向左（0）或右（1）的加速。其他环境可能有更多的动作，或者其他类型的动作（例如，连续的）。因为杆子向右倾斜，让我们向右加速推车：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">action</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># accelerate right </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">obs</span> 
<span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.03865608</span><span class="p">,</span>  <span class="mf">0.16189797</span><span class="p">,</span>  <span class="mf">0.02351508</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.27801135</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">reward</span> 
<span class="mf">1.0</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">done</span> 
<span class="bp">False</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">info</span> 
<span class="p">{}</span> 
</pre></div>
</td></tr></table>

<p><code>step()</code>表示执行给定的动作并返回四个值：</p>
<p><code>obs</code>:</p>
<p>这是新的观测，小车现在正在向右走（<code>obs[1]&gt;0</code>，注：当前速度为正，向右为正）。平衡杆仍然向右倾斜（<code>obs[2]&gt;0</code>），但是他的角速度现在为负（<code>obs[3]&lt;0</code>），所以它在下一步后可能会向左倾斜。</p>
<p><code>reward</code>：</p>
<p>在这个环境中，无论你做什么，每一步都会得到 1.0 奖励，所以游戏的目标就是尽可能长的运行。</p>
<p><code>done</code>：</p>
<p>当游戏结束时这个值会为<code>True</code>。当平衡杆倾斜太多时会发生这种情况。之后，必须重新设置环境才能重新使用。</p>
<p><code>info</code>：</p>
<p>该字典可以在其他环境中提供额外的调试信息。这些数据不应该用于训练（这是作弊）。</p>
<p>让我们硬编码一个简单的策略，当杆向左倾斜时加速左边，当杆向右倾斜时加速。我们使用这个策略来获得超过 500 步的平均回报：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">basic_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>    
    <span class="n">angle</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>    
    <span class="k">return</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">angle</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>

<span class="n">totals</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>    
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>    
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span> <span class="c1"># 最多1000 步，我们不想让它永远运行下去        </span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">basic_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>        
        <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>        
        <span class="n">episode_rewards</span> <span class="o">+=</span> <span class="n">reward</span>        
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>            
            <span class="k">break</span>    
    <span class="n">totals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这个代码希望能自我解释。让我们看看结果：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">totals</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">totals</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">totals</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">totals</span><span class="p">)</span> 
<span class="p">(</span><span class="mf">42.125999999999998</span><span class="p">,</span> <span class="mf">9.1237121830974033</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">68.0</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>即使有 500 次尝试，这一策略从未使平衡杆在超过 68 个连续的步骤里保持直立。这不太好。如果你看一下 Juyter Notebook 中的模拟，你会发现，推车越来越强烈地左右摆动，直到平衡杆倾斜太多。让我们看看神经网络是否能提出更好的策略。</p>
<h2 id="_4">神经网络策略<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p>让我们创建一个神经网络策略。就像之前我们编码的策略一样，这个神经网络将把观察作为输入，输出要执行的动作。更确切地说，它将估计每个动作的概率，然后我们将根据估计的概率随机地选择一个动作（见图 16-5）。在 CartPole 环境中，只有两种可能的动作（左或右），所以我们只需要一个输出神经元。它将输出动作 0（左）的概率<code>p</code>，动作 1（右）的概率显然将是<code>1 - p</code>。</p>
<p>例如，如果它输出 0.7，那么我们将以 70% 的概率选择动作 0，以 30% 的概率选择动作 1。</p>
<p><img alt="图16-5" src="../../images/chapter_16/16-5.png" /></p>
<p>你可能奇怪为什么我们根据神经网络给出的概率来选择随机的动作，而不是选择最高分数的动作。这种方法使智能体在<strong>探索新的行为</strong>和<strong>利用那些已知可行的行动</strong>之间找到正确的平衡。举个例子：假设你第一次去餐馆，所有的菜看起来同样吸引人，所以你随机挑选一个。如果菜好吃，你可以增加下一次点它的概率，但是你不应该把这个概率提高到 100%，否则你将永远不会尝试其他菜肴，其中一些甚至比你尝试的更好。</p>
<p>还要注意，在这个特定的环境中，过去的动作和观察可以被安全地忽略，因为每个观察都包含环境的完整状态。如果有一些隐藏状态，那么你也需要考虑过去的行为和观察。例如，如果环境仅仅揭示了推车的位置，而不是它的速度，那么你不仅要考虑当前的观测，还要考虑先前的观测，以便估计当前的速度。另一个例子是当观测是有噪声的的，在这种情况下，通常你想用过去的观察来估计最可能的当前状态。因此，CartPole 问题是简单的；观测是无噪声的，而且它们包含环境的全状态。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span> 
<span class="kn">from</span> <span class="nn">tensorflow.contrib.layers</span> <span class="kn">import</span> <span class="n">fully_connected</span>
<span class="c1"># 1. 声明神经网络结构 </span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># == env.observation_space.shape[0] </span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 这只是个简单的测试，不需要过多的隐藏层</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># 只输出向左加速的概率</span>
<span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">()</span>
<span class="c1"># 2. 建立神经网络 </span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span><span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span> <span class="c1"># 隐层激活函数使用指数线性函数             </span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span>                   
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="c1"># 3. 在概率基础上随机选择动作</span>
<span class="n">p_left_and_right</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">outputs</span><span class="p">])</span> 
<span class="n">action</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_left_and_right</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span> 
</pre></div>
</td></tr></table>

<p>让我们通读代码：</p>
<ol>
<li>
<p>在导入之后，我们定义了神经网络体系结构。输入的数量是观测空间的大小（在 CartPole 的情况下是 4 个），我们只有 4 个隐藏单元，并且不需要更多，并且我们只有 1 个输出概率（向左的概率）。</p>
</li>
<li>
<p>接下来我们构建了神经网络。在这个例子中，它是一个 vanilla 多层感知器，只有一个输出。注意，输出层使用 Logistic（Sigmoid）激活函数，以便输出从 0 到 1 的概率。如果有两个以上的可能动作，每个动作都会有一个输出神经元，相应的你将使用 Softmax 激活函数。</p>
</li>
<li>
<p>最后，我们调用<code>multinomial()</code>函数来选择一个随机动作。该函数独立地采样一个（或多个）整数，给定每个整数的对数概率。例如，如果通过设置<code>num_samples=5</code>，令数组为<code>[np.log(0.5), np.log(0.2), np.log(0.3)]</code>来调用它，那么它将输出五个整数，每个整数都有 50% 的概率是 0，20% 为 1，30% 为 2。在我们的情况下，我们只需要一个整数来表示要采取的行动。由于输出张量（output）仅包含向左的概率，所以我们必须首先将 1 - output 连接它，以得到包含左和右动作的概率的张量。请注意，如果有两个以上的可能动作，神经网络将不得不输出每个动作的概率，这时你就不需要连接步骤了。</p>
</li>
</ol>
<p>好了，现在我们有一个可以观察和输出动作的神经网络了，那我们怎么训练它呢？</p>
<h2 id="_5">评价行为：信用分配问题<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h2>
<p>如果我们知道每一步的最佳动作，我们可以像通常一样训练神经网络，通过最小化估计概率和目标概率之间的交叉熵。这只是通常的监督学习。然而，在强化学习中，智能体获得的指导的唯一途径是通过奖励，奖励通常是稀疏的和延迟的。例如，如果智能体在 100 个步骤内设法平衡杆，它怎么知道它采取的 100 个行动中的哪一个是好的，哪些是坏的？它所知道的是，在最后一次行动之后，杆子坠落了，但最后一次行动肯定不是完全负责的。这被称为信用分配问题：当智能体得到奖励时，很难知道哪些行为应该被信任（或责备）。想想一只狗在行为良好后几小时就会得到奖励，它会明白它得到了什么回报吗？</p>
<p>为了解决这个问题，一个通常的策略是基于这个动作后得分的总和来评估这个个动作，通常在每个步骤中应用衰减率<code>r</code>。例如（见图 16-6），如果一个智能体决定连续三次向右，在第一步之后得到 +10 奖励，第二步后得到 0，最后在第三步之后得到 -50，然后假设我们使用衰减率<code>r=0.8</code>，那么第一个动作将得到<code>10 +r×0 + r2×(-50)=-22</code>的分述。如果衰减率接近 0，那么与即时奖励相比，未来的奖励不会有多大意义。相反，如果衰减率接近 1，那么对未来的奖励几乎等于即时回报。典型的衰减率通常为是 0.95 或 0.99。如果衰减率为 0.95，那么未来 13 步的奖励大约是即时奖励的一半（<code>0.9513×0.5</code>），而当衰减率为 0.99，未来 69 步的奖励是即时奖励的一半。在 CartPole 环境下，行为具有相当短期的影响，因此选择 0.95 的折扣率是合理的。</p>
<p><img alt="图16-6" src="../../images/chapter_16/16-6.png" /></p>
<p>当然，一个好的动作可能会伴随着一些坏的动作，这些动作会导致平衡杆迅速下降，从而导致一个好的动作得到一个低分数（类似的，一个好行动者有时会在一部烂片中扮演主角）。然而，如果我们花足够多的时间来训练游戏，平均下来好的行为会得到比坏的更好的分数。因此，为了获得相当可靠的动作分数，我们必须运行很多次并将所有动作分数归一化（通过减去平均值并除以标准偏差）。之后，我们可以合理地假设消极得分的行为是坏的，而积极得分的行为是好的。现在我们有一个方法来评估每一个动作，我们已经准备好使用策略梯度来训练我们的第一个智能体。让我们看看如何。</p>
<h2 id="_6">策略梯度<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h2>
<p>正如前面所讨论的，PG 算法通过遵循更高回报的梯度来优化策略参数。一种流行的 PG 算法，称为增强算法，在 1929 由 Ronald Williams 提出。这是一个常见的变体：</p>
<ol>
<li>
<p>首先，让神经网络策略玩几次游戏，并在每一步计算梯度，这使得智能体更可能选择行为，但不应用这些梯度。</p>
</li>
<li>
<p>运行几次后，计算每个动作的得分（使用前面段落中描述的方法）。</p>
</li>
<li>
<p>如果一个动作的分数是正的，这意味着动作是好的，可应用较早计算的梯度，以便将来有更大的的概率选择这个动作。但是，如果分数是负的，这意味着动作是坏的，要应用负梯度来使得这个动作在将来采取的可能性更低。我们的方法就是简单地将每个梯度向量乘以相应的动作得分。</p>
</li>
<li>
<p>最后，计算所有得到的梯度向量的平均值，并使用它来执行梯度下降步骤。</p>
</li>
</ol>
<p>让我们使用 TensorFlow 实现这个算法。我们将训练我们早先建立的神经网络策略，让它学会平衡车上的平衡杆。让我们从完成之前编码的构造阶段开始，添加目标概率、代价函数和训练操作。因为我们的意愿是选择的动作是最好的动作，如果选择的动作是动作 0（左），则目标概率必须为 1，如果选择动作 1（右）则目标概率为 0：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> 
</pre></div>
</td></tr></table>

<p>现在我们有一个目标概率，我们可以定义损失函数（交叉熵）并计算梯度：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span>                    <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span> 
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> 
<span class="n">grads_and_vars</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>注意，我们正在调用优化器的<code>compute_gradients()</code>方法，而不是<code>minimize()</code>方法。这是因为我们想要在使用它们之前调整梯度。<code>compute_gradients()</code>方法返回梯度向量/变量对的列表（每个可训练变量一对）。让我们把所有的梯度放在一个列表中，以便方便地获得它们的值：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">gradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">grad</span> <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">]</span> 
</pre></div>
</td></tr></table>

<p>好，现在是棘手的部分。在执行阶段，算法将运行策略，并在每个步骤中评估这些梯度张量并存储它们的值。在多次运行之后，它如先前所解释的调整这些梯度（即，通过动作分数乘以它们并使它们归一化），并计算调整后的梯度的平均值。接下来，需要将结果梯度反馈到优化器，以便它可以执行优化步骤。这意味着对于每一个梯度向量我们需要一个占位符。此外，我们必须创建操作去应用更新的梯度。为此，我们将调用优化器的<code>apply_gradients()</code>函数，该函数接受梯度向量/变量对的列表。我们不给它原始的梯度向量，而是给它一个包含更新梯度的列表（即，通过占位符递送的梯度）：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">gradient_placeholders</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">grads_and_vars_feed</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">:</span>       
    <span class="n">gradient_placeholder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">grad</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>    
    <span class="n">gradient_placeholders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradient_placeholder</span><span class="p">)</span>  
    <span class="n">grads_and_vars_feed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">gradient_placeholder</span><span class="p">,</span> <span class="n">variable</span><span class="p">))</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars_feed</span><span class="p">)</span> 
</pre></div>
</td></tr></table>

<p>让我们后退一步，看看整个运行过程：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">4</span> 
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">4</span> 
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">()</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">])</span> 
<span class="n">hidden</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span><span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span>                          
<span class="n">logits</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>                    <span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span> 
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> 
<span class="n">p_left_and_right</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">outputs</span><span class="p">])</span> 
<span class="n">action</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_left_and_right</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> 
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span> 
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> 
<span class="n">grads_and_vars</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span> 
<span class="n">gradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">grad</span> <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">]</span> 
<span class="n">gradient_placeholders</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">grads_and_vars_feed</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">:</span>    
<span class="n">gradient_placeholder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">grad</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>    <span class="n">gradient_placeholders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradient_placeholder</span><span class="p">)</span>    
<span class="n">grads_and_vars_feed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">gradient_placeholder</span><span class="p">,</span> <span class="n">variable</span><span class="p">))</span> 
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars_feed</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span> 
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>
</td></tr></table>

<p>到执行阶段了！我们将需要两个函数来计算总折扣奖励，给予原始奖励，以及归一化多次循环的结果：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">discount_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">discount_rate</span><span class="p">):</span>    
    <span class="n">discounted_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>    
    <span class="n">cumulative_rewards</span> <span class="o">=</span> <span class="mi">0</span>    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))):</span>        
        <span class="n">cumulative_rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">+</span> <span class="n">cumulative_rewards</span> <span class="o">*</span> <span class="n">discount_rate</span>       <span class="n">discounted_rewards</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">cumulative_rewards</span>    
    <span class="k">return</span> <span class="n">discounted_rewards</span>

<span class="k">def</span> <span class="nf">discount_and_normalize_rewards</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">,</span> <span class="n">discount_rate</span><span class="p">):</span>       
    <span class="n">all_discounted_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">discount_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="k">for</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="n">all_rewards</span><span class="p">]</span>       
    <span class="n">flat_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">all_discounted_rewards</span><span class="p">)</span>    
    <span class="n">reward_mean</span> <span class="o">=</span> <span class="n">flat_rewards</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>    
    <span class="n">reward_std</span> <span class="o">=</span> <span class="n">flat_rewards</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>    
    <span class="k">return</span> <span class="p">[(</span><span class="n">discounted_rewards</span> <span class="o">-</span> <span class="n">reward_mean</span><span class="p">)</span><span class="o">/</span><span class="n">reward_std</span>  <span class="k">for</span> <span class="n">discounted_rewards</span> <span class="ow">in</span> <span class="n">all_discounted_rewards</span><span class="p">]</span> 
</pre></div>
</td></tr></table>

<p>让我们检查一下运行的如何：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">discount_rewards</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">],</span> <span class="n">discount_rate</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span> 
<span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">22.</span><span class="p">,</span> <span class="o">-</span><span class="mf">40.</span><span class="p">,</span> <span class="o">-</span><span class="mf">50.</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">discount_and_normalize_rewards</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]],</span> <span class="n">discount_rate</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span> 
<span class="p">[</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.28435071</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.86597718</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.18910299</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span> <span class="mf">1.26665318</span><span class="p">,</span>  <span class="mf">1.0727777</span> <span class="p">])]</span> 
</pre></div>
</td></tr></table>

<p>对<code>discount_rewards()</code>的调用正好返回我们所期望的（见图 16-6）。你也可以验证函数<code>iscount_and_normalize_rewards()</code>确实返回了两个步骤中每个动作的标准化分数。注意第一步比第二步差很多，所以它的归一化分数都是负的；从第一步开始的所有动作都会被认为是坏的，反之，第二步的所有动作都会被认为是好的。</p>
<p>我们现在有了训练策略所需的一切：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">250</span>      <span class="c1"># 训练迭代次数 </span>
<span class="n">n_max_steps</span> <span class="o">=</span> <span class="mi">1000</span>      <span class="c1"># 每一次的最大步长 </span>
<span class="n">n_games_per_update</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 每迭代十次训练一次策略网络 </span>
<span class="n">save_iterations</span> <span class="o">=</span> <span class="mi">10</span>    <span class="c1"># 每十次迭代保存模型</span>
<span class="n">discount_rate</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>    
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>    
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>        
        <span class="n">all_rewards</span> <span class="o">=</span> <span class="p">[]</span>    <span class="c1">#每一次的所有奖励        </span>
        <span class="n">all_gradients</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1">#每一次的所有梯度        </span>
        <span class="k">for</span> <span class="n">game</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_games_per_update</span><span class="p">):</span>            
            <span class="n">current_rewards</span> <span class="o">=</span> <span class="p">[]</span>   <span class="c1">#当前步的所有奖励        </span>
            <span class="n">current_gradients</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#当前步的所有梯度 </span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>            
            <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_max_steps</span><span class="p">):</span>                
                <span class="n">action_val</span><span class="p">,</span> <span class="n">gradients_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">action</span><span class="p">,</span> <span class="n">gradients</span><span class="p">],</span>
                <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">obs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">)})</span> <span class="c1"># 一个obs                </span>
                <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_val</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>                <span class="n">current_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>                
                <span class="n">current_gradients</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradients_val</span><span class="p">)</span>                
                <span class="k">if</span> <span class="n">done</span><span class="p">:</span>                    
                    <span class="k">break</span>            
                <span class="n">all_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_rewards</span><span class="p">)</span>            
                <span class="n">all_gradients</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_gradients</span><span class="p">)</span>
        <span class="c1"># 此时我们每10次运行一次策略，我们已经准备好使用之前描述的算法去更新策略，注：即使用迭代10次的结果来优化当前的策略。      </span>
        <span class="n">all_rewards</span> <span class="o">=</span> <span class="n">discount_and_normalize_rewards</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span>        
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{}</span>        
        <span class="k">for</span> <span class="n">var_index</span><span class="p">,</span> <span class="n">grad_placeholder</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gradient_placeholders</span><span class="p">):</span>
            <span class="c1"># 将梯度与行为分数相乘，并计算平均值</span>
            <span class="n">mean_gradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">reward</span> <span class="o">*</span> <span class="n">all_gradients</span><span class="p">[</span><span class="n">game_index</span><span class="p">][</span><span class="n">step</span><span class="p">][</span><span class="n">var_index</span><span class="p">]</span> <span class="k">for</span> <span class="n">game_index</span><span class="p">,</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span>  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">reward</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">)],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>            
            <span class="n">feed_dict</span><span class="p">[</span><span class="n">grad_placeholder</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_gradients</span>        
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>  
        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="n">save_iterations</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>           
            <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_policy_net_pg.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>每一次训练迭代都是通过运行10次的策略开始的（每次最多 1000 步，以避免永远运行）。在每一步，我们也计算梯度，假设选择的行动是最好的。在运行了这 10 次之后，我们使用<code>discount_and_normalize_rewards()</code>函数计算动作得分；我们遍历每个可训练变量，在所有次数和所有步骤中，通过其相应的动作分数来乘以每个梯度向量；并且我们计算结果的平均值。最后，我们运行训练操作，给它提供平均梯度（对每个可训练变量提供一个）。我们继续每 10 个训练次数保存一次模型。</p>
<p>我们做完了！这段代码将训练神经网络策略，它将成功地学会平衡车上的平衡杆（你可以在 Juyter notebook 上试用）。注意，实际上有两种方法可以让玩家游戏结束：要么平衡可以倾斜太大，要么车完全脱离屏幕。在 250 次训练迭代中，策略学会平衡极点，但在避免脱离屏幕方面还不够好。额外数百次的训练迭代可以解决这一问题。</p>
<p>研究人员试图找到一种即使当智能体最初对环境一无所知时也能很好地工作的算法。然而，除非你正在写论文，否则你应该尽可能多地将先前的知识注入到智能体中，因为它会极大地加速训练。例如，你可以添加与屏幕中心距离和极点角度成正比的负奖励。此外，如果你已经有一个相当好的策略，你可以训练神经网络模仿它，然后使用策略梯度来改进它。</p>
<p>尽管它相对简单，但是该算法是非常强大的。你可以用它来解决更难的问题，而不仅仅是平衡一辆手推车上的平衡杆。事实上，AlgPaGo 是基于类似的 PG 算法（加上蒙特卡罗树搜索，这超出了本书的范围）。</p>
<p>现在我们来看看另一个流行的算法。与 PG 算法直接尝试优化策略以增加奖励相反，我们现在看的算法是间接的：智能体学习去估计每个状态的未来衰减奖励的期望总和，或者在每个状态中的每个行为未来衰减奖励的期望和。然后，使用这些知识来决定如何行动。为了理解这些算法，我们必须首先介绍马尔可夫决策过程（MDP）。</p>
<h2 id="_7">马尔可夫决策过程<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h2>
<p>在二十世纪初，数学家 Andrey Markov 研究了没有记忆的随机过程，称为马尔可夫链。这样的过程具有固定数量的状态，并且在每个步骤中随机地从一个状态演化到另一个状态。它从状态<code>S</code>演变为状态<code>S'</code>的概率是固定的，它只依赖于<code>(S, S')</code>对，而不是依赖于过去的状态（系统没有记忆）。</p>
<p>图 16-7 展示了一个具有四个状态的马尔可夫链的例子。假设该过程从状态<code>S0</code>开始，并且在下一步骤中有 70% 的概率保持在该状态不变中。最终，它必然离开那个状态，并且永远不会回来，因为没有其他状态回到<code>S0</code>。如果它进入状态<code>S1</code>，那么它很可能会进入状态<code>S2</code>（90% 的概率），然后立即回到状态<code>S1</code>（以 100% 的概率）。它可以在这两个状态之间交替多次，但最终它会落入状态<code>S3</code>并永远留在那里（这是一个终端状态）。马尔可夫链可以有非常不同的应用，它们在热力学、化学、统计学等方面有着广泛的应用。</p>
<p><img alt="图16-7" src="../../images/chapter_16/16-7.png" /></p>
<p>马尔可夫决策过程最初是在 20 世纪 50 年代由 Richard Bellman 描述的。它们类似于马尔可夫链，但有一个连结：<strong>在状态转移的每一步中，一个智能体可以选择几种可能的动作中的一个，并且转移概率取决于所选择的动作。</strong>此外，一些状态转移返回一些奖励（正或负），智能体的目标是找到一个策略，随着时间的推移将最大限度地提高奖励。</p>
<p>例如，图 16-8 中所示的 MDP 在每个步骤中具有三个状态和三个可能的离散动作。如果从状态<code>S0</code>开始，随着时间的推移可以在动作<code>A0</code>、<code>A1</code>或<code>A2</code>之间进行选择。如果它选择动作<code>A1</code>，它就保持在状态<code>S0</code>中，并且没有任何奖励。因此，如果愿意的话，它可以决定永远呆在那里。但是，如果它选择动作<code>A0</code>，它有 70% 的概率获得 10 奖励，并保持在状态<code>S0</code>。然后，它可以一次又一次地尝试获得尽可能多的奖励。但它将在状态<code>S1</code>中结束这样的行为。在状态<code>S1</code>中，它只有两种可能的动作：<code>A0</code>或<code>A1</code>。它可以通过反复选择动作<code>A1</code>来选择停留，或者它可以选择动作<code>A2</code>移动到状态<code>S2</code>并得到 -50 奖励。在状态<code>S3</code>中，除了采取行动<code>A1</code>之外，别无选择，这将最有可能引导它回到状态<code>S0</code>，在途中获得 40 的奖励。通过观察这个 MDP，你能猜出哪一个策略会随着时间的推移而获得最大的回报吗？在状态<code>S0</code>中，清楚地知道<code>A0</code>是最好的选择，在状态<code>S3</code>中，智能体别无选择，只能采取行动<code>A1</code>，但是在状态<code>S1</code>中，智能体否应该保持不动（<code>A0</code>）或通过火（<code>A2</code>），这是不明确的。</p>
<p><img alt="图16-8" src="../../images/chapter_16/16-8.png" /></p>
<p>Bellman 找到了一种估计任何状态<code>S</code>的最佳状态值的方法，他提出了<code>V(s)</code>，它是智能体在其采取最佳行为达到状态<code>s</code>后所有衰减未来奖励的总和的平均期望。他表明，如果智能体的行为最佳，那么贝尔曼最优性公式适用（见公式 16-1）。这个递归公式表示，如果智能体最优地运行，那么当前状态的最优值等于在采取一个最优动作之后平均得到的奖励，加上该动作可能导致的所有可能的下一个状态的期望最优值。</p>
<p><img alt="图E16-1" src="../../images/chapter_16/E16-1.png" /></p>
<p>其中：</p>
<ul>
<li>
<p><code>T</code>为智能体选择动作<code>a</code>时从状态<code>s</code>到状态<code>s'</code>的概率</p>
</li>
<li>
<p><code>R</code>为智能体选择以动作<code>a</code>从状态<code>s</code>到状态<code>s'</code>的过程中得到的奖励</p>
</li>
<li>
<p><img alt="\gamma" src="../../images/tex-ae539dfcc999c28e25a0f3ae65c1de79.gif" />为衰减率</p>
</li>
</ul>
<p>这个等式直接引出了一种算法，该算法可以精确估计每个可能状态的最优状态值：首先将所有状态值估计初始化为零，然后用数值迭代算法迭代更新它们（见公式 16-2）。一个显著的结果是，给定足够的时间，这些估计保证收敛到最优状态值，对应于最优策略。</p>
<p><img alt="图E16-2" src="../../images/chapter_16/E16-2.png" /></p>
<p>其中：</p>
<ul>
<li><img alt="V_k (s)" src="../../images/tex-12561b9e1cad1ffa385140c6c5cf9c12.gif" />是在<code>k</code>次算法迭代对状态<code>s</code>的估计</li>
</ul>
<p>该算法是动态规划的一个例子，它将了一个复杂的问题（在这种情况下，估计潜在的未来衰减奖励的总和）变为可处理的子问题，可以迭代地处理（在这种情况下，找到最大化平均报酬与下一个衰减状态值的和的动作）</p>
<p>了解最佳状态值可能是有用的，特别是评估策略，但它没有明确地告诉智能体要做什么。幸运的是，Bellman 发现了一种非常类似的算法来估计最优状态-动作值（<em>state-action values</em>），通常称为 Q 值。状态行动<code>(S, A)</code>对的最优 Q 值，记为<code>Q(s, a)</code>，是智能体在到达状态<code>S</code>，然后选择动作<code>A</code>之后平均衰减未来奖励的期望的总和。但是在它看到这个动作的结果之前，假设它在该动作之后的动作是最优的。</p>
<p>下面是它的工作原理：再次，通过初始化所有的 Q 值估计为零，然后使用 Q 值迭代算法更新它们（参见公式 16-3）。</p>
<p><img alt="图E16-3" src="../../images/chapter_16/E16-3.png" /></p>
<p>一旦你有了最佳的 Q 值，定义最优的策略<code>π*(s)</code>，它是平凡的：当智能体处于状态<code>S</code>时，它应该选择具有最高 Q 值的动作，用于该状态：<img alt="\pi ^ * (s) =\arg \max_a Q^*(s, a)" src="../../images/tex-11b3b872c168f182e8efc0bec2e6a0c5.gif" />。</p>
<p>让我们把这个算法应用到图 16-8 所示的 MDP 中。首先，我们需要定义 MDP：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">nan</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span>  <span class="c1"># 代表不可能的动作 </span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>  <span class="c1"># shape=[s, a, s&#39;]        </span>
    <span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]],</span>       
    <span class="p">[[</span><span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">]],</span>    <span class="p">])</span> 
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>  <span class="c1"># shape=[s, a, s&#39;]        </span>
    <span class="p">[[</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]],</span>        
    <span class="p">[[</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">50.</span><span class="p">]],</span>        
    <span class="p">[[</span><span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">],</span> <span class="p">[</span><span class="mf">40.</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">]],</span>    <span class="p">])</span> 
<span class="n">possible_actions</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span> 
</pre></div>
</td></tr></table>

<p>让我们运行 Q 值迭代算法</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># -inf 对应着不可能的动作 </span>
<span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">actions</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">):</span>    
    <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">actions</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># 对所有可能的动作初始化为0.0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span> 
<span class="n">discount_rate</span> <span class="o">=</span> <span class="mf">0.95</span> 
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>   
    <span class="n">Q_prev</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>    
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>        
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">possible_actions</span><span class="p">[</span><span class="n">s</span><span class="p">]:</span>            
            <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">T</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">sp</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">sp</span><span class="p">]</span> <span class="o">+</span> <span class="n">discount_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_prev</span><span class="p">[</span><span class="n">sp</span><span class="p">]))</span>  
            <span class="k">for</span> <span class="n">sp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span> 
</pre></div>
</td></tr></table>

<p>结果的 Q 值类似于如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">Q</span> 
<span class="n">array</span><span class="p">([[</span> <span class="mf">21.89498982</span><span class="p">,</span>  <span class="mf">20.80024033</span><span class="p">,</span>  <span class="mf">16.86353093</span><span class="p">],</span>       
        <span class="p">[</span>  <span class="mf">1.11669335</span><span class="p">,</span>         <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="mf">1.17573546</span><span class="p">],</span>       
        <span class="p">[</span>        <span class="o">-</span><span class="n">inf</span><span class="p">,</span>  <span class="mf">53.86946068</span><span class="p">,</span>         <span class="o">-</span><span class="n">inf</span><span class="p">]])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 每一状态的最优动作</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> 
</pre></div>
</td></tr></table>

<p>这给我们这个 MDP 的最佳策略，当使用 0.95 的衰减率时：在状态<code>S0</code>选择动作<code>A0</code>，在状态<code>S1</code>选择动作<code>A2</code>（通过火焰！）在状态<code>S2</code>中选择动作<code>A1</code>（唯一可能的动作）。有趣的是，如果你把衰减率降低到 0.9，最优的策略改变：在状态<code>S1</code>中，最好的动作变成<code>A0</code>（保持不变；不通过火）。这是有道理的，因为如果你认为现在比未来更重要，那么未来奖励的前景是不值得立刻经历痛苦的。</p>
<h2 id="q">时间差分学习与 Q 学习<a class="headerlink" href="#q" title="Permanent link">&para;</a></h2>
<p>具有离散动作的强化学习问题通常可以被建模为马尔可夫决策过程，但是智能体最初不知道转移概率是什么（它不知道<code>T</code>），并且它不知道奖励会是什么（它不知道<code>R</code>）。它必须经历每一个状态和每一次转变并且至少知道一次奖励，并且如果要对转移概率进行合理的估计，就必须经历多次。</p>
<p>时间差分学习（TD 学习）算法与数值迭代算法非常类似，但考虑到智能体仅具有 MDP 的部分知识。一般来说，我们假设智能体最初只知道可能的状态和动作，没有更多了。智能体使用探索策略，例如，纯粹的随机策略来探索 MDP，并且随着它的发展，TD 学习算法基于实际观察到的转换和奖励来更新状态值的估计（见公式 16-4）。</p>
<p><img alt="图E16-4" src="../../images/chapter_16/E16-4.png" /></p>
<p>其中：</p>
<p><code>a</code>是学习率（例如 0.01）</p>
<p>TD 学习与随机梯度下降有许多相似之处，特别是它一次处理一个样本的行为。就像 SGD 一样，只有当你逐渐降低学习速率时，它才能真正收敛（否则它将在极值点震荡）。</p>
<p>对于每个状态<code>S</code>，该算法只跟踪智能体离开该状态时立即获得的奖励的平均值，再加上它期望稍后得到的奖励（假设它的行为最佳）。</p>
<p>类似地，此时的Q 学习算法是 Q 值迭代算法的改编版本，其适应转移概率和回报在初始未知的情况（见公式16-5）。</p>
<p><img alt="图E16-5" src="../../images/chapter_16/E16-5.png" /></p>
<p>对于每一个状态动作对<code>(s,a)</code>，该算法跟踪智能体在以动作<code>A</code>离开状态<code>S</code>时获得的即时奖励平均值<code>R</code>，加上它期望稍后得到的奖励。由于目标策略将最优地运行，所以我们取下一状态的 Q 值估计的最大值。</p>
<p>以下是如何实现 Q 学习：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy.random</span> <span class="kn">as</span> <span class="nn">rnd</span>
<span class="n">learning_rate0</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">learning_rate_decay</span> <span class="o">=</span> <span class="mf">0.1</span> 
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># 在状态 0开始</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># -inf 对应着不可能的动作 </span>
<span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">actions</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">):</span>    
    <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">actions</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># 对于所有可能的动作初始化为 0.0</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>    
    <span class="n">a</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>  <span class="c1"># 随机选择动作   </span>
    <span class="n">sp</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">T</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">])</span> <span class="c1"># 使用 T[s, a] 挑选下一状态   </span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">R</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">sp</span><span class="p">]</span>    
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">iteration</span> <span class="o">*</span> <span class="n">learning_rate_decay</span><span class="p">)</span>    
    <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">discount_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">sp</span><span class="p">]))</span>    
    <span class="n">s</span> <span class="o">=</span> <span class="n">sp</span> <span class="c1"># 移动至下一状态</span>
</pre></div>
</td></tr></table>

<p>给定足够的迭代，该算法将收敛到最优 Q 值。这被称为离线策略算法，因为正在训练的策略不是正在执行的策略。令人惊讶的是，该算法能够通过观察智能体行为随机学习（例如学习当你的老师是一个醉猴子时打高尔夫球）最佳策略。我们能做得更好吗？</p>
<h2 id="_8">探索策略<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<p>当然，只有在探索策略充分探索 MDP 的情况下，Q 学习才能起作用。尽管一个纯粹的随机策略保证最终访问每一个状态和每个转换多次，但可能需要很长的时间这样做。因此，一个更好的选择是使用 ε 贪婪策略：在每个步骤中，它以概率<code>ε</code>随机地或以概率为<code>1-ε</code>贪婪地（选择具有最高 Q 值的动作）。ε 贪婪策略的优点（与完全随机策略相比）是，它将花费越来越多的时间来探索环境中有趣的部分，因为 Q 值估计越来越好，同时仍花费一些时间访问 MDP 的未知区域。以<code>ε</code>为很高的值（例如，1）开始，然后逐渐减小它（例如，下降到 0.05）是很常见的。</p>
<p>可选择的，相比于依赖于探索的可能性，另一种方法是鼓励探索策略来尝试它以前没有尝试过的行动。这可以被实现为附加于 Q 值估计的奖金，如公式 16-6 所示。</p>
<p><img alt="图E16-6" src="../../images/chapter_16/E16-6.png" /></p>
<p>其中：</p>
<ul>
<li>
<p><code>N</code>计算了在状态<code>s</code>时选择动作<code>a</code>的次数</p>
</li>
<li>
<p><code>f</code>是一个探索函数，例如<code>f=q+K/(1+n)</code>，其中<code>K</code>是一个好奇超参数，它测量智能体被吸引到未知状态的程度。</p>
</li>
</ul>
<h2 id="q_1">近似 Q 学习<a class="headerlink" href="#q_1" title="Permanent link">&para;</a></h2>
<p>Q 学习的主要问题是，它不能很好地扩展到具有许多状态和动作的大（甚至中等）的 MDP。试着用 Q 学习来训练一个智能体去玩 Ms. Pac-Man。Ms. Pac-Man 可以吃超过 250 粒粒子，每一粒都可以存在或不存在（即已经吃过）。因此，可能状态的数目大于 2 的 250 次幂，约等于 10 的 75 次幂（并且这是考虑颗粒的可能状态）。这比在可观测的宇宙中的原子要多得多，所以你绝对无法追踪每一个 Q 值的估计值。</p>
<p>解决方案是找到一个函数，使用可管理数量的参数来近似 Q 值。这被称为近似 Q 学习。多年来，人们都是手工在状态中提取并线性组合特征（例如，最近的鬼的距离，它们的方向等）来估计 Q 值，但是 DeepMind 表明使用深度神经网络可以工作得更好，特别是对于复杂的问题。它不需要任何特征工程。用于估计 Q 值的 DNN 被称为深度 Q 网络（DQN），并且使用近似 Q 学习的 DQN 被称为深度 Q 学习。</p>
<p>在本章的剩余部分，我们将使用深度 Q 学习来训练一个智能体去玩 Ms. Pac-Man，就像 DeepMind 在 2013 所做的那样。代码可以很容易地调整，调整后学习去玩大多数 Atari 游戏的效果都相当好。在大多数动作游戏中，它可以达到超人的技能，但它在长时运行的游戏中却不太好。</p>
<h2 id="q-mspac-man">学习去使用深度 Q 学习来玩 Ms.Pac-Man<a class="headerlink" href="#q-mspac-man" title="Permanent link">&para;</a></h2>
<p>由于我们将使用 Atari 环境，我们必须首先安装 OpenAI gym 的 Atari 环境依赖项。当需要玩其他的时候，我们也会为你想玩的其他 OpenAI gym 环境安装依赖项。在 macOS 上，假设你已经安装了 Homebrew 程序，你需要运行：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>$ brew install cmake boost boost-python sdl2 swig wget
</pre></div>
</td></tr></table>

<p>在 Ubuntu 上，输入以下命令（如果使用 Python 2，用 Python 替换 Python 3）：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>$ apt-get install -y python3-numpy python3-dev cmake zlib1g-dev libjpeg-dev\    xvfb libav-tools xorg-dev python3-opengl libboost-all-dev libsdl2-dev swig 
</pre></div>
</td></tr></table>

<p>随后安装额外的 python 包：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>$ pip3 install --upgrade &#39;gym[all]&#39; 
</pre></div>
</td></tr></table>

<p>如果一切顺利，你应该能够创造一个 Ms.Pac-Man 环境：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;MsPacman-v0&quot;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">obs</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># [长，宽，通道] </span>
<span class="p">(</span><span class="mi">210</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span> 
<span class="n">Discrete</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span> 
</pre></div>
</td></tr></table>

<p>正如你所看到的，有九个离散动作可用，它对应于操纵杆的九个可能位置（左、右、上、下、中、左上等），观察结果是 Atari 屏幕的截图（见图 16-9，左），表示为 3D Numpy 矩阵。这些图像有点大，所以我们将创建一个小的预处理函数，将图像裁剪并缩小到<code>88×80</code>像素，将其转换成灰度，并提高 Ms.Pac-Man 的对比度。这将减少 DQN 所需的计算量，并加快培训练。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">mspacman_color</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">210</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">74</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">preprocess_observation</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>    
    <span class="n">img</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">176</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># 裁剪    </span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 灰度化    </span>
    <span class="n">img</span><span class="p">[</span><span class="n">img</span><span class="o">==</span><span class="n">mspacman_color</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># 提升对比度    </span>
    <span class="n">img</span> <span class="o">=</span> <span class="p">(</span><span class="n">img</span> <span class="o">-</span> <span class="mi">128</span><span class="p">)</span> <span class="o">/</span> <span class="mi">128</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># 正则化为-1到1.   </span>
    <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">88</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
</pre></div>
</td></tr></table>

<p>过程的结果如图 16-9 所示（右）。</p>
<p><img alt="图16-9" src="../../images/chapter_16/16-9.png" /></p>
<p>接下来，让我们创建 DQN。它可以只取一个状态动作对<code>(S,A)</code>作为输入，并输出相应的 Q 值<code>Q(s,a)</code>的估计值，但是由于动作是离散的，所以使用只使用状态<code>S</code>作为输入并输出每个动作的一个 Q 值估计的神经网络是更方便的。DQN 将由三个卷积层组成，接着是两个全连接层，其中包括输出层（如图 16-10）。</p>
<p><img alt="图16-10" src="../../images/chapter_16/16-10.png" /></p>
<p>正如我们将看到的，我们将使用的训练算法需要两个具有相同架构（但不同参数）的 DQN：一个将在训练期间用于驱动 Ms.Pac-Man（the <em>actor</em>，行动者），另一个将观看行动者并从其试验和错误中学习（the <em>critic</em>，评判者）。每隔一定时间，我们把评判者网络复制给行动者网络。因为我们需要两个相同的 DQN，所以我们将创建一个<code>q_network()</code>函数来构建它们：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.contrib.layers</span> <span class="kn">import</span> <span class="n">convolution2d</span><span class="p">,</span> <span class="n">fully_connected</span>
<span class="n">input_height</span> <span class="o">=</span> <span class="mi">88</span> 
<span class="n">input_width</span> <span class="o">=</span> <span class="mi">80</span> 
<span class="n">input_channels</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">conv_n_maps</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> 
<span class="n">conv_kernel_sizes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)]</span> 
<span class="n">conv_strides</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> 
<span class="n">conv_paddings</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;SAME&quot;</span><span class="p">]</span><span class="o">*</span><span class="mi">3</span> 
<span class="n">conv_activation</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">]</span><span class="o">*</span><span class="mi">3</span> 
<span class="n">n_hidden_in</span> <span class="o">=</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">11</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># conv3 有 64 个 11x10 映射</span>
<span class="n">each</span> <span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">512</span> 
<span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span> 
<span class="n">n_outputs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>  <span class="c1"># 9个离散动作</span>
<span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">q_network</span><span class="p">(</span><span class="n">X_state</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>    
    <span class="n">prev_layer</span> <span class="o">=</span> <span class="n">X_state</span>    
    <span class="n">conv_layers</span> <span class="o">=</span> <span class="p">[]</span>    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>        
        <span class="k">for</span> <span class="n">n_maps</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">activation</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">conv_n_maps</span><span class="p">,</span>                                                                  <span class="n">conv_kernel_sizes</span><span class="p">,</span> 
                                                                    <span class="n">conv_strides</span><span class="p">,</span>
                                                                    <span class="n">conv_paddings</span><span class="p">,</span>                                                                  <span class="n">conv_activation</span><span class="p">):</span>           
            <span class="n">prev_layer</span> <span class="o">=</span> <span class="n">convolution2d</span><span class="p">(</span><span class="n">prev_layer</span><span class="p">,</span> 
                                       <span class="n">num_outputs</span><span class="o">=</span><span class="n">n_maps</span><span class="p">,</span> 
                                       <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> 
                                       <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                                       <span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span>           
            <span class="n">conv_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prev_layer</span><span class="p">)</span>       
        <span class="n">last_conv_layer_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">prev_layer</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden_in</span><span class="p">])</span>     
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">last_conv_layer_flat</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span>
                                 <span class="n">activation_fn</span><span class="o">=</span><span class="n">hidden_activation</span><span class="p">,</span>                                                  <span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span> 
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> 
                                  <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                                  <span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span>  

    <span class="n">trainable_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
                                       <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>                
    <span class="n">trainable_vars_by_name</span> <span class="o">=</span> <span class="p">{</span><span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">):]:</span> <span class="n">var</span> 
                              <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">trainable_vars</span><span class="p">}</span>    
    <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">trainable_vars_by_name</span>
</pre></div>
</td></tr></table>

<p>该代码的第一部分定义了DQN体系结构的超参数。然后<code>q_network()</code>函数创建 DQN，将环境的状态<code>X_state</code>作为输入，以及变量范围的名称。请注意，我们将只使用一个观察来表示环境的状态，因为几乎没有隐藏的状态（除了闪烁的物体和鬼魂的方向）。</p>
<p><code>trainable_vars_by_name</code>字典收集了所有 DQN 的可训练变量。当我们创建操作以将评论家 DQN 复制到行动者 DQN 时，这将是有用的。字典的键是变量的名称，去掉与范围名称相对应的前缀的一部分。看起来像这样：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">trainable_vars_by_name</span> 
<span class="p">{</span><span class="s1">&#39;/Conv/biases:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span> <span class="n">at</span> <span class="mh">0x121cf7b50</span><span class="o">&gt;</span><span class="p">,</span> <span class="s1">&#39;/Conv/weights:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> 
<span class="s1">&#39;/Conv_1/biases:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> <span class="s1">&#39;/Conv_1/weights:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> <span class="s1">&#39;/Conv_2/biases:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> <span class="s1">&#39;/Conv_2/weights:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> <span class="s1">&#39;/fully_connected/biases:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> <span class="s1">&#39;/fully_connected/weights:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> <span class="s1">&#39;/fully_connected_1/biases:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">,</span> <span class="s1">&#39;/fully_connected_1/weights:0&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">...&gt;</span><span class="p">}</span>
</pre></div>
</td></tr></table>

<p>现在让我们为两个 DQN 创建输入占位符，以及复制评论家 DQN 给行动者 DQN 的操作：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">X_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> 
                         <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">input_height</span><span class="p">,</span> <span class="n">input_width</span><span class="p">,</span><span class="n">input_channels</span><span class="p">])</span>          
<span class="n">actor_q_values</span><span class="p">,</span> <span class="n">actor_vars</span> <span class="o">=</span> <span class="n">q_network</span><span class="p">(</span><span class="n">X_state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;q_networks/actor&quot;</span><span class="p">)</span> 
<span class="n">critic_q_values</span><span class="p">,</span> <span class="n">critic_vars</span> <span class="o">=</span> <span class="n">q_network</span><span class="p">(</span><span class="n">X_state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;q_networks/critic&quot;</span><span class="p">)</span>
<span class="n">copy_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">actor_var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">critic_vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">])</span>  
            <span class="k">for</span> <span class="n">var_name</span><span class="p">,</span> <span class="n">actor_var</span> <span class="ow">in</span> <span class="n">actor_vars</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span> 
<span class="n">copy_critic_to_actor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="n">copy_ops</span><span class="p">)</span> 
</pre></div>
</td></tr></table>

<p>让我们后退一步：我们现在有两个 DQN，它们都能够将环境状态（即预处理观察）作为输入，并输出在该状态下的每一个可能的动作的估计 Q 值。另外，我们有一个名为<code>copy_critic_to_actor</code>的操作，将评论家 DQN 的所有可训练变量复制到行动者 DQN。我们使用 TensorFlow 的<code>tf.group()</code>函数将所有赋值操作分组到一个方便的操作中。</p>
<p>行动者 DQN 可以用来扮演 Ms.Pac-Man（最初非常糟糕）。正如前面所讨论的，你希望它足够深入地探究游戏，所以通常情况下你想将它用 ε 贪婪策略或另一种探索策略相结合。</p>
<p>但是评论家 DQN 呢？它如何去学习玩游戏？简而言之，它将试图使其预测的 Q 值去匹配行动者通过其经验的游戏估计的 Q 值。具体来说，我们将让行动者玩一段时间，把所有的经验保存在回放记忆存储器中。每个记忆将是一个 5 元组（状态、动作、下一状态、奖励、继续），其中“继续”项在游戏结束时等于 0，否则为 1。接下来，我们定期地从回放存储器中采样一批记忆，并且我们将估计这些存储器中的 Q 值。最后，我们将使用监督学习技术训练评论家 DQN 去预测这些 Q 值。每隔几个训练周期，我们会把评论家 DQN 复制到行动者 DQN。就这样！公式 16-7 示出了用于训练评论家 DQN 的损失函数：</p>
<p><img alt="图E16-7" src="../../images/chapter_16/E16-7.png" /></p>
<p>其中：</p>
<ul>
<li><img alt="s^{(i)}, a^{(i)}, r^{(i)}" src="../../images/tex-a9a5a2bca55c71f2968500d1961e8b9b.gif" />和<img alt="s{'(i)}" src="../../images/tex-49432ed302ad60b5818085cb01e642f1.gif" />分别为状态，行为，回报，和下一状态，均从存储器中第<code>i</code>次采样得到</li>
<li><code>m</code>是记忆批处理的长度</li>
<li>θ critic和<code>θactor</code>为评论者和行动者的参数</li>
<li><img alt="Q(s^{(i)}, a^{(i)}, \theta_{critic})" src="../../images/tex-9ec72967f11b037e1e8ad34f647b04c1.gif" />是评论家 DQN 对第<code>i</code>记忆状态行为 Q 值的预测</li>
<li><img alt="Q(s^{'(i)}, a^{'}, \theta_{actor})" src="../../images/tex-7049aba70c46d7b934955ebdd0424749.gif" />是演员 DQN 在选择动作<code>A'</code>时的下一状态<code>S'</code>的期望 Q 值的预测</li>
<li><code>y</code>是第<code>i</code>记忆的目标 Q 值，注意，它等同于行动者实际观察到的奖励，再加上行动者对如果它能发挥最佳效果（据它所知），未来的回报应该是什么的预测。</li>
<li><code>J</code>为训练评论家 DQN 的损失函数。正如你所看到的，这只是由行动者 DQN 估计的目标 Q 值<code>y</code>和评论家 DQN 对这些 Q 值的预测之间的均方误差。</li>
</ul>
<p>回放记忆是可选的，但强烈推荐使它存在。没有它，你会训练评论家 DQN 使用连续的经验，这可能是相关的。这将引入大量的偏差并且减慢训练算法的收敛性。通过使用回放记忆，我们确保馈送到训练算法的存储器可以是不相关的。</p>
<p>让我们添加评论家 DQN 的训练操作。首先，我们需要能够计算其在存储器批处理中的每个状态动作的预测 Q 值。由于 DQN 为每一个可能的动作输出一个 Q 值，所以我们只需要保持与在该存储器中实际选择的动作相对应的 Q 值。为此，我们将把动作转换成一个热向量（记住这是一个满是 0 的向量，除了第<code>i</code>个索引中的1），并乘以 Q 值：这将删除所有与记忆动作对应的 Q 值外的 Q 值。然后只对第一轴求和，以获得每个存储器所需的 Q 值预测。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">X_action</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span> 
<span class="n">q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">critic_q_values</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">X_action</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> 
</pre></div>
</td></tr></table>

<p>接下来，让我们添加训练操作，假设目标Q值将通过占位符馈入。我们还创建了一个不可训练的变量<code>global_step</code>。优化器的<code>minimize()</code>操作将负责增加它。另外，我们创建了<code>init</code>操作和<code>Saver</code>。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> 
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">))</span> 
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;global_step&#39;</span><span class="p">)</span> 
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> 
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span> 
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span> 
</pre></div>
</td></tr></table>

<p>这就是训练阶段的情况。在我们查看执行阶段之前，我们需要一些工具。首先，让我们从回放记忆开始。我们将使用一个<code>deque</code>列表，因为在将数据推送到队列中并在达到最大内存大小时从列表的末尾弹出它们使是非常有效的。我们还将编写一个小函数来随机地从回放记忆中采样一批处理：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="n">replay_memory_size</span> <span class="o">=</span> <span class="mi">10000</span> 
<span class="n">replay_memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">([],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">replay_memory_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample_memories</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>    
    <span class="n">indices</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">replay_memory</span><span class="p">))[:</span><span class="n">batch_size</span><span class="p">]</span>    
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span> <span class="c1"># state, action, reward, next_state, continue    </span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>        
        <span class="n">memory</span> <span class="o">=</span> <span class="n">replay_memory</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>        
        <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="n">memory</span><span class="p">):</span>            
            <span class="n">col</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>    
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">]</span>    
    <span class="k">return</span> <span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cols</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cols</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">cols</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span><span class="n">cols</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> 
</pre></div>
</td></tr></table>

<p>接下来，我们需要行动者来探索游戏。我们使用 ε 贪婪策略，并在 50000 个训练步骤中逐步将<code>ε</code>从 1 降低到 0.05。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">eps_min</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">eps_max</span> <span class="o">=</span> <span class="mf">1.0</span> 
<span class="n">eps_decay_steps</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="k">def</span> <span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>    
    <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">eps_min</span><span class="p">,</span> <span class="n">eps_max</span> <span class="o">-</span> <span class="p">(</span><span class="n">eps_max</span><span class="o">-</span><span class="n">eps_min</span><span class="p">)</span> <span class="o">*</span> <span class="n">step</span><span class="o">/</span><span class="n">eps_decay_steps</span><span class="p">)</span>   
     <span class="k">if</span> <span class="n">rnd</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>        
        <span class="k">return</span> <span class="n">rnd</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">n_outputs</span><span class="p">)</span> <span class="c1"># 随机动作   </span>
    <span class="k">else</span><span class="p">:</span>        
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span> <span class="c1"># 最优动作</span>
</pre></div>
</td></tr></table>

<p>就是这样！我们准备好开始训练了。执行阶段不包含太复杂的东西，但它有点长，所以深呼吸。准备好了吗？来次够！首先，让我们初始化几个变量：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">100000</span>  <span class="c1"># 总的训练步长 </span>
<span class="n">training_start</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># 在游戏1000次迭代后开始训练</span>
<span class="n">training_interval</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># 每3次迭代训练一次</span>
<span class="n">save_steps</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># 每50训练步长保存模型</span>
<span class="n">copy_steps</span> <span class="o">=</span> <span class="mi">25</span>  <span class="c1"># 每25训练步长后复制评论家Q值到行动者</span>
<span class="n">discount_rate</span> <span class="o">=</span> <span class="mf">0.95</span> 
<span class="n">skip_start</span> <span class="o">=</span> <span class="mi">90</span>  <span class="c1"># 跳过游戏开始(只是等待时间) </span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span> 
<span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 游戏迭代</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;./my_dqn.ckpt&quot;</span> 
<span class="n">done</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># env 需要被重置 </span>
</pre></div>
</td></tr></table>

<p>接下来，让我们打开会话并开始训练：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>    
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">):</span>        
        <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>    
    <span class="k">else</span><span class="p">:</span>        
        <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>    
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>        
        <span class="n">step</span> <span class="o">=</span> <span class="n">global_step</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>        
        <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="n">n_steps</span><span class="p">:</span>            
            <span class="k">break</span>        
        <span class="n">iteration</span> <span class="o">+=</span> <span class="mi">1</span>        
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="c1"># 游戏结束，重来            </span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>            
            <span class="k">for</span> <span class="n">skip</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">skip_start</span><span class="p">):</span> <span class="c1"># 跳过游戏开头              </span>
                <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>            
                <span class="n">state</span> <span class="o">=</span> <span class="n">preprocess_observation</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

        <span class="c1"># 行动者评估要干什么        </span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="n">actor_q_values</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X_state</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">]})</span>        
        <span class="n">action</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

        <span class="c1"># 行动者开始玩游戏       </span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>        
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">preprocess_observation</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

        <span class="c1"># 让我们记下来刚才发生了啥        </span>
        <span class="n">replay_memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">done</span><span class="p">))</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="k">if</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="n">training_start</span> <span class="ow">or</span> <span class="n">iteration</span> <span class="o">%</span> <span class="n">training_interval</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>                <span class="k">continue</span>

        <span class="c1"># 评论家学习        </span>
        <span class="n">X_state_val</span><span class="p">,</span> <span class="n">X_action_val</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">X_next_state_val</span><span class="p">,</span> <span class="n">continues</span> <span class="o">=</span> <span class="p">(</span>            <span class="n">sample_memories</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>        
        <span class="n">next_q_values</span> <span class="o">=</span> <span class="n">actor_q_values</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X_state</span><span class="p">:</span> <span class="n">X_next_state_val</span><span class="p">})</span>    
        <span class="n">max_next_q_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">next_q_values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>        
        <span class="n">y_val</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="n">continues</span> <span class="o">*</span> <span class="n">discount_rate</span> <span class="o">*</span> <span class="n">max_next_q_values</span>        
        <span class="n">training_op</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X_state</span><span class="p">:</span> <span class="n">X_state_val</span><span class="p">,</span><span class="n">X_action</span><span class="p">:</span> <span class="n">X_action_val</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_val</span><span class="p">})</span>

        <span class="c1"># 复制评论家Q值到行动者        </span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">copy_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>            
            <span class="n">copy_critic_to_actor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

        <span class="c1"># 保存模型        </span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">save_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>            
            <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>如果检查点文件存在，我们就开始恢复模型，否则我们只需初始化变量。然后，主循环开始，其中<code>iteration</code>计算从程序开始以来游戏步骤的总数，同时<code>step</code>计算从训练开始的训练步骤的总数（如果恢复了检查点，也恢复全局步骤）。然后代码重置游戏（跳过第一个无聊的等待游戏的步骤，这步骤啥都没有）。接下来，行动者评估该做什么，并且玩游戏，并且它的经验被存储在回放记忆中。然后，每隔一段时间（热身期后），评论家开始一个训练步骤。它采样一批回放记忆，并要求行动者估计下一状态的所有动作的Q值，并应用公式 16-7 来计算目标 Q 值<code>y_val</code>.这里唯一棘手的部分是，我们必须将下一个状态的 Q 值乘以<code>continues</code>向量，以将对应于游戏结束的记忆 Q 值清零。接下来，我们进行训练操作，以提高评论家预测 Q 值的能力。最后，我们定期将评论家的 Q 值复制给行动者，然后保存模型。</p>
<p>不幸的是，训练过程是非常缓慢的：如果你使用你的破笔记本电脑进行训练的话，想让 Ms. Pac-Man 变好一点点你得花好几天，如果你看看学习曲线，计算一下每次的平均奖励，你会发现到它是非常嘈杂的。在某些情况下，很长一段时间内可能没有明显的进展，直到智能体学会在合理的时间内生存。如前所述，一种解决方案是将尽可能多的先验知识注入到模型中（例如，通过预处理、奖励等），也可以尝试通过首先训练它来模仿基本策略来引导模型。在任何情况下，RL仍然需要相当多的耐心和调整，但最终结果是非常令人兴奋的。</p>
<h2 id="_9">练习<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h2>
<ol>
<li>你怎样去定义强化学习？它与传统的监督以及非监督学习有什么不同？</li>
<li>你能想到什么本章没有提到过的强化学习应用？智能体是什么？什么是可能的动作，什么是奖励？</li>
<li>什么是衰减率？如果你修改了衰减率那最优策略会变化吗？</li>
<li>你怎么去定义强化学习智能体的表现？</li>
<li>什么是信用评估问题？它怎么出现的？你怎么解决？</li>
<li>使用回放记忆的目的是什么？</li>
<li>什么是闭策略 RL 算法？</li>
<li>使用深度 Q 学习来处理 OpenAI gym 的“BypedalWalker-v2” 。QNET 不需要对这个任务使用非常深的网络。</li>
<li>使用策略梯度训练智能体扮演 Pong，一个著名的 Atari 游戏（PANV0 在 OpenAI gym 的 Pong-v0）。注意：个人的观察不足以说明球的方向和速度。一种解决方案是一次将两次观测传递给神经网络策略。为了减少维度和加速训练，你必须预先处理这些图像（裁剪，调整大小，并将它们转换成黑白），并可能将它们合并成单个图像（例如去叠加它们）。</li>
<li>如果你有大约 100 美元备用，你可以购买 Raspberry Pi 3 再加上一些便宜的机器人组件，在 PI 上安装 TensorFlow，然后让我们嗨起来~！举个例子，看看 Lukas Biewald 的这个有趣的帖子，或者看看 GoPiGo 或 BrickPi。为什么不尝试通过使用策略梯度训练机器人来构建真实的 cartpole ？或者造一个机器人蜘蛛，让它学会走路；当它接近某个目标时，给予奖励（你需要传感器来测量目标的距离）。唯一的限制就是你的想象力。</li>
</ol>
<p>练习答案均在附录 A。</p>
<h2 id="_10">感谢<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h2>
<p>在我们结束这本书的最后一章之前，我想感谢你们读到最后一段。我真心希望你能像我写这本书一样愉快地阅读这本书，这对你的项目，或多或少都是有用的。</p>
<p>如果发现错误，请发送反馈。更一般地说，我很想知道你的想法，所以请不要犹豫，通过 O'Reilly 来与我联系，或者通过 ageron/handson-ml GITHUB 项目来练习。</p>
<p>对你来说，我最好的建议是练习和练习：如果你还没有做过这些练习，试着使用 Juyter notebook 参加所有的练习，加入 kaggle 网站或其他 ML 社区，看 ML 课程，阅读论文，参加会议，会见专家。您可能还想研究我们在本书中没有涉及的一些主题，包括推荐系统、聚类算法、异常检测算法和遗传算法。</p>
<p>我最大的希望是，这本书将激励你建立一个美妙的 ML 应用程序，这将有利于我们所有人！那会是什么呢？</p>
<blockquote>
<p>2016 年 11 月 26 日，奥列伦·格伦</p>
<p>你的支持，是我们每个开源工作者的骄傲～</p>
</blockquote>
                
                  
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://hai5g.cn/aiwiki/hands-on-ml-zh/docs/16.强化学习/";
      this.page.identifier =
        "/hands-on-ml-zh/docs/16.强化学习/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//AI-Wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../15.自编码器/" title="自编码器" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                自编码器
              </span>
            </div>
          </a>
        
        
          <a href="../B.机器学习项目清单/" title="B.机器学习项目清单" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                B.机器学习项目清单
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 AI Wiki Team
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.39abc4af.js"></script>
      
        
        
          
          <script src="../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
        <script src="https://cdn.jsdelivr.net/gh/ethantw/Han@3.3.0/dist/han.min.js"></script>
      
        <script src="../../../_static/js/extra.js?v=10"></script>
      
        <script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>