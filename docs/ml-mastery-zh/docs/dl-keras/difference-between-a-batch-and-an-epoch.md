# 神经网络中批量和迭代之间的区别是什么？

> 原文： [https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)

随机梯度下降是一种学习算法，具有许多超参数。

两个经常让初学者感到困惑的超参数是批量大小和时代数。它们都是整数值，似乎做同样的事情。

在这篇文章中，您将发现随机梯度下降中批次和时期之间的差异。

阅读这篇文章后，你会知道：

*   随机梯度下降是一种迭代学习算法，它使用训练数据集来更新模型。
*   批量大小是梯度下降的超参数，在模型的内部参数更新之前控制训练样本的数量。
*   时期数是梯度下降的超参数，其控制通过训练数据集的完整遍数。

让我们开始吧。

![What is the Difference Between a Batch and an Epoch in a Neural Network?](img/1f3275bfb5407176028269720f3a11da.png)

神经网络中批量和迭代之间的区别是什么？
[Graham Cook](https://www.flickr.com/photos/grazza123/9754438586/) 的照片，保留一些权利。

## 概观

这篇文章分为五个部分;他们是：

1.  随机梯度下降
2.  什么是样品？
3.  什么是批次？
4.  什么是大迭代？
5.  Batch 和 Epoch 有什么区别？

## 随机梯度下降

随机梯度下降（Stochastic Gradient Descent，简称 SGD）是一种用于训练机器学习算法的优化算法，最值得注意的是深度学习中使用的人工神经网络。

该算法的工作是找到一组内部模型参数，这些参数在某些表现测量中表现良好，例如对数损失或均方误差。

优化是一种搜索过程，您可以将此搜索视为学习。优化算法称为“_ 梯度下降 _”，其中“_ 梯度 _”是指误差梯度或误差斜率的计算，“下降”是指沿着该斜率向下移动朝着某种最低程度的错误。

该算法是迭代的。这意味着搜索过程发生在多个不连续的步骤上，每个步骤都希望略微改进模型参数。

每个步骤都涉及将模型与当前内部参数集一起使用，以对某些样本进行预测，将预测与实际预期结果进行比较，计算误差，并使用误差更新内部模型参数。

该更新过程对于不同的算法是不同的，但是在人工神经网络的情况下，使用[反向传播](https://en.wikipedia.org/wiki/Backpropagation)更新算法。

在我们深入研究批次和时代之前，让我们来看看样本的含义。

了解有关梯度下降的更多信息：

*   [机器学习的梯度下降](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)

## 什么是样品？

样本是单行数据。

它包含输入算法的输入和用于与预测进行比较并计算错误的输出。

训练数据集由许多行数据组成，例如，很多样品。样本也可以称为实例，观察，输入向量或特征向量。

现在我们知道样本是什么，让我们定义一个批量。

## 什么是批次？

批量大小是一个超参数，用于定义在更新内部模型参数之前要处理的样本数。

将批量视为对一个或多个样本进行迭代并进行预测的循环。在批量结束时，将预测与预期输出变量进行比较，并计算误差。从该错误中，更新算法用于改进模型，例如，沿误差梯度向下移动。

训练数据集可以分为一个或多个批次。

当所有训练样本用于创建一个批次时，学习算法称为批量梯度下降。当批量是一个样本的大小时，学习算法称为随机梯度下降。当批量大小超过一个样本且小于训练数据集的大小时，学习算法称为小批量梯度下降。

*   **批量梯度下降**。批量大小=训练集的大小
*   **随机梯度下降**。批量大小= 1
*   **Mini-Batch Gradient Descent** 。 1＆lt;批量大小＆lt;训练集的大小

在小批量梯度下降的情况下，流行的批量大小包括 32,64 和 128 个样本。您可能会在文献和教程中看到这些值在模型中使用。

**如果数据集不能按批量大小均匀分配怎么办？**

在训练模型时，这可能并且确实经常发生。它只是意味着最终批次的样品数量少于其他批次。

或者，您可以从数据集中删除一些样本或更改批量大小，以便数据集中的样本数按批次大小均匀划分。

有关这些梯度下降变化之间差异的更多信息，请参阅帖子：

*   [微量批量梯度下降的简要介绍以及如何配置批量大小](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)

批量涉及使用样本更新模型;接下来，让我们来看一个时代。

## 什么是大迭代？

时期数是一个超参数，它定义了学习算法在整个训练数据集中的工作次数。

一个时期意味着训练数据集中的每个样本都有机会更新内部模型参数。时期由一个或多个批次组成。例如，如上所述，具有一批的时期称为批量梯度下降学习算法。

您可以考虑在每个循环在训练数据集上进行的时期数量的 for 循环。在这个 for 循环中是另一个嵌套的 for 循环，它遍历每批样本，其中一个批次具有指定的“批量大小”样本数。

迭代的数量传统上很大，通常是数百或数千，允许学习算法运行直到模型的误差被充分最小化。您可能会看到文献和教程设置为 10,100,500,1000 和更大的时期数量的示例。

通常创建线图，其显示沿 x 轴的时期作为时间以及模型在 y 轴上的误差或技能。这些图有时称为学习曲线。这些图可以帮助诊断模型是否已经过度学习，学习不足或是否适合训练数据集。

有关使用 LSTM 网络学习曲线的诊断信息，请参阅帖子：

*   [如何诊断 LSTM 模型的过度拟合和欠拟合](https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/)

如果仍然不清楚，让我们看看批次和时代之间的差异。

## Batch 和 Epoch 有什么区别？

批量大小是在更新模型之前处理的多个样本。

时期数是通过训练数据集的完整传递次数。

批量的大小必须大于或等于 1 且小于或等于训练数据集中的样本数。

时期数可以设置为 1 和无穷大之间的整数值。您可以根据需要运行算法，甚至可以使用除固定数量的迭代之外的其他标准来停止算法，例如模型错误随时间的变化（或缺少更改）。

它们都是整数值，并且它们都是学习算法的超参数，例如，学习过程的参数，而不是学习过程中找到的内部模型参数。

您必须为学习算法指定批次大小和时期数。

如何配置这些参数没有神奇的规则。您必须尝试不同的值，看看哪种方法最适合您的问题。

### 工作示例

最后，让我们用一个小例子来具体化。

假设您有一个包含 200 个样本（数据行）的数据集，并且您选择的批量大小为 5 和 1,000 个迭代。

这意味着数据集将分为 40 个批次，每个批次有 5 个样本。每批五个样品后，模型权重将更新。

这也意味着一个时代将涉及 40 个批次或 40 个模型更新。

有 1000 个时期，模型将暴露或传递整个数据集 1,000 次。在整个训练过程中总共有 40,000 批次。

## 进一步阅读

如果您希望深入了解，本节将提供有关该主题的更多资源。

*   [机器学习的梯度下降](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)
*   [微量批量梯度下降的简要介绍以及如何配置批量大小](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)
*   [如何诊断 LSTM 模型的过度拟合和欠拟合](https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/)
*   [维基百科上的随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
*   [维基百科上的反向传播](https://en.wikipedia.org/wiki/Backpropagation)

## 摘要

在这篇文章中，您发现了随机梯度下降中批次和时期之间的差异。

具体来说，你学到了：

*   随机梯度下降是一种迭代学习算法，它使用训练数据集来更新模型。
*   批量大小是梯度下降的超参数，在模型的内部参数更新之前控制训练样本的数量。
*   时期数是梯度下降的超参数，其控制通过训练数据集的完整遍数。

你有任何问题吗？
在下面的评论中提出您的问题，我会尽力回答。