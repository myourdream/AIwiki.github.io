



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="AI Wiki 是一个编程竞赛知识整合站点，提供有趣又实用的编程竞赛知识以及其他有帮助的内容，帮助广大编程竞赛爱好者更快更深入地学习编程竞赛">
      
      
        <link rel="canonical" href="https://hai5g.cn/aiwiki/qa500/ch03_深度学习基础/第三章_深度学习基础/">
      
      
        <meta name="author" content="AI Wiki Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../../favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>第三章_深度学习基础 - AI Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,400i,700|Fira+Mono">
        <style>body,input{font-family:"Fira Sans","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Fira Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
      <link rel="manifest" href="../../../manifest.webmanifest">
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/ah@1.5.0/han.min.css">
    
      <link rel="stylesheet" href="../../../_static/css/extra.css?v=11">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#_1" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-header-nav__button md-logo">
          
            <i class="md-icon">school</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              AI Wiki
            </span>
            <span class="md-header-nav__topic">
              第三章_深度学习基础
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../.." title="简介" class="md-tabs__link">
          简介
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../Coursera_ML_AndrewNg/" title="机器学习" class="md-tabs__link">
          机器学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../" title="深度学习500问" class="md-tabs__link md-tabs__link--active">
          深度学习500问
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../lihang/" title="统计学习" class="md-tabs__link">
          统计学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../hands-on-ml-zh/README.old/" title="Sklearn与TensorFlow" class="md-tabs__link">
          Sklearn与TensorFlow
        </a>
      
    </li>
  

      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-nav__button md-logo">
      
        <i class="md-icon">school</i>
      
    </a>
    AI Wiki
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      简介
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        简介
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../.." title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/AI学习路线/" title="AI学习路线1" class="md-nav__link">
      AI学习路线1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/ai-union-201904/" title="AI学习路线2" class="md-nav__link">
      AI学习路线2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v0.1/" title="AI 路线图v0.1" class="md-nav__link">
      AI 路线图v0.1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v0.2/" title="AI 路线图v0.2" class="md-nav__link">
      AI 路线图v0.2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v1.0/" title="ApacheCN 人工智能知识树" class="md-nav__link">
      ApacheCN 人工智能知识树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7" type="checkbox" id="nav-1-7">
    
    <label class="md-nav__link" for="nav-1-7">
      工具软件
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-1-7">
        工具软件
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/judgers/" title="评测工具" class="md-nav__link">
      评测工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/editors/" title="编辑工具" class="md-nav__link">
      编辑工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/wsl/" title="WSL (Windows 10)" class="md-nav__link">
      WSL (Windows 10)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/spj/" title="Special Judge" class="md-nav__link">
      Special Judge
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7-5" type="checkbox" id="nav-1-7-5">
    
    <label class="md-nav__link" for="nav-1-7-5">
      Testlib
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-1-7-5">
        Testlib
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/" title="Testlib 简介" class="md-nav__link">
      Testlib 简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/general/" title="通用" class="md-nav__link">
      通用
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/generator/" title="Generator" class="md-nav__link">
      Generator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/validator/" title="Validator" class="md-nav__link">
      Validator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/interactor/" title="Interactor" class="md-nav__link">
      Interactor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/checker/" title="Checker" class="md-nav__link">
      Checker
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/docker-deploy/" title="Docker 部署" class="md-nav__link">
      Docker 部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/about/" title="关于本项目" class="md-nav__link">
      关于本项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/faq/" title="F.A.Q." class="md-nav__link">
      F.A.Q.
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      机器学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        机器学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/SUMMARY/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/math/" title="数学基础" class="md-nav__link">
      数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week1/" title="week1" class="md-nav__link">
      week1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week2/" title="week2" class="md-nav__link">
      week2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week3/" title="week3" class="md-nav__link">
      week3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week4/" title="week4" class="md-nav__link">
      week4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week5/" title="week5" class="md-nav__link">
      week5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week6/" title="week6" class="md-nav__link">
      week6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week7/" title="week7" class="md-nav__link">
      week7
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week8/" title="week8" class="md-nav__link">
      week8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week9/" title="week9" class="md-nav__link">
      week9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week10/" title="week10" class="md-nav__link">
      week10
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      深度学习500问
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        深度学习500问
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../content/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch01_math/ch01_math/" title="第一章_数学基础" class="md-nav__link">
      第一章_数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch02_机器学习基础/第二章_机器学习基础/" title="第二章_机器学习基础" class="md-nav__link">
      第二章_机器学习基础
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        第三章_深度学习基础
      </label>
    
    <a href="./" title="第三章_深度学习基础" class="md-nav__link md-nav__link--active">
      第三章_深度学习基础
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#31" title="3.1 基本概念" class="md-nav__link">
    3.1 基本概念
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#311" title="3.1.1 神经网络组成？" class="md-nav__link">
    3.1.1 神经网络组成？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#312" title="3.1.2 神经网络有哪些常用模型结构？" class="md-nav__link">
    3.1.2 神经网络有哪些常用模型结构？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#313" title="3.1.3 如何选择深度学习开发平台？" class="md-nav__link">
    3.1.3 如何选择深度学习开发平台？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#314" title="3.1.4 为什么使用深层表示?" class="md-nav__link">
    3.1.4 为什么使用深层表示?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#315" title="3.1.5 为什么深层神经网络难以训练？" class="md-nav__link">
    3.1.5 为什么深层神经网络难以训练？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#316" title="3.1.6 深度学习和机器学习有什么不同？" class="md-nav__link">
    3.1.6 深度学习和机器学习有什么不同？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32" title="3.2 网络操作与计算" class="md-nav__link">
    3.2 网络操作与计算
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321" title="3.2.1 前向传播与反向传播？" class="md-nav__link">
    3.2.1 前向传播与反向传播？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322" title="3.2.2 如何计算神经网络的输出？" class="md-nav__link">
    3.2.2 如何计算神经网络的输出？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323" title="3.2.3 如何计算卷积神经网络输出值？" class="md-nav__link">
    3.2.3 如何计算卷积神经网络输出值？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#324-pooling" title="3.2.4 如何计算 Pooling 层输出值输出值？" class="md-nav__link">
    3.2.4 如何计算 Pooling 层输出值输出值？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#325" title="3.2.5 实例理解反向传播" class="md-nav__link">
    3.2.5 实例理解反向传播
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#326" title="3.2.6 神经网络更“深”有什么意义？" class="md-nav__link">
    3.2.6 神经网络更“深”有什么意义？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33" title="3.3 超参数" class="md-nav__link">
    3.3 超参数
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#331" title="3.3.1 什么是超参数？" class="md-nav__link">
    3.3.1 什么是超参数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#332" title="3.3.2 如何寻找超参数的最优值？" class="md-nav__link">
    3.3.2 如何寻找超参数的最优值？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#333" title="3.3.3 超参数搜索一般过程？" class="md-nav__link">
    3.3.3 超参数搜索一般过程？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#34" title="3.4 激活函数" class="md-nav__link">
    3.4 激活函数
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#341" title="3.4.1 为什么需要非线性激活函数？" class="md-nav__link">
    3.4.1 为什么需要非线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#342" title="3.4.2 常见的激活函数及图像" class="md-nav__link">
    3.4.2 常见的激活函数及图像
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#343" title="3.4.3 常见激活函数的导数计算？" class="md-nav__link">
    3.4.3 常见激活函数的导数计算？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#344" title="3.4.4 激活函数有哪些性质？" class="md-nav__link">
    3.4.4 激活函数有哪些性质？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#345" title="3.4.5 如何选择激活函数？" class="md-nav__link">
    3.4.5 如何选择激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#346-relu" title="3.4.6 使用 ReLu 激活函数的优点？" class="md-nav__link">
    3.4.6 使用 ReLu 激活函数的优点？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#347" title="3.4.7 什么时候可以用线性激活函数？" class="md-nav__link">
    3.4.7 什么时候可以用线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#348-relu-0" title="3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？" class="md-nav__link">
    3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#349-softmax" title="3.4.9 Softmax 定义及作用" class="md-nav__link">
    3.4.9 Softmax 定义及作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3410-softmax" title="3.4.10 Softmax 函数如何应用于多分类？" class="md-nav__link">
    3.4.10 Softmax 函数如何应用于多分类？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3411" title="3.4.11 交叉熵代价函数定义及其求导推导" class="md-nav__link">
    3.4.11 交叉熵代价函数定义及其求导推导
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3412-tanhsigmoid" title="3.4.12 为什么Tanh收敛速度比Sigmoid快？" class="md-nav__link">
    3.4.12 为什么Tanh收敛速度比Sigmoid快？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#35-batch_size" title="3.5 Batch_Size" class="md-nav__link">
    3.5 Batch_Size
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#351-batch_size" title="3.5.1 为什么需要 Batch_Size？" class="md-nav__link">
    3.5.1 为什么需要 Batch_Size？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#352-batch_size" title="3.5.2 Batch_Size 值的选择" class="md-nav__link">
    3.5.2 Batch_Size 值的选择
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#353-batch_size" title="3.5.3 在合理范围内，增大Batch_Size有何好处？" class="md-nav__link">
    3.5.3 在合理范围内，增大Batch_Size有何好处？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#354-batch_size" title="3.5.4 盲目增大 Batch_Size 有何坏处？" class="md-nav__link">
    3.5.4 盲目增大 Batch_Size 有何坏处？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#355-batch_size" title="3.5.5 调节 Batch_Size 对训练效果影响到底如何？" class="md-nav__link">
    3.5.5 调节 Batch_Size 对训练效果影响到底如何？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#36" title="3.6 归一化" class="md-nav__link">
    3.6 归一化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#361" title="3.6.1 归一化含义？" class="md-nav__link">
    3.6.1 归一化含义？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#362" title="3.6.2 为什么要归一化？" class="md-nav__link">
    3.6.2 为什么要归一化？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#363" title="3.6.3 为什么归一化能提高求解最优解速度？" class="md-nav__link">
    3.6.3 为什么归一化能提高求解最优解速度？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#364-3d" title="3.6.4 3D 图解未归一化" class="md-nav__link">
    3.6.4 3D 图解未归一化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#365" title="3.6.5 归一化有哪些类型？" class="md-nav__link">
    3.6.5 归一化有哪些类型？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#366" title="3.6.6 局部响应归一化作用" class="md-nav__link">
    3.6.6 局部响应归一化作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#367" title="3.6.7 理解局部响应归一化" class="md-nav__link">
    3.6.7 理解局部响应归一化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#368-batch-normalization" title="3.6.8 什么是批归一化（Batch Normalization）" class="md-nav__link">
    3.6.8 什么是批归一化（Batch Normalization）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#369-bn" title="3.6.9 批归一化（BN）算法的优点" class="md-nav__link">
    3.6.9 批归一化（BN）算法的优点
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3610-bn" title="3.6.10 批归一化（BN）算法流程" class="md-nav__link">
    3.6.10 批归一化（BN）算法流程
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3611" title="3.6.11 批归一化和群组归一化比较" class="md-nav__link">
    3.6.11 批归一化和群组归一化比较
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3612-weight-normalizationbatch-normalization" title="3.6.12 Weight Normalization和Batch Normalization比较" class="md-nav__link">
    3.6.12 Weight Normalization和Batch Normalization比较
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3613-batch-normalization" title="3.6.13 Batch Normalization在什么时候用比较合适？" class="md-nav__link">
    3.6.13 Batch Normalization在什么时候用比较合适？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#37-fine-tuning" title="3.7 预训练与微调(fine tuning)" class="md-nav__link">
    3.7 预训练与微调(fine tuning)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#371" title="3.7.1 为什么无监督预训练可以帮助深度学习？" class="md-nav__link">
    3.7.1 为什么无监督预训练可以帮助深度学习？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#372-fine-tuning" title="3.7.2 什么是模型微调fine tuning" class="md-nav__link">
    3.7.2 什么是模型微调fine tuning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#373" title="3.7.3 微调时候网络参数是否更新？" class="md-nav__link">
    3.7.3 微调时候网络参数是否更新？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#374-fine-tuning" title="3.7.4 fine-tuning 模型的三种状态" class="md-nav__link">
    3.7.4 fine-tuning 模型的三种状态
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#38" title="3.8 权重偏差初始化" class="md-nav__link">
    3.8 权重偏差初始化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#381-0" title="3.8.1 全都初始化为 0" class="md-nav__link">
    3.8.1 全都初始化为 0
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#382" title="3.8.2 全都初始化为同样的值" class="md-nav__link">
    3.8.2 全都初始化为同样的值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#383" title="3.8.3 初始化为小的随机数" class="md-nav__link">
    3.8.3 初始化为小的随机数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#384-1sqrt-n" title="3.8.4 用 $ 1/\sqrt n $ 校准方差" class="md-nav__link">
    3.8.4 用 $ 1/\sqrt n $ 校准方差
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#385-sparse-initialazation" title="3.8.5 稀疏初始化(Sparse Initialazation)" class="md-nav__link">
    3.8.5 稀疏初始化(Sparse Initialazation)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#386" title="3.8.6 初始化偏差" class="md-nav__link">
    3.8.6 初始化偏差
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#39" title="3.9 学习率" class="md-nav__link">
    3.9 学习率
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#391" title="3.9.1 学习率的作用" class="md-nav__link">
    3.9.1 学习率的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#392" title="3.9.2 学习率衰减常用参数有哪些" class="md-nav__link">
    3.9.2 学习率衰减常用参数有哪些
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#393" title="3.9.3 分段常数衰减" class="md-nav__link">
    3.9.3 分段常数衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#394" title="3.9.4 指数衰减" class="md-nav__link">
    3.9.4 指数衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#395" title="3.9.5 自然指数衰减" class="md-nav__link">
    3.9.5 自然指数衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#396" title="3.9.6 多项式衰减" class="md-nav__link">
    3.9.6 多项式衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#397" title="3.9.7 余弦衰减" class="md-nav__link">
    3.9.7 余弦衰减
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#312-dropout" title="3.12 Dropout 系列问题" class="md-nav__link">
    3.12 Dropout 系列问题
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#3121" title="3.12.1 为什么要正则化？" class="md-nav__link">
    3.12.1 为什么要正则化？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3122" title="3.12.2 为什么正则化有利于预防过拟合？" class="md-nav__link">
    3.12.2 为什么正则化有利于预防过拟合？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3123-dropout" title="3.12.3 理解dropout正则化" class="md-nav__link">
    3.12.3 理解dropout正则化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3124-dropout" title="3.12.4 dropout率的选择" class="md-nav__link">
    3.12.4 dropout率的选择
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3125-dropout" title="3.12.5 dropout有什么缺点？" class="md-nav__link">
    3.12.5 dropout有什么缺点？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#313_1" title="3.13 深度学习中常用的数据增强方法？" class="md-nav__link">
    3.13 深度学习中常用的数据增强方法？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#314-internal-covariate-shift" title="3.14 如何理解 Internal Covariate Shift？" class="md-nav__link">
    3.14 如何理解 Internal Covariate Shift？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="参考文献" class="md-nav__link">
    参考文献
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch04_经典网络/第四章_经典网络/" title="第四章_经典网络" class="md-nav__link">
      第四章_经典网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch05_卷积神经网络(CNN)/第五章 卷积神经网络（CNN）/" title="第五章 卷积神经网络（CNN）" class="md-nav__link">
      第五章 卷积神经网络（CNN）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch06_循环神经网络(RNN)/第六章_循环神经网络(RNN)/" title="第六章_循环神经网络(RNN)" class="md-nav__link">
      第六章_循环神经网络(RNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch07_生成对抗网络(GAN)/ch7/" title="第七章_生成对抗网络(GAN)" class="md-nav__link">
      第七章_生成对抗网络(GAN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch08_目标检测/第八章_目标检测/" title="第八章_目标检测" class="md-nav__link">
      第八章_目标检测
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch09_图像分割/第九章_图像分割/" title="第九章_图像分割" class="md-nav__link">
      第九章_图像分割
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch10_强化学习/第十章_强化学习/" title="第十章_强化学习" class="md-nav__link">
      第十章_强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch11_迁移学习/第十一章_迁移学习/" title="第十一章_迁移学习" class="md-nav__link">
      第十一章_迁移学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch12_网络搭建及训练/第十二章_网络搭建及训练/" title="第十二章_网络搭建及训练" class="md-nav__link">
      第十二章_网络搭建及训练
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch13_优化算法/第十三章_优化算法/" title="第十三章_优化算法" class="md-nav__link">
      第十三章_优化算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch14_超参数调整/第十四章_超参数调整/" title="第十四章_超参数调整" class="md-nav__link">
      第十四章_超参数调整
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch15_GPU和框架选型/第十五章_异构运算、GPU及框架选型/" title="第十五章_异构运算、GPU及框架选型" class="md-nav__link">
      第十五章_异构运算、GPU及框架选型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch16_自然语言处理(NLP)/第十六章_NLP/" title="第十六章_NLP" class="md-nav__link">
      第十六章_NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署/" title="第十七章_模型压缩、加速及移动端部署" class="md-nav__link">
      第十七章_模型压缩、加速及移动端部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch18_后端架构选型、离线及实时计算/第十八章_后端架构选型、离线及实时计算/" title="第十八章_后端架构选型、离线及实时计算" class="md-nav__link">
      第十八章_后端架构选型、离线及实时计算
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch18_后端架构选型及应用场景/第十八章_后端架构选型及应用场景/" title="第十八章_后端架构选型及应用场景" class="md-nav__link">
      第十八章_后端架构选型及应用场景
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      统计学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        统计学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH01/" title="统计学习及监督学习概论" class="md-nav__link">
      统计学习及监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH02/" title="感知机" class="md-nav__link">
      感知机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH03/" title="K近邻法" class="md-nav__link">
      K近邻法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH04/" title="朴素贝叶斯法" class="md-nav__link">
      朴素贝叶斯法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH05/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH06/" title="逻辑斯蒂回归与最大熵模型" class="md-nav__link">
      逻辑斯蒂回归与最大熵模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH07/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH08/" title="提升方法" class="md-nav__link">
      提升方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH09/" title="EM算法及其推广" class="md-nav__link">
      EM算法及其推广
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH10/" title="隐马尔可夫模型" class="md-nav__link">
      隐马尔可夫模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH11/" title="条件随机场" class="md-nav__link">
      条件随机场
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH12/" title="监督学习方法总结" class="md-nav__link">
      监督学习方法总结
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH13/" title="无监督学习概论" class="md-nav__link">
      无监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH14/" title="聚类方法" class="md-nav__link">
      聚类方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH22/" title="无监督学习方法总结" class="md-nav__link">
      无监督学习方法总结
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Sklearn与TensorFlow
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Sklearn与TensorFlow
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/README.old/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/0.前言/" title="前言" class="md-nav__link">
      前言
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/1.机器学习概览/" title="机器学习概览" class="md-nav__link">
      机器学习概览
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/2.一个完整的机器学习项目/" title="一个完整的机器学习项目" class="md-nav__link">
      一个完整的机器学习项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/3.分类/" title="分类" class="md-nav__link">
      分类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/4.训练模型/" title="训练模型" class="md-nav__link">
      训练模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/5.支持向量机/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/6.决策树/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/7.集成学习和随机森林/" title="集成学习和随机森林" class="md-nav__link">
      集成学习和随机森林
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/9.启动并运行_TensorFlow/" title="启动并运行_TensorFlow" class="md-nav__link">
      启动并运行_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/10.人工神经网络介绍/" title="人工神经网络介绍" class="md-nav__link">
      人工神经网络介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/11.训练深层神经网络/" title="训练深层神经网络" class="md-nav__link">
      训练深层神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/12.设备和服务器上的分布式_TensorFlow/" title="设备和服务器上的分布式_TensorFlow" class="md-nav__link">
      设备和服务器上的分布式_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/13.卷积神经网络/" title="卷积神经网络" class="md-nav__link">
      卷积神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/14.循环神经网络/" title="循环神经网络" class="md-nav__link">
      循环神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/15.自编码器/" title="自编码器" class="md-nav__link">
      自编码器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/16.强化学习/" title="强化学习" class="md-nav__link">
      强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/B.机器学习项目清单/" title="机器学习项目清单" class="md-nav__link">
      机器学习项目清单
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/C.SVM_对偶问题/" title="SVM_对偶问题" class="md-nav__link">
      SVM_对偶问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/D.自动微分/" title="自动微分" class="md-nav__link">
      自动微分
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../about/" title="关于" class="md-nav__link">
      关于
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#31" title="3.1 基本概念" class="md-nav__link">
    3.1 基本概念
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#311" title="3.1.1 神经网络组成？" class="md-nav__link">
    3.1.1 神经网络组成？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#312" title="3.1.2 神经网络有哪些常用模型结构？" class="md-nav__link">
    3.1.2 神经网络有哪些常用模型结构？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#313" title="3.1.3 如何选择深度学习开发平台？" class="md-nav__link">
    3.1.3 如何选择深度学习开发平台？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#314" title="3.1.4 为什么使用深层表示?" class="md-nav__link">
    3.1.4 为什么使用深层表示?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#315" title="3.1.5 为什么深层神经网络难以训练？" class="md-nav__link">
    3.1.5 为什么深层神经网络难以训练？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#316" title="3.1.6 深度学习和机器学习有什么不同？" class="md-nav__link">
    3.1.6 深度学习和机器学习有什么不同？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32" title="3.2 网络操作与计算" class="md-nav__link">
    3.2 网络操作与计算
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321" title="3.2.1 前向传播与反向传播？" class="md-nav__link">
    3.2.1 前向传播与反向传播？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322" title="3.2.2 如何计算神经网络的输出？" class="md-nav__link">
    3.2.2 如何计算神经网络的输出？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323" title="3.2.3 如何计算卷积神经网络输出值？" class="md-nav__link">
    3.2.3 如何计算卷积神经网络输出值？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#324-pooling" title="3.2.4 如何计算 Pooling 层输出值输出值？" class="md-nav__link">
    3.2.4 如何计算 Pooling 层输出值输出值？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#325" title="3.2.5 实例理解反向传播" class="md-nav__link">
    3.2.5 实例理解反向传播
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#326" title="3.2.6 神经网络更“深”有什么意义？" class="md-nav__link">
    3.2.6 神经网络更“深”有什么意义？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33" title="3.3 超参数" class="md-nav__link">
    3.3 超参数
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#331" title="3.3.1 什么是超参数？" class="md-nav__link">
    3.3.1 什么是超参数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#332" title="3.3.2 如何寻找超参数的最优值？" class="md-nav__link">
    3.3.2 如何寻找超参数的最优值？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#333" title="3.3.3 超参数搜索一般过程？" class="md-nav__link">
    3.3.3 超参数搜索一般过程？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#34" title="3.4 激活函数" class="md-nav__link">
    3.4 激活函数
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#341" title="3.4.1 为什么需要非线性激活函数？" class="md-nav__link">
    3.4.1 为什么需要非线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#342" title="3.4.2 常见的激活函数及图像" class="md-nav__link">
    3.4.2 常见的激活函数及图像
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#343" title="3.4.3 常见激活函数的导数计算？" class="md-nav__link">
    3.4.3 常见激活函数的导数计算？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#344" title="3.4.4 激活函数有哪些性质？" class="md-nav__link">
    3.4.4 激活函数有哪些性质？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#345" title="3.4.5 如何选择激活函数？" class="md-nav__link">
    3.4.5 如何选择激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#346-relu" title="3.4.6 使用 ReLu 激活函数的优点？" class="md-nav__link">
    3.4.6 使用 ReLu 激活函数的优点？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#347" title="3.4.7 什么时候可以用线性激活函数？" class="md-nav__link">
    3.4.7 什么时候可以用线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#348-relu-0" title="3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？" class="md-nav__link">
    3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#349-softmax" title="3.4.9 Softmax 定义及作用" class="md-nav__link">
    3.4.9 Softmax 定义及作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3410-softmax" title="3.4.10 Softmax 函数如何应用于多分类？" class="md-nav__link">
    3.4.10 Softmax 函数如何应用于多分类？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3411" title="3.4.11 交叉熵代价函数定义及其求导推导" class="md-nav__link">
    3.4.11 交叉熵代价函数定义及其求导推导
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3412-tanhsigmoid" title="3.4.12 为什么Tanh收敛速度比Sigmoid快？" class="md-nav__link">
    3.4.12 为什么Tanh收敛速度比Sigmoid快？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#35-batch_size" title="3.5 Batch_Size" class="md-nav__link">
    3.5 Batch_Size
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#351-batch_size" title="3.5.1 为什么需要 Batch_Size？" class="md-nav__link">
    3.5.1 为什么需要 Batch_Size？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#352-batch_size" title="3.5.2 Batch_Size 值的选择" class="md-nav__link">
    3.5.2 Batch_Size 值的选择
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#353-batch_size" title="3.5.3 在合理范围内，增大Batch_Size有何好处？" class="md-nav__link">
    3.5.3 在合理范围内，增大Batch_Size有何好处？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#354-batch_size" title="3.5.4 盲目增大 Batch_Size 有何坏处？" class="md-nav__link">
    3.5.4 盲目增大 Batch_Size 有何坏处？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#355-batch_size" title="3.5.5 调节 Batch_Size 对训练效果影响到底如何？" class="md-nav__link">
    3.5.5 调节 Batch_Size 对训练效果影响到底如何？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#36" title="3.6 归一化" class="md-nav__link">
    3.6 归一化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#361" title="3.6.1 归一化含义？" class="md-nav__link">
    3.6.1 归一化含义？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#362" title="3.6.2 为什么要归一化？" class="md-nav__link">
    3.6.2 为什么要归一化？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#363" title="3.6.3 为什么归一化能提高求解最优解速度？" class="md-nav__link">
    3.6.3 为什么归一化能提高求解最优解速度？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#364-3d" title="3.6.4 3D 图解未归一化" class="md-nav__link">
    3.6.4 3D 图解未归一化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#365" title="3.6.5 归一化有哪些类型？" class="md-nav__link">
    3.6.5 归一化有哪些类型？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#366" title="3.6.6 局部响应归一化作用" class="md-nav__link">
    3.6.6 局部响应归一化作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#367" title="3.6.7 理解局部响应归一化" class="md-nav__link">
    3.6.7 理解局部响应归一化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#368-batch-normalization" title="3.6.8 什么是批归一化（Batch Normalization）" class="md-nav__link">
    3.6.8 什么是批归一化（Batch Normalization）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#369-bn" title="3.6.9 批归一化（BN）算法的优点" class="md-nav__link">
    3.6.9 批归一化（BN）算法的优点
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3610-bn" title="3.6.10 批归一化（BN）算法流程" class="md-nav__link">
    3.6.10 批归一化（BN）算法流程
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3611" title="3.6.11 批归一化和群组归一化比较" class="md-nav__link">
    3.6.11 批归一化和群组归一化比较
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3612-weight-normalizationbatch-normalization" title="3.6.12 Weight Normalization和Batch Normalization比较" class="md-nav__link">
    3.6.12 Weight Normalization和Batch Normalization比较
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3613-batch-normalization" title="3.6.13 Batch Normalization在什么时候用比较合适？" class="md-nav__link">
    3.6.13 Batch Normalization在什么时候用比较合适？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#37-fine-tuning" title="3.7 预训练与微调(fine tuning)" class="md-nav__link">
    3.7 预训练与微调(fine tuning)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#371" title="3.7.1 为什么无监督预训练可以帮助深度学习？" class="md-nav__link">
    3.7.1 为什么无监督预训练可以帮助深度学习？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#372-fine-tuning" title="3.7.2 什么是模型微调fine tuning" class="md-nav__link">
    3.7.2 什么是模型微调fine tuning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#373" title="3.7.3 微调时候网络参数是否更新？" class="md-nav__link">
    3.7.3 微调时候网络参数是否更新？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#374-fine-tuning" title="3.7.4 fine-tuning 模型的三种状态" class="md-nav__link">
    3.7.4 fine-tuning 模型的三种状态
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#38" title="3.8 权重偏差初始化" class="md-nav__link">
    3.8 权重偏差初始化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#381-0" title="3.8.1 全都初始化为 0" class="md-nav__link">
    3.8.1 全都初始化为 0
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#382" title="3.8.2 全都初始化为同样的值" class="md-nav__link">
    3.8.2 全都初始化为同样的值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#383" title="3.8.3 初始化为小的随机数" class="md-nav__link">
    3.8.3 初始化为小的随机数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#384-1sqrt-n" title="3.8.4 用 $ 1/\sqrt n $ 校准方差" class="md-nav__link">
    3.8.4 用 $ 1/\sqrt n $ 校准方差
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#385-sparse-initialazation" title="3.8.5 稀疏初始化(Sparse Initialazation)" class="md-nav__link">
    3.8.5 稀疏初始化(Sparse Initialazation)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#386" title="3.8.6 初始化偏差" class="md-nav__link">
    3.8.6 初始化偏差
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#39" title="3.9 学习率" class="md-nav__link">
    3.9 学习率
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#391" title="3.9.1 学习率的作用" class="md-nav__link">
    3.9.1 学习率的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#392" title="3.9.2 学习率衰减常用参数有哪些" class="md-nav__link">
    3.9.2 学习率衰减常用参数有哪些
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#393" title="3.9.3 分段常数衰减" class="md-nav__link">
    3.9.3 分段常数衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#394" title="3.9.4 指数衰减" class="md-nav__link">
    3.9.4 指数衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#395" title="3.9.5 自然指数衰减" class="md-nav__link">
    3.9.5 自然指数衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#396" title="3.9.6 多项式衰减" class="md-nav__link">
    3.9.6 多项式衰减
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#397" title="3.9.7 余弦衰减" class="md-nav__link">
    3.9.7 余弦衰减
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#312-dropout" title="3.12 Dropout 系列问题" class="md-nav__link">
    3.12 Dropout 系列问题
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#3121" title="3.12.1 为什么要正则化？" class="md-nav__link">
    3.12.1 为什么要正则化？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3122" title="3.12.2 为什么正则化有利于预防过拟合？" class="md-nav__link">
    3.12.2 为什么正则化有利于预防过拟合？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3123-dropout" title="3.12.3 理解dropout正则化" class="md-nav__link">
    3.12.3 理解dropout正则化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3124-dropout" title="3.12.4 dropout率的选择" class="md-nav__link">
    3.12.4 dropout率的选择
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3125-dropout" title="3.12.5 dropout有什么缺点？" class="md-nav__link">
    3.12.5 dropout有什么缺点？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#313_1" title="3.13 深度学习中常用的数据增强方法？" class="md-nav__link">
    3.13 深度学习中常用的数据增强方法？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#314-internal-covariate-shift" title="3.14 如何理解 Internal Covariate Shift？" class="md-nav__link">
    3.14 如何理解 Internal Covariate Shift？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="参考文献" class="md-nav__link">
    参考文献
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/myourdream/aiwiki/blob/master/docs/qa500/ch03_深度学习基础/第三章_深度学习基础.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <div class="toc">
<ul>
<li><a href="#_1">第三章 深度学习基础</a><ul>
<li><a href="#31">3.1 基本概念</a><ul>
<li><a href="#311">3.1.1 神经网络组成？</a></li>
<li><a href="#312">3.1.2 神经网络有哪些常用模型结构？</a></li>
<li><a href="#313">3.1.3 如何选择深度学习开发平台？</a></li>
<li><a href="#314">3.1.4 为什么使用深层表示?</a></li>
<li><a href="#315">3.1.5 为什么深层神经网络难以训练？</a></li>
<li><a href="#316">3.1.6 深度学习和机器学习有什么不同？</a></li>
</ul>
</li>
<li><a href="#32">3.2 网络操作与计算</a><ul>
<li><a href="#321">3.2.1 前向传播与反向传播？</a></li>
<li><a href="#322">3.2.2 如何计算神经网络的输出？</a></li>
<li><a href="#323">3.2.3 如何计算卷积神经网络输出值？</a></li>
<li><a href="#324-pooling">3.2.4 如何计算 Pooling 层输出值输出值？</a></li>
<li><a href="#325">3.2.5 实例理解反向传播</a></li>
<li><a href="#326">3.2.6 神经网络更“深”有什么意义？</a></li>
</ul>
</li>
<li><a href="#33">3.3 超参数</a><ul>
<li><a href="#331">3.3.1 什么是超参数？</a></li>
<li><a href="#332">3.3.2 如何寻找超参数的最优值？</a></li>
<li><a href="#333">3.3.3 超参数搜索一般过程？</a></li>
</ul>
</li>
<li><a href="#34">3.4 激活函数</a><ul>
<li><a href="#341">3.4.1 为什么需要非线性激活函数？</a></li>
<li><a href="#342">3.4.2 常见的激活函数及图像</a></li>
<li><a href="#343">3.4.3 常见激活函数的导数计算？</a></li>
<li><a href="#344">3.4.4 激活函数有哪些性质？</a></li>
<li><a href="#345">3.4.5 如何选择激活函数？</a></li>
<li><a href="#346-relu">3.4.6 使用 ReLu 激活函数的优点？</a></li>
<li><a href="#347">3.4.7 什么时候可以用线性激活函数？</a></li>
<li><a href="#348-relu-0">3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？</a></li>
<li><a href="#349-softmax">3.4.9 Softmax 定义及作用</a></li>
<li><a href="#3410-softmax">3.4.10 Softmax 函数如何应用于多分类？</a></li>
<li><a href="#3411">3.4.11 交叉熵代价函数定义及其求导推导</a></li>
<li><a href="#3412-tanhsigmoid">3.4.12 为什么Tanh收敛速度比Sigmoid快？</a></li>
</ul>
</li>
<li><a href="#35-batch_size">3.5 Batch_Size</a><ul>
<li><a href="#351-batch_size">3.5.1 为什么需要 Batch_Size？</a></li>
<li><a href="#352-batch_size">3.5.2 Batch_Size 值的选择</a></li>
<li><a href="#353-batch_size">3.5.3 在合理范围内，增大Batch_Size有何好处？</a></li>
<li><a href="#354-batch_size">3.5.4 盲目增大 Batch_Size 有何坏处？</a></li>
<li><a href="#355-batch_size">3.5.5 调节 Batch_Size 对训练效果影响到底如何？</a></li>
</ul>
</li>
<li><a href="#36">3.6 归一化</a><ul>
<li><a href="#361">3.6.1 归一化含义？</a></li>
<li><a href="#362">3.6.2 为什么要归一化？</a></li>
<li><a href="#363">3.6.3 为什么归一化能提高求解最优解速度？</a></li>
<li><a href="#364-3d">3.6.4 3D 图解未归一化</a></li>
<li><a href="#365">3.6.5 归一化有哪些类型？</a></li>
<li><a href="#366">3.6.6 局部响应归一化作用</a></li>
<li><a href="#367">3.6.7 理解局部响应归一化</a></li>
<li><a href="#368-batch-normalization">3.6.8 什么是批归一化（Batch Normalization）</a></li>
<li><a href="#369-bn">3.6.9 批归一化（BN）算法的优点</a></li>
<li><a href="#3610-bn">3.6.10 批归一化（BN）算法流程</a></li>
<li><a href="#3611">3.6.11 批归一化和群组归一化比较</a></li>
<li><a href="#3612-weight-normalizationbatch-normalization">3.6.12 Weight Normalization和Batch Normalization比较</a></li>
<li><a href="#3613-batch-normalization">3.6.13 Batch Normalization在什么时候用比较合适？</a></li>
</ul>
</li>
<li><a href="#37-fine-tuning">3.7 预训练与微调(fine tuning)</a><ul>
<li><a href="#371">3.7.1 为什么无监督预训练可以帮助深度学习？</a></li>
<li><a href="#372-fine-tuning">3.7.2 什么是模型微调fine tuning</a></li>
<li><a href="#373">3.7.3 微调时候网络参数是否更新？</a></li>
<li><a href="#374-fine-tuning">3.7.4 fine-tuning 模型的三种状态</a></li>
</ul>
</li>
<li><a href="#38">3.8 权重偏差初始化</a><ul>
<li><a href="#381-0">3.8.1 全都初始化为 0</a></li>
<li><a href="#382">3.8.2 全都初始化为同样的值</a></li>
<li><a href="#383">3.8.3 初始化为小的随机数</a></li>
<li><a href="#384-1sqrt-n">3.8.4 用 $ 1/\sqrt n $ 校准方差</a></li>
<li><a href="#385-sparse-initialazation">3.8.5 稀疏初始化(Sparse Initialazation)</a></li>
<li><a href="#386">3.8.6 初始化偏差</a></li>
</ul>
</li>
<li><a href="#39">3.9 学习率</a><ul>
<li><a href="#391">3.9.1 学习率的作用</a></li>
<li><a href="#392">3.9.2 学习率衰减常用参数有哪些</a></li>
<li><a href="#393">3.9.3 分段常数衰减</a></li>
<li><a href="#394">3.9.4 指数衰减</a></li>
<li><a href="#395">3.9.5 自然指数衰减</a></li>
<li><a href="#396">3.9.6 多项式衰减</a></li>
<li><a href="#397">3.9.7 余弦衰减</a></li>
</ul>
</li>
<li><a href="#312-dropout">3.12 Dropout 系列问题</a><ul>
<li><a href="#3121">3.12.1 为什么要正则化？</a></li>
<li><a href="#3122">3.12.2 为什么正则化有利于预防过拟合？</a></li>
<li><a href="#3123-dropout">3.12.3 理解dropout正则化</a></li>
<li><a href="#3124-dropout">3.12.4 dropout率的选择</a></li>
<li><a href="#3125-dropout">3.12.5 dropout有什么缺点？</a></li>
</ul>
</li>
<li><a href="#313_1">3.13 深度学习中常用的数据增强方法？</a></li>
<li><a href="#314-internal-covariate-shift">3.14 如何理解 Internal Covariate Shift？</a></li>
<li><a href="#_2">参考文献</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">第三章 深度学习基础<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="31">3.1 基本概念<a class="headerlink" href="#31" title="Permanent link">&para;</a></h2>
<h3 id="311">3.1.1 神经网络组成？<a class="headerlink" href="#311" title="Permanent link">&para;</a></h3>
<p>神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。</p>
<p><strong>感知机</strong></p>
<p>多层感知机中的特征神经元模型称为感知机，由<em>Frank Rosenblatt</em>于1957年发明。</p>
<p>简单的感知机如下图所示：</p>
<p><img alt="" src="../img/ch3/3-1.png" /></p>
<p>其中<span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span>，<span><span class="MathJax_Preview">x_2</span><script type="math/tex">x_2</script></span>，<span><span class="MathJax_Preview">x_3</span><script type="math/tex">x_3</script></span>为感知机的输入，其输出为：</p>
<div>
<div class="MathJax_Preview">
output = \left\{
\begin{aligned}
0, \quad if \ \ \sum_i w_i x_i \leqslant threshold \\
1, \quad if \ \ \sum_i w_i x_i &gt; threshold
\end{aligned}
\right.
</div>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \sum_i w_i x_i \leqslant threshold \\
1, \quad if \ \ \sum_i w_i x_i > threshold
\end{aligned}
\right.
</script>
</div>
<p>假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为$ 4 <span><span class="MathJax_Preview">分、</span><script type="math/tex">分、</script></span>1$ 分、<span><span class="MathJax_Preview">-3 <span><span class="MathJax_Preview">分，这</span><script type="math/tex">分，这</script></span> 3 3</span><script type="math/tex">-3 <span><span class="MathJax_Preview">分，这</span><script type="math/tex">分，这</script></span> 3 3</script></span> 位评分的权重分别是 <span><span class="MathJax_Preview">1、3、2</span><script type="math/tex">1、3、2</script></span>，则该歌手最终得分为 <span><span class="MathJax_Preview">4 \times 1 + 1 \times 3 + (-3) \times 2 = 1</span><script type="math/tex">4 \times 1 + 1 \times 3 + (-3) \times 2 = 1</script></span> 。按照比赛规则，选取的 <span><span class="MathJax_Preview">threshold</span><script type="math/tex">threshold</script></span> 为 <span><span class="MathJax_Preview">3</span><script type="math/tex">3</script></span>，说明只有歌手的综合评分大于$ 3$ 时，才可顺利晋级。对照感知机，该选手被淘汰，因为：</p>
<div>
<div class="MathJax_Preview">
\sum_i w_i x_i &lt; threshold=3, output = 0
</div>
<script type="math/tex; mode=display">
\sum_i w_i x_i < threshold=3, output = 0
</script>
</div>
<p>用 <span><span class="MathJax_Preview">-b</span><script type="math/tex">-b</script></span>  代替 <span><span class="MathJax_Preview">threshold</span><script type="math/tex">threshold</script></span>，输出变为：</p>
<div>
<div class="MathJax_Preview">
output = \left\{
\begin{aligned}
0, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b \leqslant 0 \\
1, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b &gt; 0
\end{aligned}
\right.
</div>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b \leqslant 0 \\
1, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b > 0
\end{aligned}
\right.
</script>
</div>
<p>设置合适的  <span><span class="MathJax_Preview">\boldsymbol{x}</span><script type="math/tex">\boldsymbol{x}</script></span>  和  <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> ，一个简单的感知机单元的与非门表示如下：</p>
<p><img alt="" src="../img/ch3/3-2.png" /></p>
<p>当输入为 <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>，<span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> 时，感知机输出为 $ 0 \times (-2) + 1 \times (-2) + 3 = 1$。</p>
<p>复杂一些的感知机由简单的感知机单元组合而成：</p>
<p><img alt="" src="../img/ch3/3-3.png" /></p>
<p><strong>多层感知机</strong></p>
<p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第 $ i $ 层的每个神经元和第 $ i-1 $ 层的每个神经元都有连接。</p>
<p><img alt="" src="../img/ch3/3.1.1.5.png" /></p>
<p>输出层可以不止有$ 1$ 个神经元。隐藏层可以只有$ 1$ 层，也可以有多层。输出层为多个神经元的神经网络例如下图所示：</p>
<p><img alt="" src="../img/ch3/3.1.1.6.png" /></p>
<h3 id="312">3.1.2 神经网络有哪些常用模型结构？<a class="headerlink" href="#312" title="Permanent link">&para;</a></h3>
<p>下图包含了大部分常用的模型：</p>
<p><img alt="" src="../img/ch3/3-7.jpg" /></p>
<h3 id="313">3.1.3 如何选择深度学习开发平台？<a class="headerlink" href="#313" title="Permanent link">&para;</a></h3>
<p>​   现有的深度学习开源平台主要有 Caffe, PyTorch, MXNet, CNTK, Theano, TensorFlow, Keras, fastai等。那如何选择一个适合自己的平台呢，下面列出一些衡量做参考。</p>
<p><strong>参考1：与现有编程平台、技能整合的难易程度</strong></p>
<p>​   主要是前期积累的开发经验和资源，比如编程语言，前期数据集存储格式等。</p>
<p><strong>参考2: 与相关机器学习、数据处理生态整合的紧密程度</strong></p>
<p>​   深度学习研究离不开各种数据处理、可视化、统计推断等软件包。考虑建模之前，是否具有方便的数据预处理工具？建模之后，是否具有方便的工具进行可视化、统计推断、数据分析。  </p>
<p><strong>参考3：对数据量及硬件的要求和支持</strong></p>
<p>​   深度学习在不同应用场景的数据量是不一样的，这也就导致我们可能需要考虑分布式计算、多GPU计算的问题。例如，对计算机图像处理研究的人员往往需要将图像文件和计算任务分部到多台计算机节点上进行执行。当下每个深度学习平台都在快速发展，每个平台对分布式计算等场景的支持也在不断演进。</p>
<p><strong>参考4：深度学习平台的成熟程度</strong></p>
<p>​   成熟程度的考量是一个比较主观的考量因素，这些因素可包括：社区的活跃程度；是否容易和开发人员进行交流；当前应用的势头。</p>
<p><strong>参考5：平台利用是否多样性？</strong></p>
<p>​   有些平台是专门为深度学习研究和应用进行开发的，有些平台对分布式计算、GPU 等构架都有强大的优化，能否用这些平台/软件做其他事情？比如有些深度学习软件是可以用来求解二次型优化；有些深度学习平台很容易被扩展，被运用在强化学习的应用中。</p>
<h3 id="314">3.1.4 为什么使用深层表示?<a class="headerlink" href="#314" title="Permanent link">&para;</a></h3>
<ol>
<li>深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。</li>
<li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</li>
</ol>
<h3 id="315">3.1.5 为什么深层神经网络难以训练？<a class="headerlink" href="#315" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>梯度消失
        梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。</p>
<p>​   梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：</p>
</li>
</ol>
<p><img alt="" src="../img/ch3/3-8.png" /></p>
<ol>
<li>
<p>梯度爆炸
        在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为<span><span class="MathJax_Preview">NaN</span><script type="math/tex">NaN</script></span>值，再也无法更新。</p>
</li>
<li>
<p>权重矩阵的退化导致模型的有效自由度减少。</p>
<p>​   参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人2014年的论文里展示了关于该退化过程的可视化：</p>
</li>
</ol>
<p><img alt="" src="../img/ch3/3-9.jpg" /></p>
<p>随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。</p>
<h3 id="316">3.1.6 深度学习和机器学习有什么不同？<a class="headerlink" href="#316" title="Permanent link">&para;</a></h3>
<p>​   <strong>机器学习</strong>：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。</p>
<p>​   <strong>深度学习</strong>：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。</p>
<p>​   传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。</p>
<p><img alt="" src="../img/ch3/3.1.6.1.png" /></p>
<p><img alt="" src="../img/ch3/3-11.jpg" /></p>
<h2 id="32">3.2 网络操作与计算<a class="headerlink" href="#32" title="Permanent link">&para;</a></h2>
<h3 id="321">3.2.1 前向传播与反向传播？<a class="headerlink" href="#321" title="Permanent link">&para;</a></h3>
<p>神经网络的计算主要有两种：前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果；反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度由深到浅更新网络参数。</p>
<p><strong>前向传播</strong></p>
<p><img alt="" src="../img/ch3/3.2.1.1.png" /></p>
<p>假设上一层结点 $ i,j,k,... ​$ 等一些结点与本层的结点 $ w ​$ 有连接，那么结点 $ w ​$ 的值怎么算呢？就是通过上一层的 $ i,j,k,... ​$ 等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如 <span><span class="MathJax_Preview">ReLu​</span><script type="math/tex">ReLu​</script></span>，<span><span class="MathJax_Preview">sigmoid​</span><script type="math/tex">sigmoid​</script></span> 等函数，最后得到的结果就是本层结点 $ w ​$ 的输出。 </p>
<p>最终不断的通过这种方法一层层的运算，得到输出层结果。</p>
<p><strong>反向传播</strong></p>
<p><img alt="" src="../img/ch3/3.2.1.2.png" /></p>
<p>由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下：</p>
<p>设最终误差为 $ E $且输出层的激活函数为线性激活函数，对于输出那么 $ E $ 对于输出节点 $ y_l $ 的偏导数是 $ y_l - t_l $，其中 $ t_l $ 是真实值，$ \frac{\partial y_l}{\partial z_l} $ 是指上面提到的激活函数，$ z_l $ 是上面提到的加权和，那么这一层的 $ E $ 对于 $ z_l $ 的偏导数为 $ \frac{\partial E}{\partial z_l} = \frac{\partial E}{\partial y_l} \frac{\partial y_l}{\partial z_l} $。同理，下一层也是这么计算，只不过 $ \frac{\partial E}{\partial y_k} $ 计算方法变了，一直反向传播到输入层，最后有 $ \frac{\partial E}{\partial x_i} = \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j} $，且 $ \frac{\partial z_j}{\partial x_i} = w_i j ​$。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。</p>
<h3 id="322">3.2.2 如何计算神经网络的输出？<a class="headerlink" href="#322" title="Permanent link">&para;</a></h3>
<p><img alt="" src="../img/ch3/3.2.2.1.png" /></p>
<p>如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $ w_{41}, w_{42}, w_{43} ​$。</p>
<p>为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $ x_1, x_2, x_3 $。</p>
<div>
<div class="MathJax_Preview">
a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})
</div>
<script type="math/tex; mode=display">
a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})
</script>
</div>
<p>其中 $ w_{4b} $ 是节点 4 的偏置项。</p>
<p>同样，我们可以继续计算出节点 5、6、7 的输出值 $ a_5, a_6, a_7 $。</p>
<p>计算输出层的节点 8 的输出值 $ y_1 ​$：</p>
<div>
<div class="MathJax_Preview">
y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})
</div>
<script type="math/tex; mode=display">
y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})
</script>
</div>
<p>其中 $ w_{8b} ​$ 是节点 8 的偏置项。</p>
<p>同理，我们还可以计算出 $ y_2 $。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 $ x_1, x_2, x_3, x_4 $ 时，神经网络的输出向量 $ y_1, y_2 $ 。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p>
<h3 id="323">3.2.3 如何计算卷积神经网络输出值？<a class="headerlink" href="#323" title="Permanent link">&para;</a></h3>
<p>假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示：</p>
<p><img alt="" src="../img/ch3/3.2.3.1.png" /></p>
<p>$ x_{i,j} $ 表示图像第  $ i $ 行第 $ j $ 列元素。$ w_{m,n} $ 表示 filter​ 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 <span><span class="MathJax_Preview">filter</span><script type="math/tex">filter</script></span> 的偏置项。 表<span><span class="MathJax_Preview">a_i,_j</span><script type="math/tex">a_i,_j</script></span>示 feature map 第 $ i$ 行第 $ j $ 列元素。 <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> 表示激活函数，这里以$ ReLU$ 函数为例。</p>
<p>卷积计算公式如下：</p>
<div>
<div class="MathJax_Preview">
a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )
</div>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )
</script>
</div>
<p>当步长为 <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> 时，计算 feature map 元素 $ a_{0,0} ​$ 如下：</p>
<p>$$
a_{0,0} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )</p>
<p>= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + \w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \</p>
<p>= 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \</p>
<p>= 4
$$</p>
<p>其计算过程图示如下：</p>
<p><img alt="" src="../img/ch3/3.2.3.2.png" /></p>
<p>以此类推，计算出全部的Feature Map。</p>
<p><img alt="" src="../img/ch3/3.2.3.4.png" /></p>
<p>当步幅为 2 时，Feature Map计算如下</p>
<p><img alt="" src="../img/ch3/3.2.3.5.png" /></p>
<p>注：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：</p>
<div>
<div class="MathJax_Preview">
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1
</div>
<script type="math/tex; mode=display">
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1
</script>
</div>
<p>​   其中 $ W_2 <span><span class="MathJax_Preview">， 是卷积后 Feature Map 的宽度；</span><script type="math/tex">， 是卷积后 Feature Map 的宽度；</script></span> W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>，如果 <span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> 的值是 <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>，那么就补 <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> 圈 <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>；<span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。</p>
<p>​   举例：假设图像宽度 $ W_1 = 5 $，filter 宽度 $ F=3 $，Zero Padding $ P=0 $，步幅 $ S=2 <span><span class="MathJax_Preview">，</span><script type="math/tex">，</script></span> Z $ 则</p>
<p>$$
W_2 = (W_1 - F + 2P)/S + 1</p>
<p>= (5-3+0)/2 + 1</p>
<p>= 2
$$</p>
<p>​   说明 Feature Map 宽度是2。同样，我们也可以计算出 Feature Map 高度也是 2。</p>
<p>如果卷积前的图像深度为 $ D ​$，那么相应的 filter 的深度也必须为 $ D ​$。深度大于 1 的卷积计算公式：</p>
<div>
<div class="MathJax_Preview">
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)
</div>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)
</script>
</div>
<p>​   其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w_{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a_{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素；其它的符号含义前面相同，不再赘述。</p>
<p>​   每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。<span><span class="MathJax_Preview">7*7*3</span><script type="math/tex">7*7*3</script></span> 输入，经过两个 <span><span class="MathJax_Preview">3*3*3</span><script type="math/tex">3*3*3</script></span> filter 的卷积(步幅为 <span><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span>)，得到了 <span><span class="MathJax_Preview">3*3*2</span><script type="math/tex">3*3*2</script></span> 的输出。图中的 Zero padding 是 <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>，也就是在输入元素的周围补了一圈 <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>。</p>
<p><img alt="" src="../img/ch3/3.2.3.6.png" /></p>
<p>​   以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 * 3 * 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 * 3 * 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<h3 id="324-pooling">3.2.4 如何计算 Pooling 层输出值输出值？<a class="headerlink" href="#324-pooling" title="Permanent link">&para;</a></h3>
<p>​   Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling：</p>
<p><img alt="" src="../img/ch3/3.2.4.1.png" /></p>
<p>​   除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。
​   对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。</p>
<h3 id="325">3.2.5 实例理解反向传播<a class="headerlink" href="#325" title="Permanent link">&para;</a></h3>
<p>​   一个典型的三层神经网络如下所示：</p>
<p><img alt="" src="../img/ch3/3.2.5.1.png" /></p>
<p>​   其中 Layer $ L_1 $ 是输入层，Layer $ L_2 $ 是隐含层，Layer $ L_3 $ 是输出层。</p>
<p>​   假设输入数据集为 $ D={x_1, x_2, ..., x_n} $，输出数据集为 $ y_1, y_2, ..., y_n $。</p>
<p>​   如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。</p>
<p>假设有如下的网络层：</p>
<p><img alt="" src="../img/ch3/3.2.5.2.png" /></p>
<p>​   输入层包含神经元 $ i_1, i_2 $，偏置 $ b_1 $；隐含层包含神经元 $ h_1, h_2 $，偏置 $ b_2 $，输出层为  $ o_1, o_2 <span><span class="MathJax_Preview">，</span><script type="math/tex">，</script></span> w_i $ 为层与层之间连接的权重，激活函数为 <span><span class="MathJax_Preview">sigmoid</span><script type="math/tex">sigmoid</script></span> 函数。对以上参数取初始值，如下图所示：</p>
<p><img alt="" src="../img/ch3/3.2.5.3.png" /></p>
<p>其中：</p>
<ul>
<li>输入数据 $ i1=0.05, i2 = 0.10 $</li>
<li>输出数据 $ o1=0.01, o2=0.99 $;</li>
<li>初始权重 $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $</li>
<li>目标：给出输入数据 $ i1,i2 $ ( <span><span class="MathJax_Preview">0.05</span><script type="math/tex">0.05</script></span>和<span><span class="MathJax_Preview">0.10</span><script type="math/tex">0.10</script></span> )，使输出尽可能与原始输出 $ o1,o2 <span><span class="MathJax_Preview">，( <span><span class="MathJax_Preview">0.01</span><script type="math/tex">0.01</script></span>和和</span><script type="math/tex">，( <span><span class="MathJax_Preview">0.01</span><script type="math/tex">0.01</script></span>和和</script></span>0.99$)接近。</li>
</ul>
<p><strong>前向传播</strong></p>
<ol>
<li>输入层 &rarr; 输出层</li>
</ol>
<p>计算神经元 $ h1 ​$ 的输入加权和：</p>
<p>$$
net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1\</p>
<p>net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775
$$</p>
<p>神经元 $ h1 $ 的输出 $ o1 $ ：（此处用到激活函数为 sigmoid 函数）：</p>
<div>
<div class="MathJax_Preview">
out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992
</div>
<script type="math/tex; mode=display">
out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992
</script>
</div>
<p>同理，可计算出神经元 $ h2 $ 的输出 $ o1 $：</p>
<div>
<div class="MathJax_Preview">
out_{h2} = 0.596884378
</div>
<script type="math/tex; mode=display">
out_{h2} = 0.596884378
</script>
</div>
<ol>
<li>隐含层&rarr;输出层：  　　</li>
</ol>
<p>计算输出层神经元 $ o1 ​$ 和 $ o2 ​$ 的值：</p>
<div>
<div class="MathJax_Preview">
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1
</div>
<script type="math/tex; mode=display">
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1
</script>
</div>
<div>
<div class="MathJax_Preview">
net_{o1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967
</div>
<script type="math/tex; mode=display">
net_{o1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967
</script>
</div>
<div>
<div class="MathJax_Preview">
out_{o1} = \frac{1}{1 + e^{-net_{o1}}} = \frac{1}{1 + e^{1.105905967}} = 0.75136079
</div>
<script type="math/tex; mode=display">
out_{o1} = \frac{1}{1 + e^{-net_{o1}}} = \frac{1}{1 + e^{1.105905967}} = 0.75136079
</script>
</div>
<p>这样前向传播的过程就结束了，我们得到输出值为 $ [0.75136079 ,  0.772928465] $，与实际值 $ [0.01 , 0.99] ​$ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
<p><strong>反向传播 </strong></p>
<p>​   1.计算总误差</p>
<p>总误差：(这里使用Square Error)</p>
<div>
<div class="MathJax_Preview">
E_{total} = \sum \frac{1}{2}(target - output)^2
</div>
<script type="math/tex; mode=display">
E_{total} = \sum \frac{1}{2}(target - output)^2
</script>
</div>
<p>但是有两个输出，所以分别计算 $ o1 $ 和 $ o2 $ 的误差，总误差为两者之和：</p>
<p><span><span class="MathJax_Preview">E_{o1} = \frac{1}{2}(target_{o1} - out_{o1})^2 
= \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083</span><script type="math/tex">E_{o1} = \frac{1}{2}(target_{o1} - out_{o1})^2 
= \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083</script></span>.</p>
<p><span><span class="MathJax_Preview">E_{o2} = 0.023560026</span><script type="math/tex">E_{o2} = 0.023560026</script></span>.</p>
<p><span><span class="MathJax_Preview">E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109</span><script type="math/tex">E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109</script></span>.</p>
<p>​   2.隐含层 &rarr; 输出层的权值更新：</p>
<p>以权重参数 $ w5 ​$ 为例，如果我们想知道 $ w5 ​$ 对整体误差产生了多少影响，可以用整体误差对 $ w5 ​$ 求偏导求出：（链式法则）</p>
<div>
<div class="MathJax_Preview">
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}
</div>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}
</script>
</div>
<p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
<p><img alt="" src="../img/ch3/3.2.5.4.png" /></p>
<h3 id="326">3.2.6 神经网络更“深”有什么意义？<a class="headerlink" href="#326" title="Permanent link">&para;</a></h3>
<p>前提：在一定范围内。</p>
<ul>
<li>在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。</li>
<li>隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。</li>
</ul>
<h2 id="33">3.3 超参数<a class="headerlink" href="#33" title="Permanent link">&para;</a></h2>
<h3 id="331">3.3.1 什么是超参数？<a class="headerlink" href="#331" title="Permanent link">&para;</a></h3>
<p>​   <strong>超参数</strong> : 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<p>​   超参数通常存在于：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定
</pre></div>
</td></tr></table>

<p>​   超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p>
<h3 id="332">3.3.2 如何寻找超参数的最优值？<a class="headerlink" href="#332" title="Permanent link">&para;</a></h3>
<p>​   在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p>
<ol>
<li>
<p>猜测和检查：根据经验或直觉，选择参数，一直迭代。</p>
</li>
<li>
<p>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</p>
</li>
<li>
<p>随机搜索：让计算机随机挑选一组值。</p>
</li>
<li>
<p>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</p>
</li>
<li>
<p>MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</p>
</li>
<li>
<p>最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</p>
</li>
</ol>
<h3 id="333">3.3.3 超参数搜索一般过程？<a class="headerlink" href="#333" title="Permanent link">&para;</a></h3>
<p>超参数搜索一般过程：
1. 将数据集划分成训练集、验证集及测试集。
2. 在训练集上根据模型的性能指标对模型参数进行优化。
3. 在验证集上根据模型的性能指标对模型的超参数进行搜索。
4. 步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</p>
<p>其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。</p>
<h2 id="34">3.4 激活函数<a class="headerlink" href="#34" title="Permanent link">&para;</a></h2>
<h3 id="341">3.4.1 为什么需要非线性激活函数？<a class="headerlink" href="#341" title="Permanent link">&para;</a></h3>
<p><strong>为什么需要激活函数？</strong></p>
<ol>
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
</ol>
<p><strong>为什么激活函数需要非线性函数？</strong></p>
<ol>
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
<li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>
<h3 id="342">3.4.2 常见的激活函数及图像<a class="headerlink" href="#342" title="Permanent link">&para;</a></h3>
<ol>
<li>sigmoid 激活函数</li>
</ol>
<p>函数的定义为：$ f(x) = \frac{1}{1 + e^{-x}} $，其值域为 $ (0,1) $。</p>
<p>函数图像如下：</p>
<p><img alt="" src="../img/ch3/3-26.png" /></p>
<ol>
<li>tanh激活函数</li>
</ol>
<p>函数的定义为：$ f(x) = tanh(x) = \frac{e^x - e<sup>{-x}}{e</sup>x + e^{-x}} $，值域为 $ (-1,1) $。</p>
<p>函数图像如下：</p>
<p><img alt="" src="../img/ch3/3-27.png" /></p>
<ol>
<li>Relu激活函数</li>
</ol>
<p>函数的定义为：$ f(x) = max(0, x) $  ，值域为 $ [0,+∞) $；</p>
<p>函数图像如下：</p>
<p><img alt="" src="../img/ch3/3-28.png" /></p>
<ol>
<li>Leak Relu 激活函数 </li>
</ol>
<p>函数定义为： $ f(x) =  \left{
   \begin{aligned}
   ax, \quad x&lt;0 \
   x, \quad x&gt;0
   \end{aligned}
   \right. $，值域为 $ (-∞,+∞) $。 </p>
<p>图像如下（$ a = 0.5 $）：</p>
<p><img alt="" src="../img/ch3/3-29.png" /></p>
<ol>
<li>SoftPlus 激活函数</li>
</ol>
<p>函数的定义为：$ f(x) = ln( 1 + e^x) $，值域为 $ (0,+∞) $。</p>
<p>函数图像如下:</p>
<p><img alt="" src="../img/ch3/3-30.png" /></p>
<ol>
<li>softmax 函数</li>
</ol>
<p>函数定义为： $ \sigma(z)_j = \frac{e<sup>{z_j}}{\sum_{k=1}</sup>K e^{z_k}} $。</p>
<p>Softmax 多用于多分类神经网络输出。</p>
<h3 id="343">3.4.3 常见激活函数的导数计算？<a class="headerlink" href="#343" title="Permanent link">&para;</a></h3>
<p>对常见激活函数，导数计算如下：</p>
<table>
<thead>
<tr>
<th>原函数</th>
<th>函数表达式</th>
<th>导数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid激活函数</td>
<td><span><span class="MathJax_Preview">f(x)=\frac{1}{1+e^{-x}}</span><script type="math/tex">f(x)=\frac{1}{1+e^{-x}}</script></span></td>
<td><span><span class="MathJax_Preview">f^{'}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))</span><script type="math/tex">f^{'}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))</script></span></td>
<td>当<span><span class="MathJax_Preview">x=10</span><script type="math/tex">x=10</script></span>,或<span><span class="MathJax_Preview">x=-10​</span><script type="math/tex">x=-10​</script></span>，<span><span class="MathJax_Preview">f^{'}(x) \approx0​</span><script type="math/tex">f^{'}(x) \approx0​</script></span>,当<span><span class="MathJax_Preview">x=0​</span><script type="math/tex">x=0​</script></span><span><span class="MathJax_Preview">f^{'}(x) =0.25​</span><script type="math/tex">f^{'}(x) =0.25​</script></span></td>
</tr>
<tr>
<td>Tanh激活函数</td>
<td><span><span class="MathJax_Preview">f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</span><script type="math/tex">f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script></span></td>
<td><span><span class="MathJax_Preview">f^{'}(x)=-(tanh(x))^2</span><script type="math/tex">f^{'}(x)=-(tanh(x))^2</script></span></td>
<td>当<span><span class="MathJax_Preview">x=10</span><script type="math/tex">x=10</script></span>,或<span><span class="MathJax_Preview">x=-10</span><script type="math/tex">x=-10</script></span>，<span><span class="MathJax_Preview">f^{'}(x) \approx0</span><script type="math/tex">f^{'}(x) \approx0</script></span>,当<span><span class="MathJax_Preview">x=0</span><script type="math/tex">x=0</script></span><span><span class="MathJax_Preview">f^{`}(x) =1</span><script type="math/tex">f^{`}(x) =1</script></span></td>
</tr>
<tr>
<td>Relu激活函数</td>
<td><span><span class="MathJax_Preview">f(x)=max(0,x)</span><script type="math/tex">f(x)=max(0,x)</script></span></td>
<td><span><span class="MathJax_Preview">c(u)=\begin{cases} 0,x&lt;0 \\ 1,x&gt;0 \\ undefined,x=0\end{cases}</span><script type="math/tex">c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \\ undefined,x=0\end{cases}</script></span></td>
<td>通常<span><span class="MathJax_Preview">x=0</span><script type="math/tex">x=0</script></span>时，给定其导数为1和0</td>
</tr>
</tbody>
</table>
<h3 id="344">3.4.4 激活函数有哪些性质？<a class="headerlink" href="#344" title="Permanent link">&para;</a></h3>
<ol>
<li>非线性： 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x $，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性： 当优化方法是基于梯度的时候，就体现了该性质；</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</li>
<li>$ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。</li>
</ol>
<h3 id="345">3.4.5 如何选择激活函数？<a class="headerlink" href="#345" title="Permanent link">&para;</a></h3>
<p>​   选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>以下是常见的选择情况：</p>
<ol>
<li>如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li>
<li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li>
<li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li>
<li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li>
<li>如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</li>
</ol>
<h3 id="346-relu">3.4.6 使用 ReLu 激活函数的优点？<a class="headerlink" href="#346-relu" title="Permanent link">&para;</a></h3>
<ol>
<li>在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
<li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。</li>
<li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。</li>
</ol>
<h3 id="347">3.4.7 什么时候可以用线性激活函数？<a class="headerlink" href="#347" title="Permanent link">&para;</a></h3>
<ol>
<li>输出层，大多使用线性激活函数。</li>
<li>在隐含层可能会使用一些线性激活函数。</li>
<li>一般用到的线性激活函数很少。</li>
</ol>
<h3 id="348-relu-0">3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？<a class="headerlink" href="#348-relu-0" title="Permanent link">&para;</a></h3>
<p>Relu 激活函数图像如下：</p>
<p><img alt="" src="../img/ch3/3-32.png" /></p>
<p>根据图像可看出具有如下特点：</p>
<ol>
<li>
<p>单侧抑制；</p>
</li>
<li>
<p>相对宽阔的兴奋边界；</p>
</li>
<li>
<p>稀疏激活性；</p>
</li>
</ol>
<p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p>
<p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<p><strong>稀疏激活性</strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x&lt;0 $ 时，ReLU 硬饱和，而当 $ x&gt;0 $ 时，则不存在饱和问题。ReLU 能够在 $ x&gt;0 $ 时保持梯度不衰减，从而缓解梯度消失问题。</p>
<h3 id="349-softmax">3.4.9 Softmax 定义及作用<a class="headerlink" href="#349-softmax" title="Permanent link">&para;</a></h3>
<p>Softmax 是一种形如下式的函数：
$$
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}
$$
​   其中，$ \theta_i $ 和 $ x $ 是列向量，$ \theta_i^T x $ 可能被换成函数关于 $ x $ 的函数 $ f_i(x) $</p>
<p>​   通过 softmax 函数，可以使得 $ P(i) $ 的范围在 $ [0,1] $ 之间。在回归和分类问题中，通常 $ \theta $ 是待求参数，通过寻找使得 $ P(i) $ 最大的 $ \theta_i $ 作为最佳参数。</p>
<p>​   但是，使得范围在 $ [0,1] $  之间的方法有很多，为啥要在前面加上以 $ e $ 的幂函数的形式呢？参考 logistic 函数：
$$
P(i) = \frac{1}{1+exp(-\theta_i^T x)}
$$
​   这个函数的作用就是使得 $ P(i) $ 在负无穷到 0 的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了 $ e $ 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。这样为多类别提供了方便（可以把 $ P(i) $ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。</p>
<p>​   softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多酚类问题的最佳输出激活函数。</p>
<h3 id="3410-softmax">3.4.10 Softmax 函数如何应用于多分类？<a class="headerlink" href="#3410-softmax" title="Permanent link">&para;</a></h3>
<p>​   softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！</p>
<p>​   假设我们有一个数组，$ V_i $ 表示 $ V $  中的第 $ i $ 个元素，那么这个元素的 softmax 值就是</p>
<div>
<div class="MathJax_Preview">
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}
</div>
<script type="math/tex; mode=display">
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}
</script>
</div>
<p>​   从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $ y=0, y=1, y=2 $ 的概率值。</p>
<p><img alt="" src="../img/ch3/3.4.9.1.png" /></p>
<p>继续看下面的图，三个输入通过 softmax 后得到一个数组 $ [0.05 , 0.10 , 0.85] ​$，这就是 soft 的功能。</p>
<p><img alt="" src="../img/ch3/3.4.9.2.png" /></p>
<p>更形象的映射过程如下图所示：</p>
<p><img alt="****" src="../img/ch3/3.4.9.3.png" /></p>
<p>​   softmax 直白来说就是将原来输出是 $ 3,1,-3 ​$ 通过 softmax 函数一作用，就映射成为 $ (0,1) ​$ 的值，而这些值的累和为 $ 1 ​$（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p>
<h3 id="3411">3.4.11 交叉熵代价函数定义及其求导推导<a class="headerlink" href="#3411" title="Permanent link">&para;</a></h3>
<p>(<strong>贡献者：黄钦建－华南理工大学</strong>)</p>
<p>​   神经元的输出就是 a = σ(z)，其中<span><span class="MathJax_Preview">z=\sum w_{j}i_{j}+b​</span><script type="math/tex">z=\sum w_{j}i_{j}+b​</script></span>是输⼊的带权和。</p>
<p><span><span class="MathJax_Preview">C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]</span><script type="math/tex">C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]</script></span></p>
<p>​   其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。</p>
<p>​   表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。</p>
<p>​   将交叉熵看做是代价函数有两点原因。</p>
<p>​   第⼀，它是⾮负的， C &gt; 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。</p>
<p>​   第⼆，如果对于所有的训练输⼊ x，神经元实际的输出接近⽬标值，那么交叉熵将接近 0。</p>
<p>​   假设在这个例⼦中， y = 0 ⽽ a ≈ 0。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 y = 0，⽽第⼆项实际上就是 − ln(1 − a) ≈ 0。反之， y = 1 ⽽ a ≈ 1。所以在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低了。（这里假设输出结果不是0，就是1，实际分类也是这样的）</p>
<p>​   综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将<span><span class="MathJax_Preview">a={\varsigma}(z)</span><script type="math/tex">a={\varsigma}(z)</script></span>代⼊到 公式中应⽤两次链式法则，得到：</p>
<p><span><span class="MathJax_Preview">\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]*\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})*\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}'(z)x_{j}\end{eqnarray}</span><script type="math/tex">\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&=&-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&=&-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}'(z)x_{j}\end{eqnarray}</script></span></p>
<p>​   根据<span><span class="MathJax_Preview">\varsigma(z)=\frac{1}{1+e^{-z}}</span><script type="math/tex">\varsigma(z)=\frac{1}{1+e^{-z}}</script></span> 的定义，和⼀些运算，我们可以得到 <span><span class="MathJax_Preview">{\varsigma}'(z)=\varsigma(z)(1-\varsigma(z))</span><script type="math/tex">{\varsigma}'(z)=\varsigma(z)(1-\varsigma(z))</script></span>。化简后可得：</p>
<p><span><span class="MathJax_Preview">\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)</span><script type="math/tex">\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)</script></span></p>
<p>​   这是⼀个优美的公式。它告诉我们权重学习的速度受到<span><span class="MathJax_Preview">\varsigma(z)-y</span><script type="math/tex">\varsigma(z)-y</script></span>，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在⼆次代价函数中类似⽅程中<span><span class="MathJax_Preview">{\varsigma}'(z)</span><script type="math/tex">{\varsigma}'(z)</script></span>导致的学习缓慢。当我们使⽤交叉熵的时候，<span><span class="MathJax_Preview">{\varsigma}'(z)</span><script type="math/tex">{\varsigma}'(z)</script></span>被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。</p>
<p>​   根据类似的⽅法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到：</p>
<p><span><span class="MathJax_Preview">\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)​</span><script type="math/tex">\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)​</script></span></p>
<p>​   再⼀次, 这避免了⼆次代价函数中类似<span><span class="MathJax_Preview">{\varsigma}'(z)</span><script type="math/tex">{\varsigma}'(z)</script></span>项导致的学习缓慢。</p>
<h3 id="3412-tanhsigmoid">3.4.12 为什么Tanh收敛速度比Sigmoid快？<a class="headerlink" href="#3412-tanhsigmoid" title="Permanent link">&para;</a></h3>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>首先看如下两个函数的求导：</p>
<p><span><span class="MathJax_Preview">tanh^{,}(x)=1-tanh(x)^{2}\in (0,1)​</span><script type="math/tex">tanh^{,}(x)=1-tanh(x)^{2}\in (0,1)​</script></span></p>
<p><span><span class="MathJax_Preview">s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]</span><script type="math/tex">s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]</script></span></p>
<p>由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。</p>
<h2 id="35-batch_size">3.5 Batch_Size<a class="headerlink" href="#35-batch_size" title="Permanent link">&para;</a></h2>
<h3 id="351-batch_size">3.5.1 为什么需要 Batch_Size？<a class="headerlink" href="#351-batch_size" title="Permanent link">&para;</a></h3>
<p>Batch的选择，首先决定的是下降的方向。</p>
<p>如果数据集比较小，可采用全数据集的形式，好处是：</p>
<ol>
<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。</li>
</ol>
<p>对于更大的数据集，假如采用全数据集的形式，坏处是：
1. 随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。
2. 以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。 </p>
<h3 id="352-batch_size">3.5.2 Batch_Size 值的选择<a class="headerlink" href="#352-batch_size" title="Permanent link">&para;</a></h3>
<p>​   假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p>
<p>​   既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？</p>
<p>​   此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。</p>
<h3 id="353-batch_size">3.5.3 在合理范围内，增大Batch_Size有何好处？<a class="headerlink" href="#353-batch_size" title="Permanent link">&para;</a></h3>
<ol>
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</li>
</ol>
<h3 id="354-batch_size">3.5.4 盲目增大 Batch_Size 有何坏处？<a class="headerlink" href="#354-batch_size" title="Permanent link">&para;</a></h3>
<ol>
<li>内存利用率提高了，但是内存容量可能撑不住了。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h3 id="355-batch_size">3.5.5 调节 Batch_Size 对训练效果影响到底如何？<a class="headerlink" href="#355-batch_size" title="Permanent link">&para;</a></h3>
<ol>
<li>Batch_Size 太小，模型表现效果极其糟糕(error飙升)。</li>
<li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li>
<li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li>
<li>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。</li>
<li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。 </li>
</ol>
<h2 id="36">3.6 归一化<a class="headerlink" href="#36" title="Permanent link">&para;</a></h2>
<h3 id="361">3.6.1 归一化含义？<a class="headerlink" href="#361" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>归纳统一样本的统计分布性。归一化在 $ 0-1​$ 之间是统计的概率分布，归一化在$ -1--+1​$ 之间是统计的坐标分布。</p>
</li>
<li>
<p>无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。</p>
</li>
<li>
<p>归一化是统一在 $ 0-1 $ 之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。</p>
</li>
<li>
<p>另外在数据中常存在奇异样本数据，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小。</p>
</li>
</ol>
<h3 id="362">3.6.2 为什么要归一化？<a class="headerlink" href="#362" title="Permanent link">&para;</a></h3>
<ol>
<li>为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。</li>
<li>为了程序运行时收敛加快。 </li>
<li>同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。</li>
<li>避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。</li>
<li>保证输出数据中数值小的不被吞食。 </li>
</ol>
<h3 id="363">3.6.3 为什么归一化能提高求解最优解速度？<a class="headerlink" href="#363" title="Permanent link">&para;</a></h3>
<p><img alt="" src="../img/ch3/3.6.3.1.png" /></p>
<p>​   上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。</p>
<p>​   当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。</p>
<p>​   因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</p>
<h3 id="364-3d">3.6.4 3D 图解未归一化<a class="headerlink" href="#364-3d" title="Permanent link">&para;</a></h3>
<p>例子：</p>
<p>​   假设 $ w1 $ 的范围在 $ [-10, 10] $，而 $ w2 $ 的范围在 $ [-100, 100] $，梯度每次都前进 1 单位，那么在 $ w1 $ 方向上每次相当于前进了 $ 1/20 $，而在 $ w2 $ 上只相当于 $ 1/200 $！某种意义上来说，在 $ w2 $ 上前进的步长更小一些,而 $ w1 $ 在搜索过程中会比 $ w2 $ “走”得更快。</p>
<p>​   这样会导致，在搜索过程中更偏向于 $ w1 $ 的方向。走出了“L”形状，或者成为“之”字形。</p>
<p><img alt="" src="../img/ch3/3-37.png" /></p>
<h3 id="365">3.6.5 归一化有哪些类型？<a class="headerlink" href="#365" title="Permanent link">&para;</a></h3>
<ol>
<li>线性归一化</li>
</ol>
<div>
<div class="MathJax_Preview">
x^{\prime} = \frac{x-min(x)}{max(x) - min(x)}
</div>
<script type="math/tex; mode=display">
x^{\prime} = \frac{x-min(x)}{max(x) - min(x)}
</script>
</div>
<p>​   适用范围：比较适用在数值比较集中的情况。</p>
<p>​   缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。</p>
<ol>
<li>标准差标准化</li>
</ol>
<div>
<div class="MathJax_Preview">
x^{\prime} = \frac{x-\mu}{\sigma}
</div>
<script type="math/tex; mode=display">
x^{\prime} = \frac{x-\mu}{\sigma}
</script>
</div>
<p>​   含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1 其中 $ \mu $ 为所有样本数据的均值，$ \sigma $ 为所有样本数据的标准差。</p>
<ol>
<li>非线性归一化</li>
</ol>
<p>适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 $ log $、指数，正切等。</p>
<h3 id="366">3.6.6 局部响应归一化作用<a class="headerlink" href="#366" title="Permanent link">&para;</a></h3>
<p>​   LRN 是一种提高深度学习准确度的技术方法。LRN 一般是在激活、池化函数后的一种方法。</p>
<p>​   在 ALexNet 中，提出了 LRN 层，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p>
<h3 id="367">3.6.7 理解局部响应归一化<a class="headerlink" href="#367" title="Permanent link">&para;</a></h3>
<p>​   局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），其公式如下：</p>
<div>
<div class="MathJax_Preview">
b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\beta
</div>
<script type="math/tex; mode=display">
b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\beta
</script>
</div>
<p>其中，
1) $ a ​$：表示卷积层（包括卷积操作和池化操作）后的输出结果，是一个四维数组[batch,height,width,channel]。</p>
<ul>
<li>batch：批次数(每一批为一张图片)。</li>
<li>height：图片高度。</li>
<li>width：图片宽度。</li>
<li>channel：通道数。可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数，或理解为处理后的图片深度。</li>
</ul>
<p>2) $ a_{x,y}^i $ 表示在这个输出结构中的一个位置 $ [a,b,c,d] $，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第 $ a $ 张图的第 $ d $ 个通道下的高度为b宽度为c的点。</p>
<p>3) $ N $：论文公式中的 $ N $ 表示通道数 (channel)。</p>
<p>4) $ a <span><span class="MathJax_Preview">，</span><script type="math/tex">，</script></span> n/2 $， $ k $ 分别表示函数中的 input,depth_radius,bias。参数 $ k, n, \alpha, \beta $ 都是超参数，一般设置 $ k=2, n=5, \alpha=1*e-4, \beta=0.75 $</p>
<p>5) $ \sum ​<span><span class="MathJax_Preview">：</span><script type="math/tex">：</script></span> \sum ​$ 叠加的方向是沿着通道方向的，即每个点值的平方和是沿着 $ a ​$ 中的第 3 维 channel 方向的，也就是一个点同方向的前面 $ n/2 ​$ 个通道（最小为第 $ 0 ​$ 个通道）和后 $ n/2 ​$ 个通道（最大为第 $ d-1 ​$ 个通道）的点的平方和(共 $ n+1 ​$ 个点)。而函数的英文注解中也说明了把 input 当成是 $ d ​$ 个 3 维的矩阵，说白了就是把 input 的通道数当作 3 维矩阵的个数，叠加的方向也是在通道方向。 </p>
<p>简单的示意图如下：</p>
<p><img alt="" src="../img/ch3/3.6.7.1.png" /></p>
<h3 id="368-batch-normalization">3.6.8 什么是批归一化（Batch Normalization）<a class="headerlink" href="#368-batch-normalization" title="Permanent link">&para;</a></h3>
<p>​   以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ \sigma(WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。 </p>
<p>​   这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。</p>
<h3 id="369-bn">3.6.9 批归一化（BN）算法的优点<a class="headerlink" href="#369-bn" title="Permanent link">&para;</a></h3>
<p>下面我们来说一下BN算法的优点： 
1. 减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数； 
2. 减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛； 
3. 可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在) 
4. 破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。 
5. 减少梯度消失，加快收敛速度，提高训练精度。</p>
<h3 id="3610-bn">3.6.10 批归一化（BN）算法流程<a class="headerlink" href="#3610-bn" title="Permanent link">&para;</a></h3>
<p>下面给出 BN 算法在训练时的过程</p>
<p>输入：上一层输出结果 $ X = {x_1, x_2, ..., x_m} $，学习参数 $ \gamma, \beta $</p>
<p>算法流程：</p>
<ol>
<li>计算上一层输出数据的均值</li>
</ol>
<div>
<div class="MathJax_Preview">
\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i)
</div>
<script type="math/tex; mode=display">
\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i)
</script>
</div>
<p>其中，$ m ​$ 是此次训练样本 batch 的大小。</p>
<ol>
<li>计算上一层输出数据的标准差</li>
</ol>
<div>
<div class="MathJax_Preview">
\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2
</div>
<script type="math/tex; mode=display">
\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2
</script>
</div>
<ol>
<li>归一化处理，得到</li>
</ol>
<div>
<div class="MathJax_Preview">
\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon}
</div>
<script type="math/tex; mode=display">
\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon}
</script>
</div>
<p>其中 $ \epsilon $ 是为了避免分母为 0 而加进去的接近于 0 的很小值</p>
<ol>
<li>重构，对经过上面归一化处理得到的数据进行重构，得到</li>
</ol>
<div>
<div class="MathJax_Preview">
y_i = \gamma \hat x_i + \beta
</div>
<script type="math/tex; mode=display">
y_i = \gamma \hat x_i + \beta
</script>
</div>
<p>其中，$ \gamma, \beta $ 为可学习参数。</p>
<p>注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ \mu_{\beta} $ 和标准差 $ \sigma_{\beta}^2 $。此时，均值 $ \mu_{\beta} $ 是计算所有 batch $ \mu_{\beta} $ 值的平均值得到，标准差 $ \sigma_{\beta}^2 $ 采用每个batch $ \sigma_{\beta}^2 $  的无偏估计得到。</p>
<h3 id="3611">3.6.11 批归一化和群组归一化比较<a class="headerlink" href="#3611" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>名称</th>
<th align="left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>批量归一化（Batch Normalization，以下简称 BN）</td>
<td align="left">可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。</td>
</tr>
<tr>
<td>群组归一化 Group Normalization (简称 GN)</td>
<td align="left">GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。</td>
</tr>
<tr>
<td>比较</td>
<td align="left">在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ;当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。</td>
</tr>
</tbody>
</table>
<h3 id="3612-weight-normalizationbatch-normalization">3.6.12 Weight Normalization和Batch Normalization比较<a class="headerlink" href="#3612-weight-normalizationbatch-normalization" title="Permanent link">&para;</a></h3>
<p>​   Weight Normalization 和 Batch Normalization 都属于参数重写（Reparameterization）的方法，只是采用的方式不同。</p>
<p>​   Weight Normalization 是对网络权值$  W $ 进行 normalization，因此也称为 Weight Normalization；</p>
<p>​   Batch Normalization 是对网络某一层输入数据进行 normalization。</p>
<p>​   Weight Normalization相比Batch Normalization有以下三点优势：</p>
<ol>
<li>
<p>Weight Normalization 通过重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入 minbatch 的依赖，适用于 RNN（LSTM）网络（Batch Normalization 不能直接用于RNN，进行 normalization 操作，原因在于：1) RNN 处理的 Sequence 是变长的；2) RNN 是基于 time step 计算，如果直接使用 Batch Normalization 处理，需要保存每个 time step 下，mini btach 的均值和方差，效率低且占内存）。</p>
</li>
<li>
<p>Batch Normalization 基于一个 mini batch 的数据计算均值和方差，而不是基于整个 Training set 来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization 不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。相反，Weight Normalization 对通过标量 $ g $ 和向量 $ v $ 对权重 $ W $ 进行重写，重写向量 $ v $ 是固定的，因此，基于 Weight Normalization 的 Normalization 可以看做比 Batch Normalization 引入更少的噪声。    </p>
</li>
<li>
<p>不需要额外的存储空间来保存 mini batch 的均值和方差，同时实现 Weight Normalization 时，对深度学习网络进行正向信号传播和反向梯度计算带来的额外计算开销也很小。因此，要比采用 Batch Normalization 进行 normalization 操作时，速度快。  但是 Weight Normalization 不具备 Batch Normalization 把网络每一层的输出 Y 固定在一个变化范围的作用。因此，采用 Weight Normalization 进行 Normalization 时需要特别注意参数初始值的选择。</p>
</li>
</ol>
<h3 id="3613-batch-normalization">3.6.13 Batch Normalization在什么时候用比较合适？<a class="headerlink" href="#3613-batch-normalization" title="Permanent link">&para;</a></h3>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​   在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>
<p>​   BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。</p>
<h2 id="37-fine-tuning">3.7 预训练与微调(fine tuning)<a class="headerlink" href="#37-fine-tuning" title="Permanent link">&para;</a></h2>
<h3 id="371">3.7.1 为什么无监督预训练可以帮助深度学习？<a class="headerlink" href="#371" title="Permanent link">&para;</a></h3>
<p>深度网络存在问题:</p>
<ol>
<li>
<p>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</p>
</li>
<li>
<p>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</p>
</li>
<li>
<p>梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</p>
</li>
</ol>
<p><strong>解决方法：</strong></p>
<p>​   逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。</p>
<p>经过预训练最终能得到比较好的局部最优解。</p>
<h3 id="372-fine-tuning">3.7.2 什么是模型微调fine tuning<a class="headerlink" href="#372-fine-tuning" title="Permanent link">&para;</a></h3>
<p>​   用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning). </p>
<p><strong>模型的微调举例说明：</strong></p>
<p>​   我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。</p>
<h3 id="373">3.7.3 微调时候网络参数是否更新？<a class="headerlink" href="#373" title="Permanent link">&para;</a></h3>
<p>答案：会更新。</p>
<ol>
<li>finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。 </li>
<li>直接训练是按照网络定义指定的方式初始化。</li>
<li>finetune是用你已经有的参数文件来初始化。</li>
</ol>
<h3 id="374-fine-tuning">3.7.4 fine-tuning 模型的三种状态<a class="headerlink" href="#374-fine-tuning" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>状态一：只预测，不训练。
特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</p>
</li>
<li>
<p>状态二：训练，但只训练最后分类层。
特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</p>
</li>
<li>
<p>状态三：完全训练，分类层+之前卷积层都训练
特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</p>
</li>
</ol>
<h2 id="38">3.8 权重偏差初始化<a class="headerlink" href="#38" title="Permanent link">&para;</a></h2>
<h3 id="381-0">3.8.1 全都初始化为 0<a class="headerlink" href="#381-0" title="Permanent link">&para;</a></h3>
<p><strong>偏差初始化陷阱</strong>： 都初始化为 0。</p>
<p><strong>产生陷阱原因</strong>：因为并不知道在训练神经网络中每一个权重最后的值，但是如果进行了恰当的数据归一化后，我们可以有理由认为有一半的权重是正的，另一半是负的。令所有权重都初始化为 0，如果神经网络计算出来的输出值是一样的，神经网络在进行反向传播算法计算出来的梯度值也一样，并且参数更新值也一样。更一般地说，如果权重初始化为同一个值，网络就是对称的。</p>
<p><strong>形象化理解</strong>：在神经网络中考虑梯度下降的时候，设想你在爬山，但身处直线形的山谷中，两边是对称的山峰。由于对称性，你所在之处的梯度只能沿着山谷的方向，不会指向山峰；你走了一步之后，情况依然不变。结果就是你只能收敛到山谷中的一个极大值，而走不到山峰上去。</p>
<h3 id="382">3.8.2 全都初始化为同样的值<a class="headerlink" href="#382" title="Permanent link">&para;</a></h3>
<p>​   偏差初始化陷阱： 都初始化为一样的值。
​   以一个三层网络为例：
首先看下结构</p>
<p><img alt="" src="../img/ch3/3.8.2.1.png" /></p>
<p>它的表达式为： </p>
<div>
<div class="MathJax_Preview">
a_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})
</div>
<script type="math/tex; mode=display">
a_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})
</script>
</div>
<div>
<div class="MathJax_Preview">
a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})
</div>
<script type="math/tex; mode=display">
a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})
</script>
</div>
<div>
<div class="MathJax_Preview">
a_3^{(2)} = f(W_{31}^{(1)} x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})
</div>
<script type="math/tex; mode=display">
a_3^{(2)} = f(W_{31}^{(1)} x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})
</script>
</div>
<div>
<div class="MathJax_Preview">
h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)} a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)})
</div>
<script type="math/tex; mode=display">
h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)} a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)})
</script>
</div>
<div>
<div class="MathJax_Preview">
xa_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + 
</div>
<script type="math/tex; mode=display">
xa_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + 
</script>
</div>
<p>如果每个权重都一样，那么在多层网络中，从第二层开始，每一层的输入值都是相同的了也就是$ a1=a2=a3=.... $，既然都一样，就相当于一个输入了，为啥呢？？</p>
<p>如果是反向传递算法（如果这里不明白请看上面的连接），其中的偏置项和权重项的迭代的偏导数计算公式如下</p>
<p>$$
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b;x,y) = a_j^{(l)} \delta_i^{(l+1)}</p>
<p>\frac{\partial}{\partial b_{i}^{(l)}} J(W,b;x,y) = \delta_i^{(l+1)}
$$</p>
<p>$ \delta ​$ 的计算公式</p>
<div>
<div class="MathJax_Preview">
\delta_i^{(l)} = (\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \delta_j^{(l+1)} ) f^{\prime}(z_i^{(l)})
</div>
<script type="math/tex; mode=display">
\delta_i^{(l)} = (\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \delta_j^{(l+1)} ) f^{\prime}(z_i^{(l)})
</script>
</div>
<p>如果用的是 sigmoid 函数</p>
<div>
<div class="MathJax_Preview">
f^{\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)})
</div>
<script type="math/tex; mode=display">
f^{\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)})
</script>
</div>
<p>把后两个公式代入，可以看出所得到的梯度下降法的偏导相同，不停的迭代，不停的相同，不停的迭代，不停的相同......，最后就得到了相同的值（权重和截距）。</p>
<h3 id="383">3.8.3 初始化为小的随机数<a class="headerlink" href="#383" title="Permanent link">&para;</a></h3>
<p>​   将权重初始化为很小的数字是一个普遍的打破网络对称性的解决办法。这个想法是，神经元在一开始都是随机的、独一无二的，所以它们会计算出不同的更新，并将自己整合到整个网络的各个部分。一个权重矩阵的实现可能看起来像 $ W=0.01∗np.random.randn(D,H) $，其中 randn 是从均值为 0 的单位标准高斯分布进行取样。通过这个公式(函数)，每个神经元的权重向量初始化为一个从多维高斯分布取样的随机向量，所以神经元在输入空间中指向随机的方向(so the neurons point in random direction in the input space). 应该是指输入空间对于随机方向有影响)。其实也可以从均匀分布中来随机选取小数，但是在实际操作中看起来似乎对最后的表现并没有太大的影响。</p>
<p>​   备注：并不是数字越小就会表现的越好。比如，如果一个神经网络层的权重非常小，那么在反向传播算法就会计算出很小的梯度(因为梯度 gradient 是与权重成正比的)。在网络不断的反向传播过程中将极大地减少“梯度信号”，并可能成为深层网络的一个需要注意的问题。</p>
<h3 id="384-1sqrt-n">3.8.4 用 $ 1/\sqrt n $ 校准方差<a class="headerlink" href="#384-1sqrt-n" title="Permanent link">&para;</a></h3>
<p>​   上述建议的一个问题是，随机初始化神经元的输出的分布有一个随输入量增加而变化的方差。结果证明，我们可以通过将其权重向量按其输入的平方根(即输入的数量)进行缩放，从而将每个神经元的输出的方差标准化到 1。也就是说推荐的启发式方法 (heuristic) 是将每个神经元的权重向量按下面的方法进行初始化: $ w=np.random.randn(n)/\sqrt n $，其中 n 表示输入的数量。这保证了网络中所有的神经元最初的输出分布大致相同，并在经验上提高了收敛速度。</p>
<h3 id="385-sparse-initialazation">3.8.5 稀疏初始化(Sparse Initialazation)<a class="headerlink" href="#385-sparse-initialazation" title="Permanent link">&para;</a></h3>
<p>​   另一种解决未校准方差问题的方法是把所有的权重矩阵都设为零，但是为了打破对称性，每个神经元都是随机连接地(从如上面所介绍的一个小的高斯分布中抽取权重)到它下面的一个固定数量的神经元。一个典型的神经元连接的数目可能是小到 10 个。</p>
<h3 id="386">3.8.6 初始化偏差<a class="headerlink" href="#386" title="Permanent link">&para;</a></h3>
<p>​   将偏差初始化为零是可能的，也是很常见的，因为非对称性破坏是由权重的小随机数导致的。因为 ReLU 具有非线性特点，所以有些人喜欢使用将所有的偏差设定为小的常数值如 0.01，因为这样可以确保所有的 ReLU 单元在最开始就激活触发(fire)并因此能够获得和传播一些梯度值。然而，这是否能够提供持续的改善还不太清楚(实际上一些结果表明这样做反而使得性能更加糟糕)，所以更通常的做法是简单地将偏差初始化为 0.</p>
<h2 id="39">3.9 学习率<a class="headerlink" href="#39" title="Permanent link">&para;</a></h2>
<h3 id="391">3.9.1 学习率的作用<a class="headerlink" href="#391" title="Permanent link">&para;</a></h3>
<p>​   在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost 来估计模型的参数。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。</p>
<p>​   在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。故而如何对学习率的更新成为了研究者的关注点。
​   在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减</p>
<h3 id="392">3.9.2 学习率衰减常用参数有哪些<a class="headerlink" href="#392" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>learning_rate</td>
<td>初始学习率</td>
</tr>
<tr>
<td>global_step</td>
<td>用于衰减计算的全局步数，非负，用于逐步计算衰减指数</td>
</tr>
<tr>
<td>decay_steps</td>
<td>衰减步数，必须是正值，决定衰减周期</td>
</tr>
<tr>
<td>decay_rate</td>
<td>衰减率</td>
</tr>
<tr>
<td>end_learning_rate</td>
<td>最低的最终学习率</td>
</tr>
<tr>
<td>cycle</td>
<td>学习率下降后是否重新上升</td>
</tr>
<tr>
<td>alpha</td>
<td>最小学习率</td>
</tr>
<tr>
<td>num_periods</td>
<td>衰减余弦部分的周期数</td>
</tr>
<tr>
<td>initial_variance</td>
<td>噪声的初始方差</td>
</tr>
<tr>
<td>variance_decay</td>
<td>衰减噪声的方差</td>
</tr>
</tbody>
</table>
<h3 id="393">3.9.3 分段常数衰减<a class="headerlink" href="#393" title="Permanent link">&para;</a></h3>
<p>​   分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。</p>
<p><img alt="" src="../img/ch3/learnrate1.png" /></p>
<h3 id="394">3.9.4 指数衰减<a class="headerlink" href="#394" title="Permanent link">&para;</a></h3>
<p>​   以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为：
$$
decayed{_}learning{_}rate =learning{_}rate*decay{_}rate^{\frac{global{_step}}{decay{_}steps}}
$$
​   这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如下图所示，绿色的为学习率随
训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。</p>
<p><img alt="" src="../img/ch3/learnrate2.png" /></p>
<h3 id="395">3.9.5 自然指数衰减<a class="headerlink" href="#395" title="Permanent link">&para;</a></h3>
<p>​   它与指数衰减方式相似，不同的在于它的衰减底数是<span><span class="MathJax_Preview">e</span><script type="math/tex">e</script></span>，故而其收敛的速度更快，一般用于相对比较
容易训练的网络，便于较快的收敛，其更新规则如下
$$
decayed{_}learning{_}rate =learning{_}rate*e^{\frac{-decay{_rate}}{global{_}step}}
$$
​   下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p>
<p><img alt="" src="../img/ch3/learnrate3.png" /></p>
<h3 id="396">3.9.6 多项式衰减<a class="headerlink" href="#396" title="Permanent link">&para;</a></h3>
<p>​   应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照
给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示。
$$
global{_}step=min(global{_}step,decay{_}steps)
$$</p>
<div>
<div class="MathJax_Preview">
decayed{\_}learning{\_}rate =(learning{\_}rate-end{\_}learning{\_}rate)* \left( 1-\frac{global{\_step}}{decay{\_}steps}\right)^{power} \\
 +end{\_}learning{\_}rate
</div>
<script type="math/tex; mode=display">
decayed{\_}learning{\_}rate =(learning{\_}rate-end{\_}learning{\_}rate)* \left( 1-\frac{global{\_step}}{decay{\_}steps}\right)^{power} \\
 +end{\_}learning{\_}rate
</script>
</div>
<p>​   需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示.它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。
$$
decay{_}steps = decay{_}steps*ceil \left( \frac{global{_}step}{decay{_}steps}\right)
$$
​   如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p>
<p><img alt="" src="../img/ch3/learnrate4.png" /></p>
<h3 id="397">3.9.7 余弦衰减<a class="headerlink" href="#397" title="Permanent link">&para;</a></h3>
<p>​   余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：
$$
global{_}step=min(global{_}step,decay{_}steps)
$$</p>
<div>
<div class="MathJax_Preview">
cosine{\_}decay=0.5*\left( 1+cos\left( \pi* \frac{global{\_}step}{decay{\_}steps}\right)\right)
</div>
<script type="math/tex; mode=display">
cosine{\_}decay=0.5*\left( 1+cos\left( \pi* \frac{global{\_}step}{decay{\_}steps}\right)\right)
</script>
</div>
<div>
<div class="MathJax_Preview">
decayed=(1-\alpha)*cosine{\_}decay+\alpha
</div>
<script type="math/tex; mode=display">
decayed=(1-\alpha)*cosine{\_}decay+\alpha
</script>
</div>
<div>
<div class="MathJax_Preview">
decayed{\_}learning{\_}rate=learning{\_}rate*decayed
</div>
<script type="math/tex; mode=display">
decayed{\_}learning{\_}rate=learning{\_}rate*decayed
</script>
</div>
<p>​   如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式。</p>
<p><img alt="" src="../img/ch3/learnrate5.png" /></p>
<h2 id="312-dropout">3.12 Dropout 系列问题<a class="headerlink" href="#312-dropout" title="Permanent link">&para;</a></h2>
<h3 id="3121">3.12.1 为什么要正则化？<a class="headerlink" href="#3121" title="Permanent link">&para;</a></h3>
<ol>
<li>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。  </li>
<li>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。</li>
</ol>
<h3 id="3122">3.12.2 为什么正则化有利于预防过拟合？<a class="headerlink" href="#3122" title="Permanent link">&para;</a></h3>
<p><img alt="" src="../img/ch3/3.12.2.1.png" /> 
<img alt="" src="../img/ch3/3.12.2.2.png" /> </p>
<p>左图是高偏差，右图是高方差，中间是Just Right，这几张图我们在前面课程中看到过。  </p>
<h3 id="3123-dropout">3.12.3 理解dropout正则化<a class="headerlink" href="#3123-dropout" title="Permanent link">&para;</a></h3>
<p>​   Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？  </p>
<p>​   直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。  </p>
<h3 id="3124-dropout">3.12.4 dropout率的选择<a class="headerlink" href="#3124-dropout" title="Permanent link">&para;</a></h3>
<ol>
<li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li>
<li>dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8） </li>
<li>对参数 $ w $ 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。</li>
<li>球形半径 $ c $ 是一个需要调整的参数，可以使用验证集进行参数调优。</li>
<li>dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。</li>
<li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。</li>
</ol>
<h3 id="3125-dropout">3.12.5 dropout有什么缺点？<a class="headerlink" href="#3125-dropout" title="Permanent link">&para;</a></h3>
<p>​   dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。  </p>
<h2 id="313_1">3.13 深度学习中常用的数据增强方法？<a class="headerlink" href="#313_1" title="Permanent link">&para;</a></h2>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<ul>
<li>
<p>Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；</p>
</li>
<li>
<p>PCA  Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；</p>
</li>
<li>
<p>Random Scale：尺度变换；</p>
</li>
<li>
<p>Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换；</p>
</li>
<li>
<p>Horizontal/Vertical Flip：水平/垂直翻转；</p>
</li>
<li>
<p>Shift：平移变换；</p>
</li>
<li>
<p>Rotation/Reflection：旋转/仿射变换；</p>
</li>
<li>
<p>Noise：高斯噪声、模糊处理；</p>
</li>
<li>
<p>Label Shuffle：类别不平衡数据的增广；</p>
</li>
</ul>
<h2 id="314-internal-covariate-shift">3.14 如何理解 Internal Covariate Shift？<a class="headerlink" href="#314-internal-covariate-shift" title="Permanent link">&para;</a></h2>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​   深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p>
<p>​   Google 将这一现象总结为 Internal Covariate Shift，简称 ICS。 什么是 ICS 呢？</p>
<p>​   大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。</p>
<p>​   大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p>
<p><strong>那么ICS会导致什么问题？</strong></p>
<p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
<p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</p>
<p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</p>
<p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p>
<h2 id="_2">参考文献<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>[1] Rosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain.[J]. Psychological Review, 1958, 65(6):386-408.</p>
<p>[2] Duvenaud D , Rippel O , Adams R P , et al. Avoiding pathologies in very deep networks[J]. Eprint Arxiv, 2014:202-210.</p>
<p>[3] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors[J]. Cognitive modeling, 1988, 5(3): 1.</p>
<p>[4] Hecht-Nielsen R. Theory of the backpropagation neural network[M]//Neural networks for perception. Academic Press, 1992: 65-93.</p>
<p>[5] Felice M. Which deep learning network is best for you?| CIO[J]. 2017.</p>
<p>[6] Conneau A, Schwenk H, Barrault L, et al. Very deep convolutional networks for natural language processing[J]. arXiv preprint arXiv:1606.01781, 2016, 2.</p>
<p>[7] Ba J, Caruana R. Do deep nets really need to be deep?[C]//Advances in neural information processing systems. 2014: 2654-2662.</p>
<p>[8] Nielsen M A. Neural networks and deep learning[M]. USA: Determination press, 2015.</p>
<p>[9] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.</p>
<p>[10] 周志华. 机器学习[M].清华大学出版社, 2016.</p>
<p>[11] Kim J, Kwon Lee J, Mu Lee K. Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 1646-1654.</p>
<p>[12] Chen Y, Lin Z, Zhao X, et al. Deep learning-based classification of hyperspectral data[J]. IEEE Journal of Selected topics in applied earth observations and remote sensing, 2014, 7(6): 2094-2107.</p>
<p>[13] Domhan T, Springenberg J T, Hutter F. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves[C]//Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.</p>
<p>[14] Maclaurin D, Duvenaud D, Adams R. Gradient-based hyperparameter optimization through reversible learning[C]//International Conference on Machine Learning. 2015: 2113-2122.</p>
<p>[15] Srivastava R K, Greff K, Schmidhuber J. Training very deep networks[C]//Advances in neural information processing systems. 2015: 2377-2385.</p>
<p>[16] Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.</p>
<p>[17] Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28<sup>th</sup> international conference on machine learning (ICML-11). 2011: 689-696.</p>
<p>[18] Deng L, Yu D. Deep learning: methods and applications[J]. Foundations and Trends® in Signal Processing, 2014, 7(3–4): 197-387.</p>
<p>[19] Erhan D, Bengio Y, Courville A, et al. Why does unsupervised pre-training help deep learning?[J]. Journal of Machine Learning Research, 2010, 11(Feb): 625-660.</p>
<p>[20] Dong C, Loy C C, He K, et al. Learning a deep convolutional network for image super resolution[C]//European conference on computer vision. Springer, Cham, 2014: 184-199.</p>
<p>[21] 郑泽宇，梁博文，顾思宇.TensorFlow：实战Google深度学习框架（第2版）[M].电子工业出版社,2018.</p>
<p>[22] 焦李成. 深度学习优化与识别[M].清华大学出版社,2017.</p>
<p>[23] 吴岸城. 神经网络与深度学习[M].电子工业出版社,2016.</p>
<p>[24] Wei, W.G.H., Liu, T., Song, A., et al. (2018) An Adaptive Natural Gradient Method with Adaptive Step Size in Multilayer Perceptrons. Chinese Automation Congress, 1593-1597.</p>
<p>[25] Y Feng, Y Li.An Overview of Deep Learning Optimization Methods and Learning Rate Attenuation Methods[J].Hans Journal of Data Mining,2018,8(4),186-200.</p>
                
                  
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://hai5g.cn/aiwiki/qa500/ch03_深度学习基础/第三章_深度学习基础/";
      this.page.identifier =
        "/qa500/ch03_深度学习基础/第三章_深度学习基础/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//AI-Wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../ch02_机器学习基础/第二章_机器学习基础/" title="第二章_机器学习基础" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                第二章_机器学习基础
              </span>
            </div>
          </a>
        
        
          <a href="../../ch04_经典网络/第四章_经典网络/" title="第四章_经典网络" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                第四章_经典网络
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 AI Wiki Team
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.39abc4af.js"></script>
      
        
        
          
          <script src="../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
        <script src="https://cdn.jsdelivr.net/gh/ethantw/Han@3.3.0/dist/han.min.js"></script>
      
        <script src="../../../_static/js/extra.js?v=10"></script>
      
        <script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>