



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="AI Wiki 是一个编程竞赛知识整合站点，提供有趣又实用的编程竞赛知识以及其他有帮助的内容，帮助广大编程竞赛爱好者更快更深入地学习编程竞赛">
      
      
        <link rel="canonical" href="https://hai5g.cn/aiwiki/qa500/ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署/">
      
      
        <meta name="author" content="AI Wiki Team">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../../favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>第十七章_模型压缩、加速及移动端部署 - AI Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,400,400i,700|Fira+Mono">
        <style>body,input{font-family:"Fira Sans","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Fira Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
      <link rel="manifest" href="../../../manifest.webmanifest">
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/ah@1.5.0/han.min.css">
    
      <link rel="stylesheet" href="../../../_static/css/extra.css?v=11">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#_1" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-header-nav__button md-logo">
          
            <i class="md-icon">school</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              AI Wiki
            </span>
            <span class="md-header-nav__topic">
              第十七章_模型压缩、加速及移动端部署
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../.." title="简介" class="md-tabs__link">
          简介
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../Coursera_ML_AndrewNg/" title="机器学习" class="md-tabs__link">
          机器学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../" title="深度学习500问" class="md-tabs__link md-tabs__link--active">
          深度学习500问
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../lihang/" title="统计学习" class="md-tabs__link">
          统计学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../hands-on-ml-zh/README.old/" title="Sklearn与TensorFlow" class="md-tabs__link">
          Sklearn与TensorFlow
        </a>
      
    </li>
  

      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://hai5g.cn/aiwiki" title="AI Wiki" class="md-nav__button md-logo">
      
        <i class="md-icon">school</i>
      
    </a>
    AI Wiki
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/myourdream/aiwiki/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    aiwiki
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      简介
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        简介
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../.." title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/AI学习路线/" title="AI学习路线1" class="md-nav__link">
      AI学习路线1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/ai-union-201904/" title="AI学习路线2" class="md-nav__link">
      AI学习路线2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v0.1/" title="AI 路线图v0.1" class="md-nav__link">
      AI 路线图v0.1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v0.2/" title="AI 路线图v0.2" class="md-nav__link">
      AI 路线图v0.2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/ai-roadmap/v1.0/" title="ApacheCN 人工智能知识树" class="md-nav__link">
      ApacheCN 人工智能知识树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7" type="checkbox" id="nav-1-7">
    
    <label class="md-nav__link" for="nav-1-7">
      工具软件
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-1-7">
        工具软件
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/judgers/" title="评测工具" class="md-nav__link">
      评测工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/editors/" title="编辑工具" class="md-nav__link">
      编辑工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/wsl/" title="WSL (Windows 10)" class="md-nav__link">
      WSL (Windows 10)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/spj/" title="Special Judge" class="md-nav__link">
      Special Judge
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1-7-5" type="checkbox" id="nav-1-7-5">
    
    <label class="md-nav__link" for="nav-1-7-5">
      Testlib
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-1-7-5">
        Testlib
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/" title="Testlib 简介" class="md-nav__link">
      Testlib 简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/general/" title="通用" class="md-nav__link">
      通用
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/generator/" title="Generator" class="md-nav__link">
      Generator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/validator/" title="Validator" class="md-nav__link">
      Validator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/interactor/" title="Interactor" class="md-nav__link">
      Interactor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/testlib/checker/" title="Checker" class="md-nav__link">
      Checker
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/docker-deploy/" title="Docker 部署" class="md-nav__link">
      Docker 部署
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/about/" title="关于本项目" class="md-nav__link">
      关于本项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../intro/faq/" title="F.A.Q." class="md-nav__link">
      F.A.Q.
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      机器学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        机器学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/SUMMARY/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/math/" title="数学基础" class="md-nav__link">
      数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week1/" title="week1" class="md-nav__link">
      week1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week2/" title="week2" class="md-nav__link">
      week2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week3/" title="week3" class="md-nav__link">
      week3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week4/" title="week4" class="md-nav__link">
      week4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week5/" title="week5" class="md-nav__link">
      week5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week6/" title="week6" class="md-nav__link">
      week6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week7/" title="week7" class="md-nav__link">
      week7
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week8/" title="week8" class="md-nav__link">
      week8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week9/" title="week9" class="md-nav__link">
      week9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../Coursera_ML_AndrewNg/markdown/week10/" title="week10" class="md-nav__link">
      week10
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      深度学习500问
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        深度学习500问
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../content/" title="目录" class="md-nav__link">
      目录
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch01_math/ch01_math/" title="第一章_数学基础" class="md-nav__link">
      第一章_数学基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch02_机器学习基础/第二章_机器学习基础/" title="第二章_机器学习基础" class="md-nav__link">
      第二章_机器学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch03_深度学习基础/第三章_深度学习基础/" title="第三章_深度学习基础" class="md-nav__link">
      第三章_深度学习基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch04_经典网络/第四章_经典网络/" title="第四章_经典网络" class="md-nav__link">
      第四章_经典网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch05_卷积神经网络(CNN)/第五章 卷积神经网络（CNN）/" title="第五章 卷积神经网络（CNN）" class="md-nav__link">
      第五章 卷积神经网络（CNN）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch06_循环神经网络(RNN)/第六章_循环神经网络(RNN)/" title="第六章_循环神经网络(RNN)" class="md-nav__link">
      第六章_循环神经网络(RNN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch07_生成对抗网络(GAN)/ch7/" title="第七章_生成对抗网络(GAN)" class="md-nav__link">
      第七章_生成对抗网络(GAN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch08_目标检测/第八章_目标检测/" title="第八章_目标检测" class="md-nav__link">
      第八章_目标检测
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch09_图像分割/第九章_图像分割/" title="第九章_图像分割" class="md-nav__link">
      第九章_图像分割
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch10_强化学习/第十章_强化学习/" title="第十章_强化学习" class="md-nav__link">
      第十章_强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch11_迁移学习/第十一章_迁移学习/" title="第十一章_迁移学习" class="md-nav__link">
      第十一章_迁移学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch12_网络搭建及训练/第十二章_网络搭建及训练/" title="第十二章_网络搭建及训练" class="md-nav__link">
      第十二章_网络搭建及训练
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch13_优化算法/第十三章_优化算法/" title="第十三章_优化算法" class="md-nav__link">
      第十三章_优化算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch14_超参数调整/第十四章_超参数调整/" title="第十四章_超参数调整" class="md-nav__link">
      第十四章_超参数调整
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch15_GPU和框架选型/第十五章_异构运算、GPU及框架选型/" title="第十五章_异构运算、GPU及框架选型" class="md-nav__link">
      第十五章_异构运算、GPU及框架选型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch16_自然语言处理(NLP)/第十六章_NLP/" title="第十六章_NLP" class="md-nav__link">
      第十六章_NLP
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        第十七章_模型压缩、加速及移动端部署
      </label>
    
    <a href="./" title="第十七章_模型压缩、加速及移动端部署" class="md-nav__link md-nav__link--active">
      第十七章_模型压缩、加速及移动端部署
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#171" title="17.1 模型压缩理解" class="md-nav__link">
    17.1 模型压缩理解
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#172" title="17.2 为什么需要模型压缩和加速？" class="md-nav__link">
    17.2 为什么需要模型压缩和加速？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#173" title="17.3 模型压缩的必要性及可行性" class="md-nav__link">
    17.3 模型压缩的必要性及可行性
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#174" title="17.4 目前有哪些深度学习模型压缩方法？" class="md-nav__link">
    17.4 目前有哪些深度学习模型压缩方法？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1741" title="17.4.1 前端压缩和后端压缩对比" class="md-nav__link">
    17.4.1 前端压缩和后端压缩对比
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1742" title="17.4.2 网络剪枝" class="md-nav__link">
    17.4.2 网络剪枝
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1743" title="17.4.3 典型剪枝方法对比" class="md-nav__link">
    17.4.3 典型剪枝方法对比
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1744" title="17.4.4 网络蒸馏" class="md-nav__link">
    17.4.4 网络蒸馏
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1745" title="17.4.5 前端压缩" class="md-nav__link">
    17.4.5 前端压缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1746" title="17.4.6 后端压缩" class="md-nav__link">
    17.4.6 后端压缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1746_1" title="17.4.6 低秩分解" class="md-nav__link">
    17.4.6 低秩分解
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1747" title="17.4.7 总体压缩效果评价指标有哪些？" class="md-nav__link">
    17.4.7 总体压缩效果评价指标有哪些？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1748" title="17.4.8 几种轻量化网络结构对比" class="md-nav__link">
    17.4.8 几种轻量化网络结构对比
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1749" title="17.4.9 网络压缩未来研究方向有哪些？" class="md-nav__link">
    17.4.9 网络压缩未来研究方向有哪些？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#175" title="17.5 目前有哪些深度学习模型优化加速方法？" class="md-nav__link">
    17.5 目前有哪些深度学习模型优化加速方法？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1751" title="17.5.1 模型优化加速方法" class="md-nav__link">
    17.5.1 模型优化加速方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1752-tensorrt" title="17.5.2 TensorRT加速原理" class="md-nav__link">
    17.5.2 TensorRT加速原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1753-tensorrt" title="17.5.3 TensorRT如何优化重构模型？" class="md-nav__link">
    17.5.3 TensorRT如何优化重构模型？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1754-tensorrt" title="17.5.4 TensorRT加速效果如何？" class="md-nav__link">
    17.5.4 TensorRT加速效果如何？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#176-4" title="17.6 影响神经网络速度的4个因素（再稍微详细一点）" class="md-nav__link">
    17.6 影响神经网络速度的4个因素（再稍微详细一点）
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#177" title="17.7 压缩和加速方法如何选择？" class="md-nav__link">
    17.7 压缩和加速方法如何选择？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#178" title="17.8 改变网络结构设计为什么会实现模型压缩、加速？" class="md-nav__link">
    17.8 改变网络结构设计为什么会实现模型压缩、加速？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1781-group-convolution" title="17.8.1 Group convolution" class="md-nav__link">
    17.8.1 Group convolution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1782-depthwise-separable-convolution" title="17.8.2. Depthwise separable convolution" class="md-nav__link">
    17.8.2. Depthwise separable convolution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1783-channelmac" title="17.8.3 输入输出的channel相同时，MAC最小" class="md-nav__link">
    17.8.3 输入输出的channel相同时，MAC最小
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1784" title="17.8.4 减少组卷积的数量" class="md-nav__link">
    17.8.4 减少组卷积的数量
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1785" title="17.8.5 减少网络碎片化程度(分支数量)" class="md-nav__link">
    17.8.5 减少网络碎片化程度(分支数量)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1787" title="17.8.7 减少元素级操作" class="md-nav__link">
    17.8.7 减少元素级操作
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#179" title="17.9 常用的轻量级网络有哪些？" class="md-nav__link">
    17.9 常用的轻量级网络有哪些？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1791-sequeezenet" title="17.9.1 SequeezeNet" class="md-nav__link">
    17.9.1 SequeezeNet
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" title="1.1 设计思想" class="md-nav__link">
    1.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" title="1.2 网络架构" class="md-nav__link">
    1.2 网络架构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" title="1.3实验结果" class="md-nav__link">
    1.3实验结果
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1792-mobilenet" title="17.9.2 MobileNet" class="md-nav__link">
    17.9.2 MobileNet
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" title="2.1 设计思想" class="md-nav__link">
    2.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" title="2.2 网络架构" class="md-nav__link">
    2.2 网络架构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" title="2.3 实验结果" class="md-nav__link">
    2.3 实验结果
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1793-mobilenet-v2" title="17.9.3 MobileNet-v2" class="md-nav__link">
    17.9.3 MobileNet-v2
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" title="3.1 设计思想" class="md-nav__link">
    3.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" title="3.2 网络架构" class="md-nav__link">
    3.2 网络架构
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1794-xception" title="17.9.4 Xception" class="md-nav__link">
    17.9.4 Xception
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" title="4.1设计思想" class="md-nav__link">
    4.1设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" title="4.2网络架构" class="md-nav__link">
    4.2网络架构
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1795-shufflenet-v1" title="17.9.5 ShuffleNet-v1" class="md-nav__link">
    17.9.5 ShuffleNet-v1
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51" title="5.1 设计思想" class="md-nav__link">
    5.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52" title="5.2 网络架构" class="md-nav__link">
    5.2 网络架构
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1796-shufflenet-v2" title="17.9.6 ShuffleNet-v2" class="md-nav__link">
    17.9.6 ShuffleNet-v2
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61" title="6.1 设计思想" class="md-nav__link">
    6.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62" title="6.2 网络结构" class="md-nav__link">
    6.2 网络结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64-shufflenet-v2" title="6.4  ShuffleNet-v2具有高精度的原因" class="md-nav__link">
    6.4  ShuffleNet-v2具有高精度的原因
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1710" title="17.10 现有移动端开源框架及其特点" class="md-nav__link">
    17.10 现有移动端开源框架及其特点
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#17101-ncnn" title="17.10.1 NCNN" class="md-nav__link">
    17.10.1 NCNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17102-qnnpack" title="17.10.2 QNNPACK" class="md-nav__link">
    17.10.2 QNNPACK
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17103-prestissimo" title="17.10.3 Prestissimo" class="md-nav__link">
    17.10.3 Prestissimo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17104-mdlmobile-deep-learning" title="17.10.4 MDL（mobile-deep-learning）" class="md-nav__link">
    17.10.4 MDL（mobile-deep-learning）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17105-paddle-mobile" title="17.10.5 Paddle-Mobile" class="md-nav__link">
    17.10.5 Paddle-Mobile
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17106-mace-mobile-ai-compute-engine" title="17.10.6 MACE（ Mobile AI Compute Engine）" class="md-nav__link">
    17.10.6 MACE（ Mobile AI Compute Engine）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17107-feathercnn" title="17.10.7 FeatherCNN" class="md-nav__link">
    17.10.7 FeatherCNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17108-tensorflow-lite" title="17.10.8 TensorFlow Lite" class="md-nav__link">
    17.10.8 TensorFlow Lite
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17109-pocketflow" title="17.10.9 PocketFlow" class="md-nav__link">
    17.10.9 PocketFlow
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#171010" title="17.10.10 其他几款支持移动端深度学习的开源框架" class="md-nav__link">
    17.10.10 其他几款支持移动端深度学习的开源框架
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#171011-mdlncnn-tflite" title="17.10.11 MDL、NCNN和 TFLite比较" class="md-nav__link">
    17.10.11 MDL、NCNN和 TFLite比较
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1711" title="17.11 移动端开源框架部署" class="md-nav__link">
    17.11 移动端开源框架部署
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1781-ncnn" title="17.8.1 以NCNN为例" class="md-nav__link">
    17.8.1 以NCNN为例
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1782-qnnpack" title="17.8.2 以QNNPACK为例" class="md-nav__link">
    17.8.2 以QNNPACK为例
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1784-androidmace" title="17.8.4 在Android手机上使用MACE实现图像分类" class="md-nav__link">
    17.8.4 在Android手机上使用MACE实现图像分类
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1783-androidpaddlemobile" title="17.8.3 在Android手机上使用PaddleMobile实现图像分类" class="md-nav__link">
    17.8.3 在Android手机上使用PaddleMobile实现图像分类
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#179_1" title="17.9 移动端开源框架部署疑难" class="md-nav__link">
    17.9 移动端开源框架部署疑难
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="参考文献" class="md-nav__link">
    参考文献
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch18_后端架构选型、离线及实时计算/第十八章_后端架构选型、离线及实时计算/" title="第十八章_后端架构选型、离线及实时计算" class="md-nav__link">
      第十八章_后端架构选型、离线及实时计算
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../ch18_后端架构选型及应用场景/第十八章_后端架构选型及应用场景/" title="第十八章_后端架构选型及应用场景" class="md-nav__link">
      第十八章_后端架构选型及应用场景
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      统计学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        统计学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH01/" title="统计学习及监督学习概论" class="md-nav__link">
      统计学习及监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH02/" title="感知机" class="md-nav__link">
      感知机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH03/" title="K近邻法" class="md-nav__link">
      K近邻法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH04/" title="朴素贝叶斯法" class="md-nav__link">
      朴素贝叶斯法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH05/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH06/" title="逻辑斯蒂回归与最大熵模型" class="md-nav__link">
      逻辑斯蒂回归与最大熵模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH07/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH08/" title="提升方法" class="md-nav__link">
      提升方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH09/" title="EM算法及其推广" class="md-nav__link">
      EM算法及其推广
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH10/" title="隐马尔可夫模型" class="md-nav__link">
      隐马尔可夫模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH11/" title="条件随机场" class="md-nav__link">
      条件随机场
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH12/" title="监督学习方法总结" class="md-nav__link">
      监督学习方法总结
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH13/" title="无监督学习概论" class="md-nav__link">
      无监督学习概论
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH14/" title="聚类方法" class="md-nav__link">
      聚类方法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../lihang/CH22/" title="无监督学习方法总结" class="md-nav__link">
      无监督学习方法总结
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Sklearn与TensorFlow
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Sklearn与TensorFlow
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/README.old/" title="简介" class="md-nav__link">
      简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/0.前言/" title="前言" class="md-nav__link">
      前言
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/1.机器学习概览/" title="机器学习概览" class="md-nav__link">
      机器学习概览
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/2.一个完整的机器学习项目/" title="一个完整的机器学习项目" class="md-nav__link">
      一个完整的机器学习项目
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/3.分类/" title="分类" class="md-nav__link">
      分类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/4.训练模型/" title="训练模型" class="md-nav__link">
      训练模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/5.支持向量机/" title="支持向量机" class="md-nav__link">
      支持向量机
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/6.决策树/" title="决策树" class="md-nav__link">
      决策树
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/7.集成学习和随机森林/" title="集成学习和随机森林" class="md-nav__link">
      集成学习和随机森林
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/9.启动并运行_TensorFlow/" title="启动并运行_TensorFlow" class="md-nav__link">
      启动并运行_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/10.人工神经网络介绍/" title="人工神经网络介绍" class="md-nav__link">
      人工神经网络介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/11.训练深层神经网络/" title="训练深层神经网络" class="md-nav__link">
      训练深层神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/12.设备和服务器上的分布式_TensorFlow/" title="设备和服务器上的分布式_TensorFlow" class="md-nav__link">
      设备和服务器上的分布式_TensorFlow
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/13.卷积神经网络/" title="卷积神经网络" class="md-nav__link">
      卷积神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/14.循环神经网络/" title="循环神经网络" class="md-nav__link">
      循环神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/15.自编码器/" title="自编码器" class="md-nav__link">
      自编码器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/16.强化学习/" title="强化学习" class="md-nav__link">
      强化学习
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/B.机器学习项目清单/" title="机器学习项目清单" class="md-nav__link">
      机器学习项目清单
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/C.SVM_对偶问题/" title="SVM_对偶问题" class="md-nav__link">
      SVM_对偶问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../hands-on-ml-zh/docs/D.自动微分/" title="自动微分" class="md-nav__link">
      自动微分
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../about/" title="关于" class="md-nav__link">
      关于
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#171" title="17.1 模型压缩理解" class="md-nav__link">
    17.1 模型压缩理解
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#172" title="17.2 为什么需要模型压缩和加速？" class="md-nav__link">
    17.2 为什么需要模型压缩和加速？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#173" title="17.3 模型压缩的必要性及可行性" class="md-nav__link">
    17.3 模型压缩的必要性及可行性
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#174" title="17.4 目前有哪些深度学习模型压缩方法？" class="md-nav__link">
    17.4 目前有哪些深度学习模型压缩方法？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1741" title="17.4.1 前端压缩和后端压缩对比" class="md-nav__link">
    17.4.1 前端压缩和后端压缩对比
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1742" title="17.4.2 网络剪枝" class="md-nav__link">
    17.4.2 网络剪枝
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1743" title="17.4.3 典型剪枝方法对比" class="md-nav__link">
    17.4.3 典型剪枝方法对比
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1744" title="17.4.4 网络蒸馏" class="md-nav__link">
    17.4.4 网络蒸馏
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1745" title="17.4.5 前端压缩" class="md-nav__link">
    17.4.5 前端压缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1746" title="17.4.6 后端压缩" class="md-nav__link">
    17.4.6 后端压缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1746_1" title="17.4.6 低秩分解" class="md-nav__link">
    17.4.6 低秩分解
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1747" title="17.4.7 总体压缩效果评价指标有哪些？" class="md-nav__link">
    17.4.7 总体压缩效果评价指标有哪些？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1748" title="17.4.8 几种轻量化网络结构对比" class="md-nav__link">
    17.4.8 几种轻量化网络结构对比
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1749" title="17.4.9 网络压缩未来研究方向有哪些？" class="md-nav__link">
    17.4.9 网络压缩未来研究方向有哪些？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#175" title="17.5 目前有哪些深度学习模型优化加速方法？" class="md-nav__link">
    17.5 目前有哪些深度学习模型优化加速方法？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1751" title="17.5.1 模型优化加速方法" class="md-nav__link">
    17.5.1 模型优化加速方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1752-tensorrt" title="17.5.2 TensorRT加速原理" class="md-nav__link">
    17.5.2 TensorRT加速原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1753-tensorrt" title="17.5.3 TensorRT如何优化重构模型？" class="md-nav__link">
    17.5.3 TensorRT如何优化重构模型？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1754-tensorrt" title="17.5.4 TensorRT加速效果如何？" class="md-nav__link">
    17.5.4 TensorRT加速效果如何？
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#176-4" title="17.6 影响神经网络速度的4个因素（再稍微详细一点）" class="md-nav__link">
    17.6 影响神经网络速度的4个因素（再稍微详细一点）
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#177" title="17.7 压缩和加速方法如何选择？" class="md-nav__link">
    17.7 压缩和加速方法如何选择？
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#178" title="17.8 改变网络结构设计为什么会实现模型压缩、加速？" class="md-nav__link">
    17.8 改变网络结构设计为什么会实现模型压缩、加速？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1781-group-convolution" title="17.8.1 Group convolution" class="md-nav__link">
    17.8.1 Group convolution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1782-depthwise-separable-convolution" title="17.8.2. Depthwise separable convolution" class="md-nav__link">
    17.8.2. Depthwise separable convolution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1783-channelmac" title="17.8.3 输入输出的channel相同时，MAC最小" class="md-nav__link">
    17.8.3 输入输出的channel相同时，MAC最小
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1784" title="17.8.4 减少组卷积的数量" class="md-nav__link">
    17.8.4 减少组卷积的数量
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1785" title="17.8.5 减少网络碎片化程度(分支数量)" class="md-nav__link">
    17.8.5 减少网络碎片化程度(分支数量)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1787" title="17.8.7 减少元素级操作" class="md-nav__link">
    17.8.7 减少元素级操作
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#179" title="17.9 常用的轻量级网络有哪些？" class="md-nav__link">
    17.9 常用的轻量级网络有哪些？
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1791-sequeezenet" title="17.9.1 SequeezeNet" class="md-nav__link">
    17.9.1 SequeezeNet
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" title="1.1 设计思想" class="md-nav__link">
    1.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" title="1.2 网络架构" class="md-nav__link">
    1.2 网络架构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" title="1.3实验结果" class="md-nav__link">
    1.3实验结果
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1792-mobilenet" title="17.9.2 MobileNet" class="md-nav__link">
    17.9.2 MobileNet
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" title="2.1 设计思想" class="md-nav__link">
    2.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" title="2.2 网络架构" class="md-nav__link">
    2.2 网络架构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" title="2.3 实验结果" class="md-nav__link">
    2.3 实验结果
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1793-mobilenet-v2" title="17.9.3 MobileNet-v2" class="md-nav__link">
    17.9.3 MobileNet-v2
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" title="3.1 设计思想" class="md-nav__link">
    3.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" title="3.2 网络架构" class="md-nav__link">
    3.2 网络架构
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1794-xception" title="17.9.4 Xception" class="md-nav__link">
    17.9.4 Xception
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" title="4.1设计思想" class="md-nav__link">
    4.1设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" title="4.2网络架构" class="md-nav__link">
    4.2网络架构
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1795-shufflenet-v1" title="17.9.5 ShuffleNet-v1" class="md-nav__link">
    17.9.5 ShuffleNet-v1
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51" title="5.1 设计思想" class="md-nav__link">
    5.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52" title="5.2 网络架构" class="md-nav__link">
    5.2 网络架构
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1796-shufflenet-v2" title="17.9.6 ShuffleNet-v2" class="md-nav__link">
    17.9.6 ShuffleNet-v2
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61" title="6.1 设计思想" class="md-nav__link">
    6.1 设计思想
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62" title="6.2 网络结构" class="md-nav__link">
    6.2 网络结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64-shufflenet-v2" title="6.4  ShuffleNet-v2具有高精度的原因" class="md-nav__link">
    6.4  ShuffleNet-v2具有高精度的原因
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1710" title="17.10 现有移动端开源框架及其特点" class="md-nav__link">
    17.10 现有移动端开源框架及其特点
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#17101-ncnn" title="17.10.1 NCNN" class="md-nav__link">
    17.10.1 NCNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17102-qnnpack" title="17.10.2 QNNPACK" class="md-nav__link">
    17.10.2 QNNPACK
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17103-prestissimo" title="17.10.3 Prestissimo" class="md-nav__link">
    17.10.3 Prestissimo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17104-mdlmobile-deep-learning" title="17.10.4 MDL（mobile-deep-learning）" class="md-nav__link">
    17.10.4 MDL（mobile-deep-learning）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17105-paddle-mobile" title="17.10.5 Paddle-Mobile" class="md-nav__link">
    17.10.5 Paddle-Mobile
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17106-mace-mobile-ai-compute-engine" title="17.10.6 MACE（ Mobile AI Compute Engine）" class="md-nav__link">
    17.10.6 MACE（ Mobile AI Compute Engine）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17107-feathercnn" title="17.10.7 FeatherCNN" class="md-nav__link">
    17.10.7 FeatherCNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17108-tensorflow-lite" title="17.10.8 TensorFlow Lite" class="md-nav__link">
    17.10.8 TensorFlow Lite
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17109-pocketflow" title="17.10.9 PocketFlow" class="md-nav__link">
    17.10.9 PocketFlow
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#171010" title="17.10.10 其他几款支持移动端深度学习的开源框架" class="md-nav__link">
    17.10.10 其他几款支持移动端深度学习的开源框架
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#171011-mdlncnn-tflite" title="17.10.11 MDL、NCNN和 TFLite比较" class="md-nav__link">
    17.10.11 MDL、NCNN和 TFLite比较
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1711" title="17.11 移动端开源框架部署" class="md-nav__link">
    17.11 移动端开源框架部署
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1781-ncnn" title="17.8.1 以NCNN为例" class="md-nav__link">
    17.8.1 以NCNN为例
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1782-qnnpack" title="17.8.2 以QNNPACK为例" class="md-nav__link">
    17.8.2 以QNNPACK为例
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1784-androidmace" title="17.8.4 在Android手机上使用MACE实现图像分类" class="md-nav__link">
    17.8.4 在Android手机上使用MACE实现图像分类
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1783-androidpaddlemobile" title="17.8.3 在Android手机上使用PaddleMobile实现图像分类" class="md-nav__link">
    17.8.3 在Android手机上使用PaddleMobile实现图像分类
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#179_1" title="17.9 移动端开源框架部署疑难" class="md-nav__link">
    17.9 移动端开源框架部署疑难
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="参考文献" class="md-nav__link">
    参考文献
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">
            评论
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/myourdream/aiwiki/blob/master/docs/qa500/ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <div class="toc">
<ul>
<li><a href="#_1">第十七章 模型压缩及移动端部署</a><ul>
<li><a href="#171">17.1 模型压缩理解</a></li>
<li><a href="#172">17.2 为什么需要模型压缩和加速？</a></li>
<li><a href="#173">17.3 模型压缩的必要性及可行性</a></li>
<li><a href="#174">17.4 目前有哪些深度学习模型压缩方法？</a><ul>
<li><a href="#1741">17.4.1 前端压缩和后端压缩对比</a></li>
<li><a href="#1742">17.4.2 网络剪枝</a></li>
<li><a href="#1743">17.4.3 典型剪枝方法对比</a></li>
<li><a href="#1744">17.4.4 网络蒸馏</a></li>
<li><a href="#1745">17.4.5 前端压缩</a></li>
<li><a href="#1746">17.4.6 后端压缩</a></li>
<li><a href="#1746_1">17.4.6 低秩分解</a></li>
<li><a href="#1747">17.4.7 总体压缩效果评价指标有哪些？</a></li>
<li><a href="#1748">17.4.8 几种轻量化网络结构对比</a></li>
<li><a href="#1749">17.4.9 网络压缩未来研究方向有哪些？</a></li>
</ul>
</li>
<li><a href="#175">17.5 目前有哪些深度学习模型优化加速方法？</a><ul>
<li><a href="#1751">17.5.1 模型优化加速方法</a></li>
<li><a href="#1752-tensorrt">17.5.2 TensorRT加速原理</a></li>
<li><a href="#1753-tensorrt">17.5.3 TensorRT如何优化重构模型？</a></li>
<li><a href="#1754-tensorrt">17.5.4 TensorRT加速效果如何？</a></li>
</ul>
</li>
<li><a href="#176-4">17.6 影响神经网络速度的4个因素（再稍微详细一点）</a></li>
<li><a href="#177">17.7 压缩和加速方法如何选择？</a></li>
<li><a href="#178">17.8 改变网络结构设计为什么会实现模型压缩、加速？</a><ul>
<li><a href="#1781-group-convolution">17.8.1 Group convolution</a></li>
<li><a href="#1782-depthwise-separable-convolution">17.8.2. Depthwise separable convolution</a></li>
<li><a href="#1783-channelmac">17.8.3 输入输出的channel相同时，MAC最小</a></li>
<li><a href="#1784">17.8.4 减少组卷积的数量</a></li>
<li><a href="#1785">17.8.5 减少网络碎片化程度(分支数量)</a></li>
<li><a href="#1787">17.8.7 减少元素级操作</a></li>
</ul>
</li>
<li><a href="#179">17.9 常用的轻量级网络有哪些？</a><ul>
<li><a href="#1791-sequeezenet">17.9.1 SequeezeNet</a><ul>
<li><a href="#11">1.1 设计思想</a></li>
<li><a href="#12">1.2 网络架构</a></li>
<li><a href="#13">1.3实验结果</a></li>
</ul>
</li>
<li><a href="#1792-mobilenet">17.9.2 MobileNet</a><ul>
<li><a href="#21">2.1 设计思想</a></li>
<li><a href="#22">2.2 网络架构</a></li>
<li><a href="#23">2.3 实验结果</a></li>
</ul>
</li>
<li><a href="#1793-mobilenet-v2">17.9.3 MobileNet-v2</a><ul>
<li><a href="#31">3.1 设计思想</a></li>
<li><a href="#32">3.2 网络架构</a></li>
</ul>
</li>
<li><a href="#1794-xception">17.9.4 Xception</a><ul>
<li><a href="#41">4.1设计思想</a></li>
<li><a href="#42">4.2网络架构</a></li>
</ul>
</li>
<li><a href="#1795-shufflenet-v1">17.9.5 ShuffleNet-v1</a><ul>
<li><a href="#51">5.1 设计思想</a></li>
<li><a href="#52">5.2 网络架构</a></li>
</ul>
</li>
<li><a href="#1796-shufflenet-v2">17.9.6 ShuffleNet-v2</a><ul>
<li><a href="#61">6.1 设计思想</a></li>
<li><a href="#62">6.2 网络结构</a></li>
<li><a href="#64-shufflenet-v2">6.4  ShuffleNet-v2具有高精度的原因</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#1710">17.10 现有移动端开源框架及其特点</a><ul>
<li><a href="#17101-ncnn">17.10.1 NCNN</a></li>
<li><a href="#17102-qnnpack">17.10.2 QNNPACK</a></li>
<li><a href="#17103-prestissimo">17.10.3 Prestissimo</a></li>
<li><a href="#17104-mdlmobile-deep-learning">17.10.4 MDL（mobile-deep-learning）</a></li>
<li><a href="#17105-paddle-mobile">17.10.5 Paddle-Mobile</a></li>
<li><a href="#17106-mace-mobile-ai-compute-engine">17.10.6 MACE（ Mobile AI Compute Engine）</a></li>
<li><a href="#17107-feathercnn">17.10.7 FeatherCNN</a></li>
<li><a href="#17108-tensorflow-lite">17.10.8 TensorFlow Lite</a></li>
<li><a href="#17109-pocketflow">17.10.9 PocketFlow</a></li>
<li><a href="#171010">17.10.10 其他几款支持移动端深度学习的开源框架</a></li>
<li><a href="#171011-mdlncnn-tflite">17.10.11 MDL、NCNN和 TFLite比较</a></li>
</ul>
</li>
<li><a href="#1711">17.11 移动端开源框架部署</a><ul>
<li><a href="#1781-ncnn">17.8.1 以NCNN为例</a></li>
<li><a href="#1782-qnnpack">17.8.2 以QNNPACK为例</a></li>
<li><a href="#1784-androidmace">17.8.4 在Android手机上使用MACE实现图像分类</a></li>
<li><a href="#1783-androidpaddlemobile">17.8.3 在Android手机上使用PaddleMobile实现图像分类</a></li>
</ul>
</li>
<li><a href="#179_1">17.9 移动端开源框架部署疑难</a></li>
<li><a href="#_2">参考文献</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">第十七章 模型压缩及移动端部署<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<p>​   深度神经网络在人工智能的应用中，包括语音识别、计算机视觉、自然语言处理等各方面，在取得巨大成功的同时，这些深度神经网络需要巨大的计算开销和内存开销，严重阻碍了资源受限下的使用。本章总结了模型压缩、加速一般原理和方法，以及在移动端如何部署。</p>
<h2 id="171">17.1 模型压缩理解<a class="headerlink" href="#171" title="Permanent link">&para;</a></h2>
<p>​   模型压缩是指利用数据集对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署再受限的硬件环境中。</p>
<h2 id="172">17.2 为什么需要模型压缩和加速？<a class="headerlink" href="#172" title="Permanent link">&para;</a></h2>
<p>（1）随着AI技术的飞速发展，越来越多的公司希望在自己的移动端产品中注入AI能力。</p>
<p>（2）对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要。  </p>
<p>（3）模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。</p>
<p>（4）复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。</p>
<p>（5）智能设备的流行提供了内存、CPU、能耗和宽带等资源，使得深度学习模型部署在智能移动设备上变得可行。 <br />
（6）高效的深度学习方法可以有效的帮助嵌入式设备、分布式系统完成复杂工作，在移动端部署深度学习有很重要的意义。   </p>
<h2 id="173">17.3 模型压缩的必要性及可行性<a class="headerlink" href="#173" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>必要性</th>
<th>首先是资源受限，其次在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。</th>
</tr>
</thead>
<tbody>
<tr>
<td>可行性</td>
<td>模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。论文<Predicting parameters in deep learning>提出，很多的深度神经网络仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。</td>
</tr>
<tr>
<td>最终目的</td>
<td>最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测</td>
</tr>
</tbody>
</table>
<h2 id="174">17.4 目前有哪些深度学习模型压缩方法？<a class="headerlink" href="#174" title="Permanent link">&para;</a></h2>
<p>​   目前深度学习模型压缩方法主要分为更精细化模型设计、模型裁剪、核的稀疏化、量化、低秩分解、迁移学习等方法，而这些方法又可分为前端压缩和后端压缩。</p>
<h3 id="1741">17.4.1 前端压缩和后端压缩对比<a class="headerlink" href="#1741" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th align="center">对比项目</th>
<th align="center">前端压缩</th>
<th align="center">后端压缩</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">含义</td>
<td align="center">不会改变原始网络结构的压缩技术</td>
<td align="center">会大程度上改变原始网络结构的压缩技术</td>
</tr>
<tr>
<td align="center">主要方法</td>
<td align="center">知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝</td>
<td align="center">低秩近似、未加限制的剪枝、参数量化、二值网络</td>
</tr>
<tr>
<td align="center">实现难度</td>
<td align="center">较简单</td>
<td align="center">较难</td>
</tr>
<tr>
<td align="center">是否可逆</td>
<td align="center">可逆</td>
<td align="center">不可逆</td>
</tr>
<tr>
<td align="center">成熟应用</td>
<td align="center">剪枝</td>
<td align="center">低秩近似、参数量化</td>
</tr>
<tr>
<td align="center">待发展应用</td>
<td align="center">知识蒸馏</td>
<td align="center">二值网络</td>
</tr>
</tbody>
</table>
<h3 id="1742">17.4.2 网络剪枝<a class="headerlink" href="#1742" title="Permanent link">&para;</a></h3>
<p>深度学习模型因其<strong>稀疏性</strong>，可以被裁剪为结构精简的网络模型，具体包括结构性剪枝与非结构性剪枝。</p>
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>非结构化剪枝</td>
<td>通常是连接级、细粒度的剪枝方法，精度相对较高，但依赖于特定算法库或硬件平台的支持</td>
<td>Deep Compression [5], Sparse-Winograd [6] 算法等；</td>
</tr>
<tr>
<td>结构化剪枝</td>
<td>是filter级或layer级、粗粒度的剪枝方法，精度相对较低，但剪枝策略更为有效，不需要特定算法库或硬件平台的支持，能够直接在成熟深度学习框架上运行。</td>
<td>如局部方式的、通过layer by layer方式的、最小化输出FM重建误差的Channel Pruning [7], ThiNet [8], Discrimination-aware Channel Pruning [9]；全局方式的、通过训练期间对BN层Gamma系数施加L1正则约束的Network Slimming [10]；全局方式的、按Taylor准则对Filter作重要性排序的Neuron Pruning [11]；全局方式的、可动态重新更新pruned filters参数的剪枝方法 [12];<br /><a href="https://blog.csdn.net/baidu_31437863/article/details/84474847">https://blog.csdn.net/baidu_31437863/article/details/84474847</a></td>
</tr>
</tbody>
</table>
<p>如果按剪枝粒度分，从粗到细，可分为中间隐含层剪枝、通道剪枝、卷积核剪枝、核内剪枝、单个权重剪枝。下面按照剪枝粒度的分类从粗（左）到细（右）。</p>
<p><img alt="" src="../img/ch17/剪枝粒度分类.png" /></p>
<p>（a）层间剪枝   （b）特征图剪枝    （c）k*k核剪枝   （d）核内剪枝</p>
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>单个权重粒度</td>
<td>早期 Le Cun[16]提出的 OBD(optimal brain damage)将网络中的任意权重参数都看作单个参数,能够有效地提高预测准确率,却不能减小运行时间;同时,剪枝代价过高,只适用于小网络</td>
</tr>
<tr>
<td>核内权重粒度</td>
<td>网络中的任意权重被看作是单个参数并进行随机非结构化剪枝,该粒度的剪枝导致网络连接不规整,需要通过稀疏表达来减少内存占用,进而导致在前向传播预测时,需要大量的条件判断和额外空间来标明零或非零参数的位置,因此不适用于并行计算</td>
</tr>
<tr>
<td>卷积核粒度与通道粒度</td>
<td>卷积核粒度与通道粒度属于粗粒度剪枝,不依赖任何稀疏卷积计算库及专用硬件;同时,能够在获得高压缩率的同时大量减小测试阶段的计算时间.由</td>
</tr>
</tbody>
</table>
<p>从剪枝目标上分类，可分为减少参数/网络复杂度、减少过拟合/增加泛化能力/提高准确率、减小部署运行时间/提高网络效率及减小训练时间等。</p>
<h3 id="1743">17.4.3 典型剪枝方法对比<a class="headerlink" href="#1743" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th align="center">剪枝方法</th>
<th align="center">修剪对象</th>
<th align="center">修剪方式</th>
<th align="center">效果</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Deep Compression</td>
<td align="center">权重</td>
<td align="center">随机修剪</td>
<td align="center">50倍压缩</td>
</tr>
<tr>
<td align="center">Structured Pruning</td>
<td align="center">权重</td>
<td align="center">组稀疏+排他性稀疏</td>
<td align="center">性能提升</td>
</tr>
<tr>
<td align="center">Network Slimming</td>
<td align="center">特征图通道</td>
<td align="center">根据尺度因子修剪</td>
<td align="center">节省计算资源</td>
</tr>
<tr>
<td align="center">mProp</td>
<td align="center">梯度</td>
<td align="center">修剪幅值小的梯度</td>
<td align="center">加速</td>
</tr>
</tbody>
</table>
<h3 id="1744">17.4.4 网络蒸馏<a class="headerlink" href="#1744" title="Permanent link">&para;</a></h3>
<p>​   网络精馏是指利用大量未标记的迁移数据(transfer data),让小模型去拟合大模型,从而让小模型学到与大模型相似的函数映射.网络精馏可以看成在同一个域上迁移学习[34]的一种特例,目的是获得一个比原模型更为精简的网络,整体的框架图如图 4所示. </p>
<p><img alt="" src="../img/ch17/网络蒸馏.png" /></p>
<h3 id="1745">17.4.5 前端压缩<a class="headerlink" href="#1745" title="Permanent link">&para;</a></h3>
<p>（1）知识蒸馏</p>
<p>​   一个复杂模型可由多个简单模型或者强约束条件训练得到。复杂模型特点是性能好，但其参数量大，计算效率低。小模型特点是计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中,使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。 
（2）紧凑的模型结构设计 <br />
​   紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个3x3的卷积替换一个5x5的卷积、使用深度可分离卷积等等方式降低计算参数量。  目前很多网络基于模块化设计思想，在深度和宽度两个维度上都很大，导致参数冗余。因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。<br />
（3）滤波器层面的剪枝 <br />
​   滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。  具体操作是在训练时使用稀疏约束（加入权重的稀疏正则项，引导模型的大部分权重趋向于0）。完成训练后，剪去滤波器上的这些 0 。</p>
<p>​   优点是简单，缺点是剪得不干净，非结构化剪枝会增加内存访问成本。</p>
<h3 id="1746">17.4.6 后端压缩<a class="headerlink" href="#1746" title="Permanent link">&para;</a></h3>
<p>（1）低秩近似<br />
​   在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。  </p>
<table>
<thead>
<tr>
<th align="left">事项</th>
<th align="left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">优点</td>
<td align="left">可以降低存储和计算消耗；<br />一般可以压缩2-3倍；精度几乎没有损失；</td>
</tr>
<tr>
<td align="left">缺点</td>
<td align="left">模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能 ；   <br />超参数的数量随着网络层数的增加呈线性变化趋势，例如中间层的特征通道数等等。 <br />随着模型复杂度的提升，搜索空间急剧增大。</td>
</tr>
</tbody>
</table>
<p>（2）未加限制的剪枝    </p>
<p>​   完成训练后，不加限制地剪去那些冗余参数。</p>
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>保持模型性能不损失的情况下，减少参数量9-11倍； <br />剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力；</td>
</tr>
<tr>
<td>缺点</td>
<td>极度依赖专门的运行库和特殊的运行平台，不具有通用性；<br /> 压缩率过大时，破坏性能；</td>
</tr>
</tbody>
</table>
<p>（3）参数量化    </p>
<p>​   神经网络的参数类型一般是32位浮点型，使用较小的精度代替32位所表示的精度。或者是将多个权重映射到同一数值，权重共享。<strong>量化其实是一种权值共享的策略</strong>。量化后的权值张量是一个高度稀疏的有很多共享权值的矩阵，对非零参数，我们还可以进行定点压缩，以获得更高的压缩率。 </p>
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>模型性能损失很小，大小减少8-16倍；</td>
</tr>
<tr>
<td>缺点</td>
<td>压缩率大时，性能显著下降； <br />依赖专门的运行库，通用性较差；</td>
</tr>
<tr>
<td>举例</td>
<td>二值化网络：XNORnet [13], ABCnet with Multiple Binary Bases [14], <br />Bin-net with High-Order Residual Quantization [15], Bi-Real Net [16]；<br/>三值化网络：Ternary weight networks [17], Trained Ternary Quantization [18]；</td>
</tr>
<tr>
<td>W1-A8 或 W2-A8量化： Learning Symmetric Quantization [19]；</td>
<td></td>
</tr>
<tr>
<td>INT8量化：TensorFlow-lite [20], TensorRT [21]；</td>
<td></td>
</tr>
<tr>
<td>其他（非线性）：Intel INQ [22], log-net, CNNPack [23] 等；</td>
<td></td>
</tr>
<tr>
<td>原文：<a href="https://blog.csdn.net/baidu_31437863/article/details/84474847">https://blog.csdn.net/baidu_31437863/article/details/84474847</a></td>
<td></td>
</tr>
<tr>
<td>总结</td>
<td>最为典型就是二值网络、XNOR网络等。其主要原理就是采用1bit对网络的输入、权重、响应进行编码。减少模型大小的同时，原始网络的卷积操作可以被bit-wise运算代替，极大提升了模型的速度。但是，如果原始网络结果不够复杂（模型描述能力），由于二值网络会较大程度降低模型的表达能力。因此现阶段有相关的论文开始研究n-bit编码方式成为n值网络或者多值网络或者变bit、组合bit量化来克服二值网络表达能力不足的缺点。</td>
</tr>
</tbody>
</table>
<p>（4）二值网络</p>
<p>​   相对量化更为极致，对于32bit浮点型数用1bit二进制数-1或者1表示，可大大减小模型尺寸。  </p>
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>网络体积小，运算速度快，有时可避免部分网络的overfitting</td>
</tr>
<tr>
<td>缺点</td>
<td>二值神经网络损失的信息相对于浮点精度是非常大；<br />粗糙的二值化近似导致训练时模型收敛速度非常慢</td>
</tr>
</tbody>
</table>
<p>（5）三值网络</p>
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>相对于二值神经网络，三值神经网络(Ternary Weight Networks)在同样的模型结构下可以达到成百上千倍的表达能力提升;并且，在计算时间复杂度上，三元网络和二元网络的计算复杂度是一样的。<br />例如，对于ResNet-18层网络中最常出现的卷积核(3x3大小)，二值神经网络模型最多可以表达2的3x3次方(=512)种结构，而三元神经网络则可以表达3的3x3次方(=19683)种卷积核结构。在表达能力上，三元神经网络相对要高19683/512 = 38倍。因此，三元神经网络模型能够在保证计算复杂度很低的情况下大幅的提高网络的表达能力，进而可以在精度上相对于二值神经网络有质的飞跃。另外，由于对中间信息的保存更多，三元神经网络可以极大的加快网络训练时的收敛速度，从而更快、更稳定的达到最优的结果。</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="1746_1">17.4.6 低秩分解<a class="headerlink" href="#1746_1" title="Permanent link">&para;</a></h3>
<p>基于低秩分解的深度神经网络压缩与加速的核心思想是利用矩阵或张量分解技术估计并分解深度模型中的原始卷积核．卷积计算是整个卷积神经网络中计算复杂 度 最 高 的 计 算 操 作，通 过 分 解４Ｄ 卷积核张量，可以有效地减少模型内部的冗余性．此外对于２Ｄ的全 连 接 层 矩 阵 参 数，同样可以利用低秩分解技术进行处理．但由于卷积层与全连接层的分解方式不同，本文分别从卷积层和全连接层２个不同角度回顾与分析低秩分解技术在深度神经网络中的应用.</p>
<p>在２０１３年，Ｄｅｎｉｌ等人［５７］从理论上利用低秩分解的技术并分析了深度神经网络存在大量的冗余信
息，开创了基于低秩分解的深度网络模型压缩与加速的新思路．如图７所示，展示了主流的张量分解后卷积 计 算．</p>
<p><img alt="" src="../img/ch17/低秩分解模型压缩加速.jpg" /></p>
<p>(出自《深度神经网络压缩与加速综述》)</p>
<h3 id="1747">17.4.7 总体压缩效果评价指标有哪些？<a class="headerlink" href="#1747" title="Permanent link">&para;</a></h3>
<p>​   网络压缩评价指标包括运行效率、参数压缩率、准确率.与基准模型比较衡量性能提升时,可以使用提升倍数(speedup)或提升比例(ratio)。</p>
<table>
<thead>
<tr>
<th>评价指标</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率</td>
<td>目前,大部分研究工作均会测量 Top-1 准确率,只有在 ImageNet 这类大型数据集上才会只用 Top-5 准确率.为方便比较</td>
</tr>
<tr>
<td>参数压缩率</td>
<td>统计网络中所有可训练的参数,根据机器浮点精度转换为字节(byte)量纲,通常保留两位有效数字以作近似估计.</td>
</tr>
<tr>
<td>运行效率</td>
<td>可以从网络所含浮点运算次数(FLOP)、网络所含乘法运算次数(MULTS)或随机实验测得的网络平均前向传播所需时间这 3 个角度来评价</td>
</tr>
</tbody>
</table>
<h3 id="1748">17.4.8 几种轻量化网络结构对比<a class="headerlink" href="#1748" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th align="center">网络结构</th>
<th align="center">TOP1 准确率/%</th>
<th align="center">参数量/M</th>
<th align="center">CPU运行时间/ms</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">MobileNet V1</td>
<td align="center">70.6</td>
<td align="center">4.2</td>
<td align="center">123</td>
</tr>
<tr>
<td align="center">ShuffleNet(1.5)</td>
<td align="center">69.0</td>
<td align="center">2.9</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">ShuffleNet(x2)</td>
<td align="center">70.9</td>
<td align="center">4.4</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">MobileNet V2</td>
<td align="center">71.7</td>
<td align="center">3.4</td>
<td align="center">80</td>
</tr>
<tr>
<td align="center">MobileNet V2(1.4)</td>
<td align="center">74.7</td>
<td align="center">6.9</td>
<td align="center">149</td>
</tr>
</tbody>
</table>
<h3 id="1749">17.4.9 网络压缩未来研究方向有哪些？<a class="headerlink" href="#1749" title="Permanent link">&para;</a></h3>
<p>网络剪枝、网络精馏和网络分解都能在一定程度上实现网络压缩的目的.回归到深度网络压缩的本质目的上,即提取网络中的有用信息,以下是一些值得研究和探寻的方向. 
(1) 权重参数对结果的影响度量.深度网络的最终结果是由全部的权重参数共同作用形成的,目前,关于单个卷积核/卷积核权重的重要性的度量仍然是比较简单的方式,尽管文献[14]中给出了更为细节的分析,但是由于计算难度大,并不实用.因此,如何通过更有效的方式来近似度量单个参数对模型的影响,具有重要意义. 
(2) 学生网络结构的构造.学生网络的结构构造目前仍然是由人工指定的,然而,不同的学生网络结构的训练难度不同,最终能够达到的效果也有差异.因此,如何根据教师网络结构设计合理的网络结构在精简模型的条件下获取较高的模型性能,是未来的一个研究重点. 
(3) 参数重建的硬件架构支持.通过分解网络可以无损地获取压缩模型,在一些对性能要求高的场景中是非常重要的.然而,参数的重建步骤会拖累预测阶段的时间开销,如何通过硬件的支持加速这一重建过程,将是未来的一个研究方向. 
(4) 任务或使用场景层面的压缩.大型网络通常是在量级较大的数据集上训练完成的,比如,在 ImageNet上训练的模型具备对 1 000 类物体的分类,但在一些具体场景的应用中,可能仅需要一个能识别其中几类的小型模型.因此,如何从一个全功能的网络压缩得到部分功能的子网络,能够适应很多实际应用场景的需求. 
(5) 网络压缩效用的评价.目前,对各类深度网络压缩算法的评价是比较零碎的,侧重于和被压缩的大型网络在参数量和运行时间上的比较.未来的研究可以从提出更加泛化的压缩评价标准出发,一方面平衡运行速度和模型大小在不同应用场景下的影响;另一方面,可以从模型本身的结构性出发,对压缩后的模型进行评价. </p>
<p>（出自《深度网络模型压缩综述》）</p>
<h2 id="175">17.5 目前有哪些深度学习模型优化加速方法？<a class="headerlink" href="#175" title="Permanent link">&para;</a></h2>
<p><a href="https://blog.csdn.net/nature553863/article/details/81083955">https://blog.csdn.net/nature553863/article/details/81083955</a></p>
<h3 id="1751">17.5.1 模型优化加速方法<a class="headerlink" href="#1751" title="Permanent link">&para;</a></h3>
<p>模型优化加速能够提升网络的计算效率，具体包括： <br />
（1）Op-level的快速算法：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等； <br />
（2）Layer-level的快速算法：Sparse-block net [1] 等； <br />
（3）优化工具与库：TensorRT (Nvidia), Tensor Comprehension (Facebook) 和 Distiller (Intel) 等；   </p>
<p>原文：<a href="https://blog.csdn.net/nature553863/article/details/81083955">https://blog.csdn.net/nature553863/article/details/81083955</a>   </p>
<h3 id="1752-tensorrt">17.5.2 TensorRT加速原理<a class="headerlink" href="#1752-tensorrt" title="Permanent link">&para;</a></h3>
<p><a href="https://blog.csdn.net/xh_hit/article/details/79769599">https://blog.csdn.net/xh_hit/article/details/79769599</a></p>
<p>​   在计算资源并不丰富的嵌入式设备上，TensorRT之所以能加速神经网络的的推断主要得益于两点：</p>
<ul>
<li>
<p>首先是TensorRT支持int8和fp16的计算，通过在减少计算量和保持精度之间达到一个理想的trade-off，达到加速推断的目的。</p>
</li>
<li>
<p>更为重要的是TensorRT对于网络结构进行了重构和优化，主要体现在一下几个方面。</p>
</li>
</ul>
<p>(1) TensorRT通过解析网络模型将网络中无用的输出层消除以减小计算。</p>
<p>(2) 对于网络结构的垂直整合，即将目前主流神经网络的Conv、BN、Relu三个层融合为了一个层，例如将图1所示的常见的Inception结构重构为图2所示的网络结构。</p>
<p>(3) 对于网络结构的水平组合，水平组合是指将输入为相同张量和执行相同操作的层融合一起，例如图2向图3的转化。</p>
<p><img alt="" src="../img/ch17/tensorRT1.png" /></p>
<p><img alt="" src="../img/ch17/tensorRT2.png" /></p>
<p><img alt="" src="../img/ch17/tensorRT3.png" /></p>
<p>​   以上3步即是TensorRT对于所部署的深度学习网络的优化和重构，根据其优化和重构策略，第一和第二步适用于所有的网络架构，但是第三步则对于含有Inception结构的神经网络加速效果最为明显。</p>
<p>​   Tips: 想更好地利用TensorRT加速网络推断，可在基础网络中多采用Inception模型结构，充分发挥TensorRT的优势。</p>
<h3 id="1753-tensorrt">17.5.3 TensorRT如何优化重构模型？<a class="headerlink" href="#1753-tensorrt" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>条件</th>
<th>方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>若训练的网络模型包含TensorRT支持的操作</td>
<td>1、对于Caffe与TensorFlow训练的模型，若包含的操作都是TensorRT支持的，则可以直接由TensorRT优化重构</td>
</tr>
<tr>
<td></td>
<td>2、对于MXnet, PyTorch或其他框架训练的模型，若包含的操作都是TensorRT支持的，可以采用TensorRT API重建网络结构，并间接优化重构；</td>
</tr>
<tr>
<td>若训练的网络模型包含TensorRT不支持的操作</td>
<td>1、TensorFlow模型可通过tf.contrib.tensorrt转换，其中不支持的操作会保留为TensorFlow计算节点；</td>
</tr>
<tr>
<td></td>
<td>2、不支持的操作可通过Plugin API实现自定义并添加进TensorRT计算图；</td>
</tr>
<tr>
<td></td>
<td>3、将深度网络划分为两个部分，一部分包含的操作都是TensorRT支持的，可以转换为TensorRT计算图。另一部则采用其他框架实现，如MXnet或PyTorch；</td>
</tr>
</tbody>
</table>
<h3 id="1754-tensorrt">17.5.4 TensorRT加速效果如何？<a class="headerlink" href="#1754-tensorrt" title="Permanent link">&para;</a></h3>
<p>以下是在TitanX (Pascal)平台上，TensorRT对大型分类网络的优化加速效果：</p>
<table>
<thead>
<tr>
<th align="center">Network</th>
<th align="center">Precision</th>
<th align="center">Framework/GPU:TitanXP</th>
<th align="center">Avg.Time(Batch=8,unit:ms)</th>
<th align="center">Top1 Val.Acc.(ImageNet-1k)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Resnet50</td>
<td align="center">fp32</td>
<td align="center">TensorFlow</td>
<td align="center">24.1</td>
<td align="center">0.7374</td>
</tr>
<tr>
<td align="center">Resnet50</td>
<td align="center">fp32</td>
<td align="center">MXnet</td>
<td align="center">15.7</td>
<td align="center">0.7374</td>
</tr>
<tr>
<td align="center">Resnet50</td>
<td align="center">fp32</td>
<td align="center">TRT4.0.1</td>
<td align="center">12.1</td>
<td align="center">0.7374</td>
</tr>
<tr>
<td align="center">Resnet50</td>
<td align="center">int8</td>
<td align="center">TRT4.0.1</td>
<td align="center">6</td>
<td align="center">0.7226</td>
</tr>
<tr>
<td align="center">Resnet101</td>
<td align="center">fp32</td>
<td align="center">TensorFlow</td>
<td align="center">36.7</td>
<td align="center">0.7612</td>
</tr>
<tr>
<td align="center">Resnet101</td>
<td align="center">fp32</td>
<td align="center">MXnet</td>
<td align="center">25.8</td>
<td align="center">0.7612</td>
</tr>
<tr>
<td align="center">Resnet101</td>
<td align="center">fp32</td>
<td align="center">TRT4.0.1</td>
<td align="center">19.3</td>
<td align="center">0.7612</td>
</tr>
<tr>
<td align="center">Resnet101</td>
<td align="center">int8</td>
<td align="center">TRT4.0.1</td>
<td align="center">9</td>
<td align="center">0.7574</td>
</tr>
</tbody>
</table>
<h2 id="176-4">17.6 影响神经网络速度的4个因素（再稍微详细一点）<a class="headerlink" href="#176-4" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p>FLOPs(FLOPs就是网络执行了多少multiply-adds操作)；  </p>
</li>
<li>
<p>MAC(内存访问成本)；   </p>
</li>
<li>
<p>并行度(如果网络并行度高，速度明显提升)；   </p>
</li>
<li>
<p>计算平台(GPU，ARM)   </p>
</li>
</ol>
<h2 id="177">17.7 压缩和加速方法如何选择？<a class="headerlink" href="#177" title="Permanent link">&para;</a></h2>
<p>​   １）对于在线计算内存存储有限的应用场景或设备，可以选择参数共享和参数剪枝方法，特别是二值量化权值和激活、结构化剪枝．其他方法虽然能够有效的压缩模型中的权值参数，但无法减小计算中隐藏的内存大小（如特征图）．
​   ２）如果在应用中用到的紧性模型需要利用预训练模型，那么参数剪枝、参数共享以及低秩分解将成为首要考虑的方法．相反地，若不需要借助预训练模型，则可以考虑紧性滤波设计及知识蒸馏方法．
​   ３）若需要一次性端对端训练得到压缩与加速后模型，可以利用基于紧性滤波设计的深度神经网络压缩与加速方法．
​   ４）一般情况下，参数剪枝，特别是非结构化剪枝，能大大压缩模型大小，且不容易丢失分类精度．对于需要稳定的模型分类的应用，非结构化剪枝成为首要选择．
​   ５）若采用的数据集较小时，可以考虑知识蒸馏方法．对于小样本的数据集，学生网络能够很好地迁移教师模型的知识，提高学生网络的判别性．
​   ６）主流的５个深度神经网络压缩与加速算法相互之间是正交的，可以结合不同技术进行进一步的压缩与加速．如：韩 松 等 人［３０］结合了参数剪枝和参数共享；温伟等人［６４］以及 Ａｌｖａｒｅｚ等人［８５］结合了参数剪枝和低秩分解．此外对于特定的应用场景，如目标检测，可以对卷积层和全连接层使用不同的压缩与加速技术分别处理．</p>
<p>参考《深度神经网络压缩与加速综述》</p>
<h2 id="178">17.8 改变网络结构设计为什么会实现模型压缩、加速？<a class="headerlink" href="#178" title="Permanent link">&para;</a></h2>
<h3 id="1781-group-convolution">17.8.1 Group convolution<a class="headerlink" href="#1781-group-convolution" title="Permanent link">&para;</a></h3>
<p>​   Group convolution最早出现在AlexNet中，是为了解决单卡显存不够，将网络部署到多卡上进行训练而提出。Group convolution可以减少单个卷积1/g的参数量。如何计算的呢？  </p>
<p>​   假设</p>
<ul>
<li>输入特征的的维度为<span><span class="MathJax_Preview">H*W*C_1</span><script type="math/tex">H*W*C_1</script></span>;</li>
<li>卷积核的维度为<span><span class="MathJax_Preview">H_1*W_1*C_1</span><script type="math/tex">H_1*W_1*C_1</script></span>，共<span><span class="MathJax_Preview">C_2</span><script type="math/tex">C_2</script></span>个；</li>
<li>输出特征的维度为<span><span class="MathJax_Preview">H_1*W_1*C_2</span><script type="math/tex">H_1*W_1*C_2</script></span> 。  </li>
</ul>
<p>传统卷积计算方式如下：<br />
<img alt="image" src="../img/ch17/1.png" />
传统卷积运算量为：<br />
$$
A = H*W * h1 * w1 * c1 * c2
$$
Group convolution是将输入特征的维度c1分成g份，每个group对应的channel数为c1/g，特征维度H * W * c1/g；，每个group对应的卷积核的维度也相应发生改变为h1 * w1 * c1/9，共c2/g个；每个group相互独立运算，最后将结果叠加在一起。<br />
Group convolution计算方式如下：<br />
<img alt="image" src="../img/ch17/2.png" />
Group convolution运算量为：<br />
$$
B = H * W * h1 * w1 * c1/g * c2/g * g
$$
Group卷积相对于传统卷积的运算量：<br />
$$
\dfrac{B}{A} = \dfrac{ H * W * h1 * w1 * c1/g * c2/g * g}{H * W * h1 * w1 * c1 * c2} = \dfrac{1}{g}
$$
由此可知：group卷积相对于传统卷积减少了1/g的参数量。</p>
<h3 id="1782-depthwise-separable-convolution">17.8.2. Depthwise separable convolution<a class="headerlink" href="#1782-depthwise-separable-convolution" title="Permanent link">&para;</a></h3>
<p>Depthwise separable convolution是由depthwise conv和pointwise conv构成。<br />
depthwise conv(DW)有效减少参数数量并提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。<br />
pointwise conv(PW)实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。
假设输入特征的的维度为H * W * c1；卷积核的维度为h1 * w1 * c1，共c2个；输出特征的维度为 H1 * W1 * c2。<br />
传统卷积计算方式如下：<br />
<img alt="image" src="../img/ch17/3.png" />
传统卷积运算量为：<br />
$$
A = H * W * h1 * w1 * c1 * c2
$$
DW卷积的计算方式如下：<br />
<img alt="image" src="../img/ch17/4.png" />
DW卷积运算量为： 
$$
B_DW = H * W * h1 * w1 * 1 * c1
$$
PW卷积的计算方式如下：
<img alt="image" src="../img/ch17/5.png" /></p>
<p>$$
B_PW = H_m * W_m * 1 * 1 * c_1 * c_2
$$
Depthwise separable convolution运算量为：
$$
B = B_DW + B_PW
$$
Depthwise separable convolution相对于传统卷积的运算量：
$$
\dfrac{B}{A} = \dfrac{ H * W * h_1 * w_1 * 1 * c_1 + H_m * W_m * 1 * 1 * c_1 * c_2}{H * W * h1 * w1 * c_1 * c_2}  </p>
<p>= \dfrac{1}{c_2} + \dfrac{1}{h_1 * w_1}
$$
由此可知，随着卷积通道数的增加，Depthwise separable convolution的运算量相对于传统卷积更少。</p>
<h3 id="1783-channelmac">17.8.3 输入输出的channel相同时，MAC最小<a class="headerlink" href="#1783-channelmac" title="Permanent link">&para;</a></h3>
<p><strong>卷积层的输入和输出特征通道数相等时MAC最小，此时模型速度最快。</strong><br />
假设feature map的大小为h*w，输入通道<span><span class="MathJax_Preview">c_1</span><script type="math/tex">c_1</script></span>，输出通道<span><span class="MathJax_Preview">c_2</span><script type="math/tex">c_2</script></span>。<br />
已知：
$$
FLOPs = B = h * w * c1 * c2 <br />
=&gt; c1 * c2 = \dfrac{B}{h * w} <br />
$$</p>
<div>
<div class="MathJax_Preview">
MAC = h * w * (c1 + c2) + c1 * c2  
</div>
<script type="math/tex; mode=display">
MAC = h * w * (c1 + c2) + c1 * c2  
</script>
</div>
<div>
<div class="MathJax_Preview">
=&gt; MAC \geq 2 * h * w \sqrt{\dfrac{B}{h * w}} + \dfrac{B}{h * w}
</div>
<script type="math/tex; mode=display">
=> MAC \geq 2 * h * w \sqrt{\dfrac{B}{h * w}} + \dfrac{B}{h * w}
</script>
</div>
<p>根据均值不等式得到(c1-c2)^2&gt;=0，等式成立的条件是c1=c2，也就是输入特征通道数和输出特征通道数相等时，在给定FLOPs前提下，MAC达到取值的下界。</p>
<h3 id="1784">17.8.4 减少组卷积的数量<a class="headerlink" href="#1784" title="Permanent link">&para;</a></h3>
<p><strong>过多的group操作会增大MAC，从而使模型速度变慢</strong><br />
由以上公式可知，group卷积想比与传统的卷积可以降低计算量，提高模型的效率；如果在相同的FLOPs时，group卷积为了满足FLOPs会是使用更多channels，可以提高模型的精度。但是随着channel数量的增加，也会增加MAC。<br />
FLOPs：
$$
B = \dfrac{h * w * c1 * c2}{g}
$$
MAC：
$$
MAC = h * w * (c1 + c2) + \dfrac{c1 * c2}{g}
$$
由MAC，FLOPs可知：
$$
MAC = h * w * c1 + \dfrac{B*g}{c1} + \dfrac{B}{h * w}
$$
当FLOPs固定(B不变)时，g越大，MAC越大。</p>
<h3 id="1785">17.8.5 减少网络碎片化程度(分支数量)<a class="headerlink" href="#1785" title="Permanent link">&para;</a></h3>
<p><strong>模型中分支数量越少，模型速度越快</strong><br />
此结论主要是由实验结果所得。<br />
以下为网络分支数和各分支包含的卷积数目对神经网络速度的影响。<br />
<img alt="image" src="../img/ch17/6.png" />
实验中使用的基本网络结构，分别将它们重复10次，然后进行实验。实验结果如下：
<img alt="image" src="../img/ch17/7.png" />
由实验结果可知，随着网络分支数量的增加，神经网络的速度在降低。网络碎片化程度对GPU的影响效果明显，对CPU不明显，但是网络速度同样在降低。</p>
<h3 id="1787">17.8.7 减少元素级操作<a class="headerlink" href="#1787" title="Permanent link">&para;</a></h3>
<p><strong>元素级操作所带来的时间消耗也不能忽视</strong><br />
ReLU ，Tensor 相加，Bias相加的操作，分离卷积（depthwise convolution）都定义为元素级操作。<br />
FLOPs大多数是对于卷积计算而言的，因为元素级操作的FLOPs相对要低很多。但是过的元素级操作也会带来时间成本。ShuffleNet作者对ShuffleNet v1和MobileNet v2的几种层操作的时间消耗做了分析，发现元素级操作对于网络速度的影响也很大。
<img alt="image" src="../img/ch17/8.png" /></p>
<h2 id="179">17.9 常用的轻量级网络有哪些？<a class="headerlink" href="#179" title="Permanent link">&para;</a></h2>
<h3 id="1791-sequeezenet">17.9.1 SequeezeNet<a class="headerlink" href="#1791-sequeezenet" title="Permanent link">&para;</a></h3>
<p>SqueenzeNet出自F. N. Iandola, S.Han等人发表的论文《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5MB model size》，作者在保证精度不损失的同时，将原始AlexNet压缩至原来的510倍。  </p>
<h4 id="11">1.1 设计思想<a class="headerlink" href="#11" title="Permanent link">&para;</a></h4>
<p>在网络结构设计方面主要采取以下三种方式：
<em> 用1*1卷积核替换3*3卷积
    * 理论上一个1*1卷积核的参数是一个3*3卷积核的1/9，可以将模型尺寸压缩9倍。
</em> 减小3*3卷积的输入通道数
    * 根据上述公式，减少输入通道数不仅可以减少卷积的运算量，而且输入通道数与输出通道数相同时还可以减少MAC。
* 延迟降采样
    * 分辨率越大的输入能够提供更多特征的信息，有利于网络的训练判断，延迟降采样可以提高网络精度。</p>
<h4 id="12">1.2 网络架构<a class="headerlink" href="#12" title="Permanent link">&para;</a></h4>
<p>SqueezeNet提出一种多分支结构——fire model，其中是由Squeeze层和expand层构成。Squeeze层是由s1个1*1卷积组成，主要是通过1*1的卷积降低expand层的输入维度；expand层利用e1个1*1和e3个3*3卷积构成多分支结构提取输入特征，以此提高网络的精度(其中e1=e3=4*s1)。
<img alt="image" src="../img/ch17/9.png" />
SqueezeNet整体网络结构如下图所示：
<img alt="image" src="../img/ch17/10.png" /></p>
<h4 id="13">1.3实验结果<a class="headerlink" href="#13" title="Permanent link">&para;</a></h4>
<p>不同压缩方法在ImageNet上的对比实验结果
<img alt="image" src="../img/ch17/11.png" />
由实验结果可知，SqueezeNet不仅保证了精度，而且将原始AlexNet从240M压缩至4.8M，压缩50倍，说明此轻量级网络设计是可行。</p>
<h3 id="1792-mobilenet">17.9.2 MobileNet<a class="headerlink" href="#1792-mobilenet" title="Permanent link">&para;</a></h3>
<p>MobileNet 是Google团队于CVPR-2017的论文《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》中针对手机等嵌入式设备提出的一种轻量级的深层神经网络，该网络结构在VGG的基础上使用DW+PW的组合，在保证不损失太大精度的同时，降低模型参数量。</p>
<h4 id="21">2.1 设计思想<a class="headerlink" href="#21" title="Permanent link">&para;</a></h4>
<ul>
<li>采用深度可分离卷积代替传统卷积<ul>
<li>采用DW卷积在减少参数数量的同时提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。</li>
<li>采用PW卷积实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。</li>
</ul>
</li>
<li>使用stride=2的卷积替换pooling<ul>
<li>直接在卷积时利用stride=2完成了下采样，从而节省了需要再去用pooling再去进行一次下采样的时间，可以提升运算速度。同时，因为pooling之前需要一个stride=1的 conv，而与stride=2 conv的计算量想比要高近4倍(<strong>个人理解</strong>)。</li>
</ul>
</li>
</ul>
<h4 id="22">2.2 网络架构<a class="headerlink" href="#22" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>DW conv和PW conv
MobileNet的网络架构主要是由DW conv和PW conv组成，相比于传统卷积可以降低<code>$\dfrac{1}{N} + \dfrac{1}{Dk}$</code>倍的计算量。<br />
标准卷积与DW conv和PW conv如图所示:<br />
<img alt="image" src="../img/ch17/12.png" />
深度可分离卷积与传统卷积运算量对比：
<img alt="image" src="../img/ch17/13.png" />
网络结构：
<img alt="image" src="../img/ch17/14.png" /></p>
</li>
<li>
<p>MobileNets的架构
<img alt="image" src="../img/ch17/15.png" /></p>
</li>
</ul>
<h4 id="23">2.3 实验结果<a class="headerlink" href="#23" title="Permanent link">&para;</a></h4>
<p><img alt="image" src="../img/ch17/16.png" />
由上表可知，使用相同的结构，深度可分离卷积虽然准确率降低1%，但是参数量减少了6/7。</p>
<h3 id="1793-mobilenet-v2">17.9.3 MobileNet-v2<a class="headerlink" href="#1793-mobilenet-v2" title="Permanent link">&para;</a></h3>
<p>MobileNet-V2是2018年1月公开在arXiv上论文《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》，是对MobileNet-V1的改进，同样是一个轻量化卷积神经网络。</p>
<h4 id="31">3.1 设计思想<a class="headerlink" href="#31" title="Permanent link">&para;</a></h4>
<ul>
<li>采用Inverted residuals<ul>
<li>为了保证网络可以提取更多的特征，在residual block中第一个1*1 Conv和3*3 DW Conv之前进行通道扩充</li>
</ul>
</li>
<li>Linear bottlenecks<ul>
<li>为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu</li>
</ul>
</li>
<li>stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut</li>
</ul>
<h4 id="32">3.2 网络架构<a class="headerlink" href="#32" title="Permanent link">&para;</a></h4>
<ul>
<li>Inverted residuals<br />
ResNet中Residuals block先经过1*1的Conv layer，把feature map的通道数降下来，再经过3*3 Conv layer，最后经过一个1*1 的Conv layer，将feature map 通道数再“扩张”回去。即采用先压缩，后扩张的方式。而 inverted residuals采用先扩张，后压缩的方式。<br />
MobileNet采用DW conv提取特征，由于DW conv本身提取的特征数就少，再经过传统residuals block进行“压缩”，此时提取的特征数会更少，因此inverted residuals对其进行“扩张”，保证网络可以提取更多的特征。
<img alt="image" src="../img/ch17/17.png" /></li>
<li>Linear bottlenecks<br />
ReLu激活函数会破坏特征。ReLu对于负的输入，输出全为0，而本来DW conv特征通道已经被“压缩”，再经过ReLu的话，又会损失一部分特征。采用Linear，目的是防止Relu破坏特征。
<img alt="image" src="../img/ch17/18.png" /></li>
<li>shortcut<br />
stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut
<img alt="image" src="../img/ch17/19.png" /></li>
<li>网络架构<br />
<img alt="image" src="../img/ch17/20.png" /></li>
</ul>
<h3 id="1794-xception">17.9.4 Xception<a class="headerlink" href="#1794-xception" title="Permanent link">&para;</a></h3>
<p>Xception是Google提出的，arXiv 的V1 于2016年10月公开《Xception: Deep Learning with Depthwise Separable Convolutions 》，Xception是对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。</p>
<h4 id="41">4.1设计思想<a class="headerlink" href="#41" title="Permanent link">&para;</a></h4>
<ul>
<li>采用depthwise separable convolution来替换原来Inception v3中的卷积操作<br />
    与原版的Depth-wise convolution有两个不同之处：<ul>
<li>第一个：原版Depth-wise convolution，先逐通道卷积，再1*1卷积; 而Xception是反过来，先1*1卷积，再逐通道卷积；</li>
<li>第二个：原版Depth-wise convolution的两个卷积之间是不带激活函数的，而Xception在经过1*1卷积之后会带上一个Relu的非线性激活函数；</li>
</ul>
</li>
</ul>
<h4 id="42">4.2网络架构<a class="headerlink" href="#42" title="Permanent link">&para;</a></h4>
<p>feature map在空间和通道上具有一定的相关性，通过Inception模块和非线性激活函数实现通道之间的解耦。增多3*3的卷积的分支的数量，使它与1*1的卷积的输出通道数相等，此时每个3*3的卷积只作用与一个通道的特征图上，作者称之为“极致的Inception（Extream Inception）”模块，这就是Xception的基本模块。
<img alt="image" src="../img/ch17/21.png" /></p>
<h3 id="1795-shufflenet-v1">17.9.5 ShuffleNet-v1<a class="headerlink" href="#1795-shufflenet-v1" title="Permanent link">&para;</a></h3>
<p>ShuffleNet 是Face++团队提出的，晚于MobileNet两个月在arXiv上公开《ShuffleNet： An Extremely Efficient Convolutional Neural Network for Mobile Devices 》用于移动端前向部署的网络架构。ShuffleNet基于MobileNet的group思想，将卷积操作限制到特定的输入通道。而与之不同的是，ShuffleNet将输入的group进行打散，从而保证每个卷积核的感受野能够分散到不同group的输入中，增加了模型的学习能力。</p>
<h4 id="51">5.1 设计思想<a class="headerlink" href="#51" title="Permanent link">&para;</a></h4>
<ul>
<li>采用group conv减少大量参数<ul>
<li>roup conv与DW conv存在相同的“信息流通不畅”问题 </li>
</ul>
</li>
<li>采用channel shuffle解决上述问题<ul>
<li>MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle</li>
</ul>
</li>
<li>采用concat替换add操作<ul>
<li>avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失</li>
</ul>
</li>
</ul>
<h4 id="52">5.2 网络架构<a class="headerlink" href="#52" title="Permanent link">&para;</a></h4>
<p>MobileNet中1*1卷积的操作占据了约95%的计算量，所以作者将1*1也更改为group卷积，使得相比MobileNet的计算量大大减少。
<img alt="image" src="../img/ch17/22.png" />
group卷积与DW存在同样使“通道信息交流不畅”的问题，MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle。<br />
ShuffleNet的shuffle操作如图所示
<img alt="image" src="../img/ch17/24.png" />
avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失；实验表明：多多使用通道(提升通道的使用率)，有助于提高小模型的准确率。
<img alt="image" src="../img/ch17/23.png" />
网络结构：<br />
<img alt="image" src="../img/ch17/25.png" /></p>
<h3 id="1796-shufflenet-v2">17.9.6 ShuffleNet-v2<a class="headerlink" href="#1796-shufflenet-v2" title="Permanent link">&para;</a></h3>
<p>huffleNet-v2 是Face++团队提出的《ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design》，旨在设计一个轻量级但是保证精度、速度的深度网络。</p>
<h4 id="61">6.1 设计思想<a class="headerlink" href="#61" title="Permanent link">&para;</a></h4>
<ul>
<li>文中提出影响神经网络速度的4个因素：<ul>
<li>a. FLOPs(FLOPs就是网络执行了多少multiply-adds操作)</li>
<li>b. MAC(内存访问成本)</li>
<li>c. 并行度(如果网络并行度高，速度明显提升)</li>
<li>d. 计算平台(GPU，ARM)</li>
</ul>
</li>
<li>ShuffleNet-v2 提出了4点网络结构设计策略：<ul>
<li>G1.输入输出的channel相同时，MAC最小</li>
<li>G2.过度的组卷积会增加MAC</li>
<li>G3.网络碎片化会降低并行度</li>
<li>G4.元素级运算不可忽视  </li>
</ul>
</li>
</ul>
<h4 id="62">6.2 网络结构<a class="headerlink" href="#62" title="Permanent link">&para;</a></h4>
<p>depthwise convolution 和 瓶颈结构增加了 MAC，用了太多的 group，跨层连接中的 element-wise Add 操作也是可以优化的点。所以在 shuffleNet V2 中增加了几种新特性。<br />
所谓的 channel split 其实就是将通道数一分为2，化成两分支来代替原先的分组卷积结构（G2），并且每个分支中的卷积层都是保持输入输出通道数相同（G1），其中一个分支不采取任何操作减少基本单元数（G3），最后使用了 concat 代替原来的 elementy-wise add，并且后面不加 ReLU 直接（G4），再加入channle shuffle 来增加通道之间的信息交流。 对于下采样层，在这一层中对通道数进行翻倍。 在网络结构的最后，即平均值池化层前加入一层 1x1 的卷积层来进一步的混合特征。
<img alt="image" src="../img/ch17/26.png" />
网络结构<br />
<img alt="image" src="../img/ch17/27.png" /></p>
<h4 id="64-shufflenet-v2">6.4  ShuffleNet-v2具有高精度的原因<a class="headerlink" href="#64-shufflenet-v2" title="Permanent link">&para;</a></h4>
<ul>
<li>由于高效，可以增加更多的channel，增加网络容量</li>
<li>采用split使得一部分特征直接与下面的block相连，特征复用(DenseNet)</li>
</ul>
<h2 id="1710">17.10 现有移动端开源框架及其特点<a class="headerlink" href="#1710" title="Permanent link">&para;</a></h2>
<h3 id="17101-ncnn">17.10.1 NCNN<a class="headerlink" href="#17101-ncnn" title="Permanent link">&para;</a></h3>
<p>１、开源时间：2017年7月　　　</p>
<p>２、开源用户：腾讯优图　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/ncnn">https://github.com/Tencent/ncnn</a> 　　</p>
<p>4、特点：</p>
<ul>
<li>1）NCNN考虑了手机端的硬件和系统差异以及调用方式，架构设计以手机端运行为主要原则。</li>
<li>2）无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架（以开源时间为参照对象）。</li>
<li>3）基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP。   </li>
</ul>
<p>5、功能：    </p>
<ul>
<li>1、NCNN支持卷积神经网络、多分支多输入的复杂网络结构，如vgg、googlenet、resnet、squeezenet 等。</li>
<li>2、NCNN无需依赖任何第三方库。    </li>
<li>3、NCNN全部使用C/C++实现，以及跨平台的cmake编译系统，可轻松移植到其他系统和设备上。    </li>
<li>4、汇编级优化，计算速度极快。使用ARM NEON指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。 </li>
<li>5、精细的数据结构设计，没有采用需消耗大量内存的通常框架——im2col + 矩阵乘法，使得内存占用极低。   </li>
<li>6、支持多核并行计算，优化CPU调度。   </li>
<li>7、整体库体积小于500K，可精简到小于300K。   </li>
<li>8、可扩展的模型设计，支持8bit 量化和半精度浮点存储。   </li>
<li>9、支持直接内存引用加载网络模型。   </li>
<li>10、可注册自定义层实现并扩展。   </li>
</ul>
<p>6、NCNN在Android端部署示例</p>
<ul>
<li>1）选择合适的Android Studio版本并安装。</li>
<li>2）根据需求选择NDK版本并安装。</li>
<li>3）在Android Studio上配置NDK的环境变量。</li>
<li>4）根据自己需要编译NCNN sdk</li>
</ul>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>mkdir build-android cd build-android cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \ -DANDROID_ABI=&quot;armeabi-v7a&quot; -DANDROID_ARM_NEON=ON \ -DANDROID_PLATFORM=android-14 .. make make install
</pre></div>
</td></tr></table>

<p>​   安装完成之后，install下有include和lib两个文件夹。</p>
<p>​   备注：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>ANDROID_ABI 是架构名字，&quot;armeabi-v7a&quot; 支持绝大部分手机硬件 
ANDROID_ARM_NEON 是否使用 NEON 指令集，设为 ON 支持绝大部分手机硬件 
ANDROID_PLATFORM 指定最低系统版本，&quot;android-14&quot; 就是 android-4.0
</pre></div>
</td></tr></table>

<ul>
<li>5）进行NDK开发。</li>
</ul>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>1）assets文件夹下放置你的bin和param文件。
2）jni文件夹下放置你的cpp和mk文件。
3）修改你的app gradle文件。
4）配置Android.mk和Application.mk文件。
5）进行java接口的编写。
6）读取拷贝bin和param文件（有些则是pb文件，根据实际情况）。
7）进行模型的初始化和执行预测等操作。
8）build。
9）cd到src/main/jni目录下，执行ndk-build，生成.so文件。
10）接着就可写自己的操作处理需求。
</pre></div>
</td></tr></table>

<h3 id="17102-qnnpack">17.10.2 QNNPACK<a class="headerlink" href="#17102-qnnpack" title="Permanent link">&para;</a></h3>
<p>全称：Quantized Neural Network PACKage（量化神经网络包）　　　</p>
<p>１、开源时间：2018年10月　　　</p>
<p>２、开源用户：Facebook　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/pytorch/QNNPACK">https://github.com/pytorch/QNNPACK</a>　　　　</p>
<p>４、特点：　　　</p>
<p>​   １）低密度卷积优化函数库；　　　</p>
<p>２）可在手机上实时运行Mask R-CNN 和 DensePose;</p>
<p>​   ３） 能在性能受限的移动设备中用 100ms 以内的时间实施图像分类；　　　</p>
<p>5、QNNPACK 如何提高效率？</p>
<p>1)<strong>QNNPACK 使用与安卓神经网络 API 兼容的线性量化方案</strong></p>
<p>QNNPACK 的输入矩阵来自低精度、移动专用的计算机视觉模型。其它库在计算A和B矩阵相乘时，重新打包 A 和 B 矩阵以更好地利用缓存层次结构，希望在大量计算中分摊打包开销，QNNPACK 删除所有计算非必需的内存转换，针对 A和B矩阵相乘适用于一级缓存的情况进行了优化。</p>
<p><img alt="" src="../img/ch17/QNNPACK1.jpeg" /></p>
<p>​   1）优化了L1缓存计算，不需要输出中间结果，直接输出最终结果，节省内存带宽和缓存占用。</p>
<p>具体分析：</p>
<ul>
<li>常规实现：在量化矩阵-矩阵乘法中，8位整数的乘积通常会被累加至 32 位的中间结果中，随后重新量化以产生 8 位的输出。遇到大矩阵尺寸时，比如有时K太大，A和B的面板无法直接转入缓存，此时，需利用缓存层次结构，借助GEMM将A和B的面板沿着K维分割成固定大小的子面板，以便于每个子面板都能适应L1缓存，随后为每个子面板调用微内核。这一缓存优化需要 PDOT 为内核输出 32  位中间结果，最终将它们相加并重新量化为 8 位整数。</li>
<li>优化实现：由于  ONNPACK 对于面板 A 和 B 总是适应 L1 缓存的移动神经网络进行了优化，因此它在调用微内核时处理整个 A 和 B  的面板。而由于无需在微内核之外积累 32 位的中间结果，QNNPACK 会将 32 位的中间结果整合进微内核中并写出 8  位值，这节省了内存带宽和缓存占用。</li>
</ul>
<p><img alt="" src="../img/ch17/QNNPACK2.jpeg" /></p>
<p>​   2）取消了矩阵 A 的重新打包。</p>
<ul>
<li>
<p>常规实现：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>矩阵 B 包含静态权重，可以一次性转换成任何内存布局，但矩阵  A 包含卷积输入，每次推理运行都会改变。因此，重新打包矩阵 A 在每次运行时都会产生开销。尽管存在开销，传统的 GEMM实现还是出于以下两个原因对矩阵 A 进行重新打包：

a 缓存关联性及微内核效率受限。如果不重新打包，微内核将不得不读取被潜在的大跨距隔开的几行A。如果这个跨距恰好是 2 的许多次幂的倍数，面板中不同行 A  的元素可能会落入同一缓存集中。如果冲突的行数超过了缓存关联性，它们就会相互驱逐，性能也会大幅下降。

b 打包对微内核效率的影响与当前所有移动处理器支持的  SIMD  向量指令的使用密切相关。这些指令加载、存储或者计算小型的固定大小元素向量，而不是单个标量（scalar）。在矩阵相乘中，充分利用向量指令达到高性能很重要。在传统的  GEMM 实现中，微内核把 MR 元素重新打包到向量暂存器里的 MR 线路中。
</pre></div>
</td></tr></table>

</li>
<li>
<p>优化实现：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>a 当面板适配一级缓存时，不会存在缓存关联性及微内核效率受限的问题。

b 在 QNNPACK 实现中，MR  元素在存储中不是连续的，微内核需要把它们加载到不同的向量暂存器中。越来越大的暂存器压力迫使 QNNPACK 使用较小的 MRxNR  拼贴，但实际上这种差异很小，而且可以通过消除打包开销来补偿。例如，在 32 位 ARM 架构上，QNNPACK 使用 4×8 微内核，其中  57% 的向量指令是乘-加；另一方面，gemmlowp 库使用效率稍高的 4×12 微内核，其中 60% 的向量指令是乘-加。微内核加载 A  的多个行，乘以 B 的满列，结果相加，然后完成再量化并记下量化和。A 和 B 的元素被量化为 8 位整数，但乘积结果相加到 32 位。大部分  ARM 和 ARM64 处理器没有直接完成这一运算的指令，所以它必须分解为多个支持运算。QNNPACK  提供微内核的两个版本，其不同之处在于用于乘以 8 位值并将它们累加到 32 位的指令序列。
</pre></div>
</td></tr></table>

</li>
</ul>
<p>2)<strong>从矩阵相乘到卷积</strong></p>
<p><img alt="" src="../img/ch17/QNNPACK3.jpeg" /></p>
<p>​   传统实现：</p>
<p>​   简单的 1×1  卷积可直接映射到矩阵相乘</p>
<p>​   但对于具备较大卷积核、padding 或子采样（步幅）的卷积而言则并非如此。但是，这些较复杂的卷积能够通过记忆变换  im2col 映射到矩阵相乘。对于每个输出像素，im2col 复制输入图像的图像块并将其计算为 2D 矩阵。由于每个输出像素都受 KHxKWxC  输入像素值的影响（KH 和 KW 分别指卷积核的高度和宽度，C 指输入图像中的通道数），因此该矩阵的大小是输入图像的 KHxKW  倍，im2col 给内存占用和性能都带来了一定的开销。和 Caffe 一样，大部分深度学习框架转而使用基于 im2col  的实现，利用现有的高度优化矩阵相乘库来执行卷积操作。</p>
<p>​   优化实现：</p>
<p>​   Facebook  研究者在 QNNPACK 中实现了一种更高效的算法。</p>
<ul>
<li>他们没有变换卷积输入使其适应矩阵相乘的实现，而是调整 PDOT 微内核的实现，在运行中执行  im2col 变换。这样就无需将输入张量的实际输入复制到 im2col 缓存，而是使用输入像素行的指针设置 indirection  buffer，输入像素与每个输出像素的计算有关。</li>
<li>研究者还修改了矩阵相乘微内核，以便从 indirection buffer  加载虚构矩阵（imaginary matrix）A 的行指针，indirection buffer 通常比 im2col buffer  小得多。</li>
<li>此外，如果两次推断运行的输入张量存储位置不变，则 indirection buffer  还可使用输入张量行的指针进行初始化，然后在多次推断运行中重新使用。研究者观察到具备 indirection buffer 的微内核不仅消除了  im2col 变换的开销，其性能也比矩阵相乘微内核略好（可能由于输入行在计算不同输出像素时被重用）。</li>
</ul>
<p>3)<strong>深度卷积</strong></p>
<p><img alt="" src="../img/ch17/QNNPACK4.jpeg" /></p>
<p>分组卷积（grouped   convolution）将输入和输出通道分割成多组，然后对每个组进行分别处理。在有限条件下，当组数等于通道数时，该卷积就是深度卷积，常用于当前的神经网络架构中。深度卷积对每个通道分别执行空间滤波，展示了与正常卷积非常不同的计算模式。因此，通常要向深度卷积提供单独实现，QNNPACK  包括一个高度优化版本 3×3 深度卷积。</p>
<p>深度卷积的传统实现是每次都在卷积核元素上迭代，然后将一个卷积核行和一个输入行的结果累加到输出行。对于一个  3×3 的深度卷积，此类实现将把每个输出行更新 9 次。在 QNNPACK 中，研究者计算所有 3×3 卷积核行和 3×3  输入行的结果，一次性累加到输出行，然后再处理下个输出行。</p>
<p>QNNPACK  实现高性能的关键因素在于完美利用通用暂存器（GPR）来展开卷积核元素上的循环，同时避免在 hot loop 中重新加载地址寄存器。32-bit  ARM 架构将实现限制在 14 个 GPR。在 3×3 深度卷积中，需要读取 9 个输入行和 9 个卷积核行。这意味着如果想完全展开循环必须存储  18 个地址。然而，实践中推断时卷积核不会发生变化。因此 Facebook 研究者使用之前在 CxKHxKW 中的滤波器，将它们封装进  [C/8]xKWxKHx8，这样就可以仅使用具备地址增量（address increment）的一个 GPR 访问所有滤波器。（研究者使用数字 8  的原因在于，在一个命令中加载 8 个元素然后减去零，在 128-bit NEON 暂存器中生成 8 个 16-bit 值。）然后使用 9  个输入行指针，指针将滤波器重新装进 10 个 GPR，完全展开滤波器元素上的循环。64-bit ARM 架构相比 32-bit 架构，GPR  的数量翻了一倍。QNNPACK 利用额外的 ARM64 GPR，一次性存储 3×5 输入行的指针，并计算 3 个输出行。</p>
<p>7、性能优势：</p>
<p>​   测试结果显示出 QNNPACK 在端到端基准上的性能优势。在量化当前最优 MobileNetV2 架构上，基于QNNPACK 的 Caffe2 算子的速度大约是 TensorFlow Lite 速度的 2 倍，在多种手机上都是如此。除了 QNNPACK 之外，Facebook 还开源了 Caffe2 quantized MobileNet v2 模型，其 top-1 准确率比相应的 TensorFlow 模型高出 1.3%。    </p>
<p><strong>MobileNetV1</strong></p>
<p>MobileNetV1  架构在使用深度卷积（depthwise convolution）使模型更适合移动设备方面具备开创性。MobileNetV1 包括几乎整个  1×1 卷积和 3×3 卷积。Facebook 研究者将量化 MobileNetV1 模型从 TensorFlow Lite 转换而来，并在  TensorFlow Lite 和 QNNPACK 的 32-bit ARM 设备上对 MobileNetV1 进行基准测试。二者运行时均使用 4  线程，研究者观察到 QNNPACK 的运行速度几何平均值是 TensorFlow Lite 的 1.8 倍。</p>
<p><img alt="" src="../img/ch17/mv1.jpg" /></p>
<p><strong>MobileNetV2</strong></p>
<p>作为移动视觉任务的当前最优架构之一，MobileNetV2  引入了瓶颈构造块和瓶颈之间的捷径连接。研究者在 MobileNetV2 分类模型的量化版上对比基于 QNNPACK 的 Caffe2 算子和  TensorFlow Lite 实现。使用的量化 Caffe2 MobileNetV2 模型已开源，量化 TensorFlow Lite  模型来自官方库：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md</a>。下表展示了二者在常用测试集上的  top1 准确率：</p>
<p><img alt="" src="../img/ch17/mv2.jpg" /></p>
<p>​   Facebook 研究者利用这些模型建立了 Facebook AI 性能评估平台（<a href="https://github.com/facebook/FAI-PEP">https://github.com/facebook/FAI-PEP</a>）的基准，该基准基于 32-bit ARM 环境的大量手机设备。对于 TensorFlow Lite 线程设置，研究者尝试了一到四个线程，并报告了最快速的结果。结果显示 TensorFlow Lite 使用四线程的性能最优，因此后续研究中使用四线程来对比 TensorFlow Lite 和 QNNPACK。下表展示了结果，以及在典型智能手机和高端机上，基于 QNNPACK 的算子速度比 TensorFlow Lite 快得多。</p>
<p><img alt="" src="../img/ch17/mv3.jpg" /></p>
<p>Facebook开源高性能内核库QNNPACK
<a href="https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc</a>
<a href="http://www.sohu.com/a/272158070_610300">http://www.sohu.com/a/272158070_610300</a></p>
<p>支持移动端深度学习的几种开源框架
<a href="https://blog.csdn.net/zchang81/article/details/74280019">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="17103-prestissimo">17.10.3 Prestissimo<a class="headerlink" href="#17103-prestissimo" title="Permanent link">&para;</a></h3>
<p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：九言科技　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/in66-dev/In-Prestissimo">https://github.com/in66-dev/In-Prestissimo</a>　　</p>
<p>４、功能特点：　</p>
<p><strong>基础功能</strong></p>
<ul>
<li>支持卷积神经网络，支持多输入和多分支结构</li>
<li>精炼简洁的API设计，使用方便</li>
<li>提供调试接口，支持打印各个层的数据以及耗时</li>
<li>不依赖任何第三方计算框架，整体库体积 500K 左右（32位 约400k，64位 约600k）</li>
<li>纯 C++ 实现，跨平台，支持 android 和 ios</li>
<li>模型为纯二进制文件，不暴露开发者设计的网络结构</li>
</ul>
<p><strong>极快的速度</strong></p>
<ul>
<li>大到框架设计，小到汇编书写上全方位的优化，iphone7 上跑 SqueezeNet 仅需 26ms（单线程）</li>
<li>支持浮点(float)和整型(int)两种运算模式，float模式精度与caffe相同，int模式运算速度快，大部分网络用int的精度便已经足够</li>
<li>以巧妙的内存布局提升cpu的cache命中率，在中低端机型上性能依然强劲</li>
<li>针对 float-arm32, float-arm64, int-arm32, int-arm64 四个分支均做了细致的优化，保证arm32位和arm64位版本都有非常好的性能</li>
</ul>
<p><strong>SqueezeNet-v1.1 测试结果</strong></p>
<p><strong>Note</strong>: 手机测试性能存在一定的抖动，连续多次运算取平均时间</p>
<p><strong>Note</strong>: 像华为mate8, mate9，Google nexus 6 虽然是64位的CPU，但测试用的是 32位的库，因此cpu架构依然写 arm-v7a</p>
<table>
<thead>
<tr>
<th align="center">CPU架构</th>
<th align="center">机型</th>
<th align="center">CPU</th>
<th align="center">ncnn（4线程）</th>
<th align="center">mdl</th>
<th align="center">Prestissimo_float(单线程)</th>
<th align="center">Prestissimo_int(单线程)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">arm-v7a</td>
<td align="center">小米2</td>
<td align="center">高通APQ8064 1.5GHz</td>
<td align="center">185 ms</td>
<td align="center">370 ms</td>
<td align="center">184 ms</td>
<td align="center">115 ms</td>
</tr>
<tr>
<td align="center">arm-v7a</td>
<td align="center">小米2s</td>
<td align="center">四核 骁龙APQ8064 Pro 1.7GHz</td>
<td align="center">166 ms</td>
<td align="center">-</td>
<td align="center">136 ms</td>
<td align="center">96 ms</td>
</tr>
<tr>
<td align="center">arm-v7a</td>
<td align="center">红米Note 4x</td>
<td align="center">骁龙625 四核2.0GHz</td>
<td align="center">124 ms</td>
<td align="center">306 ms</td>
<td align="center">202 ms</td>
<td align="center">110 ms</td>
</tr>
<tr>
<td align="center">arm-v7a</td>
<td align="center">Google Nexus 6</td>
<td align="center">骁龙805 四核 2.7GHz</td>
<td align="center">84 ms</td>
<td align="center">245 ms</td>
<td align="center">103 ms</td>
<td align="center">63 ms</td>
</tr>
<tr>
<td align="center">arm-v7a</td>
<td align="center">Vivo x6d</td>
<td align="center">联发科 MT6752 1.7GHz</td>
<td align="center">245 ms</td>
<td align="center">502 ms</td>
<td align="center">370 ms</td>
<td align="center">186 ms</td>
</tr>
<tr>
<td align="center">arm-v7a</td>
<td align="center">华为 Mate 8</td>
<td align="center">海思麒麟950 4大4小 2.3GHz 1.8GHz</td>
<td align="center">75 ms</td>
<td align="center">180 ms</td>
<td align="center">95 ms</td>
<td align="center">57 ms</td>
</tr>
<tr>
<td align="center">arm-v7a</td>
<td align="center">华为 Mate 9</td>
<td align="center">海思麒麟960 4大4小 2.4GHz 1.8GHz</td>
<td align="center">61 ms</td>
<td align="center">170 ms</td>
<td align="center">94 ms</td>
<td align="center">48 ms</td>
</tr>
<tr>
<td align="center">arm-v8</td>
<td align="center">iphone7</td>
<td align="center">Apple A10 Fusion 2.34GHz</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">27 ms</td>
<td align="center">26 ms</td>
</tr>
</tbody>
</table>
<p><strong>未开放特性</strong></p>
<ul>
<li>多核并行加速（多核机器可以再提升30%-100% 的速度）</li>
<li>depthwise卷积运算（支持mobilenet）</li>
<li>模型压缩功能，压缩后的模型体积可缩小到20%以下</li>
<li>GPU 运算模式（Android 基于opengl es 3.1，ios 基于metal）</li>
</ul>
<p><strong>同类框架对比</strong></p>
<table>
<thead>
<tr>
<th align="center">框架</th>
<th align="center">caffe</th>
<th align="center">tensorflow</th>
<th align="center">mdl-android</th>
<th align="center">mdl-ios</th>
<th align="center">ncnn</th>
<th align="center">CoreML</th>
<th align="center">Prestissimo</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">计算硬件</td>
<td align="center">cpu</td>
<td align="center">cpu</td>
<td align="center">cpu</td>
<td align="center">gpu</td>
<td align="center">cpu</td>
<td align="center">gpu</td>
<td align="center">cpu （gpu版本未开放）</td>
</tr>
<tr>
<td align="center">计算速度</td>
<td align="center">慢</td>
<td align="center">慢</td>
<td align="center">慢</td>
<td align="center">很快</td>
<td align="center">很快</td>
<td align="center">极快</td>
<td align="center">极快</td>
</tr>
<tr>
<td align="center">库大小</td>
<td align="center">大</td>
<td align="center">较大</td>
<td align="center">中等</td>
<td align="center">小</td>
<td align="center">小</td>
<td align="center">小</td>
<td align="center">小</td>
</tr>
<tr>
<td align="center">兼容性</td>
<td align="center">好</td>
<td align="center">好</td>
<td align="center">好</td>
<td align="center">限ios8以上</td>
<td align="center">很好</td>
<td align="center">仅支持 ios11</td>
<td align="center">很好</td>
</tr>
<tr>
<td align="center">模型支持度</td>
<td align="center">很好</td>
<td align="center">好</td>
<td align="center">-</td>
<td align="center">差（仅限指定模型）</td>
<td align="center">较好</td>
<td align="center">-</td>
<td align="center">中等（当前版本不支持mobilenet）</td>
</tr>
</tbody>
</table>
<p><strong>使用方法-模型转换</strong></p>
<p>绝影支持的是私有的模型文件格式，需要把 caffe 训练出来的模型转换为 .prestissimo 格式，模型转换工具为 caffe2Prestissimo.out。caffe2Prestissimo.out 依赖 protobuf 3.30。将 XXX.prototxt 和 YYY.caffemodel 转化为 Prestissimo 模型 ZZZ.prestissimo：（得到）./caffe2Prestissimo.out XXX.prototxt YYY.caffemodel ZZZ.prestissimo</p>
<h3 id="17104-mdlmobile-deep-learning">17.10.4 MDL（mobile-deep-learning）<a class="headerlink" href="#17104-mdlmobile-deep-learning" title="Permanent link">&para;</a></h3>
<p>１、开源时间：2017年9月（已暂停更新）　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/allonli/mobile-deep-learning">https://github.com/allonli/mobile-deep-learning</a></p>
<p>４、功能特点：</p>
<ul>
<li>一键部署，脚本参数就可以切换ios或者android</li>
<li>支持iOS  gpu运行MobileNet、squeezenet模型</li>
<li>已经测试过可以稳定运行MobileNet、GoogLeNet v1、squeezenet、ResNet-50模型</li>
<li>体积极小，无任何第三方依赖。纯手工打造。</li>
<li>提供量化函数，对32位float转8位uint直接支持，模型体积量化后4M上下</li>
<li>与ARM相关算法团队线上线下多次沟通，针对ARM平台会持续优化</li>
<li>NEON使用涵盖了卷积、归一化、池化所有方面的操作</li>
<li>汇编优化，针对寄存器汇编操作具体优化</li>
<li>loop unrolling 循环展开，为提升性能减少不必要的CPU消耗，全部展开判断操作</li>
<li>将大量繁重的计算任务前置到overhead过程</li>
</ul>
<p>5、框架结构</p>
<p><img alt="" src="../img/ch17/MDL1.png" /></p>
<p>MDL 框架主要包括：<strong>模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层（JNI Interfaces）。</strong></p>
<p>​   其中，模型转换模块主要负责将Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。</p>
<p>6、MDL 的性能及兼容性</p>
<ul>
<li>体积 armv7 300k+</li>
<li>速度 iOS GPU mobilenet 可以达到 40ms、squeezenet 可以达到 30ms</li>
</ul>
<p>​        MDL  从立项到开源，已经迭代了一年多。移动端比较关注的多个指标都表现良好，如体积、功耗、速度。百度内部产品线在应用前也进行过多次对比，和已开源的相关项目对比，MDL  能够在保证速度和能耗的同时支持多种深度学习模型，如 mobilenet、googlenet v1、squeezenet 等，且具有 iOS  GPU 版本，squeezenet 一次运行最快可以达到 3-40ms。</p>
<p><strong>同类框架对比</strong></p>
<p>​    框架Caffe2TensorFlowncnnMDL(CPU)MDL(GPU)硬件CPUCPUCPUCPUGPU速度慢慢快快极快体积大大小小小兼容Android&amp;iOSAndroid&amp;iOSAndroid&amp;iOSAndroid&amp;iOSiOS</p>
<p>​    与支持 CNN 的移动端框架对比，MDL 速度快、性能稳定、兼容性好、demo 完备。</p>
<p><strong>兼容性</strong></p>
<p>​    MDL 在 iOS 和 Android 平台均可以稳定运行，其中 iOS10 及以上平台有基于 GPU 运算的 API，性能表现非常出色，在 Android 平台则是纯 CPU 运行。高中低端机型运行状态和手机百度及其他 App 上的覆盖都有绝对优势。</p>
<p>​    MDL 同时也支持 Caffe 模型直接转换为 MDL 模型。</p>
<h3 id="17105-paddle-mobile">17.10.5 Paddle-Mobile<a class="headerlink" href="#17105-paddle-mobile" title="Permanent link">&para;</a></h3>
<p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/PaddlePaddle/paddle-mobile">https://github.com/PaddlePaddle/paddle-mobile</a>　</p>
<p>４、功能特点：</p>
<p><strong>功能特点</strong></p>
<ul>
<li>
<p>高性能支持ARM CPU </p>
</li>
<li>
<p>支持Mali GPU</p>
</li>
<li>
<p>支持Andreno GPU</p>
</li>
<li>
<p>支持苹果设备的GPU Metal实现</p>
</li>
<li>
<p>支持ZU5、ZU9等FPGA开发板</p>
</li>
<li>
<p>支持树莓派等arm-linux开发板</p>
</li>
</ul>
<h3 id="17106-mace-mobile-ai-compute-engine">17.10.6 MACE（ Mobile AI Compute Engine）<a class="headerlink" href="#17106-mace-mobile-ai-compute-engine" title="Permanent link">&para;</a></h3>
<p>１、开源时间：2018年4月(持续更新，v0.9.0 (2018-07-20))　　　</p>
<p>２、开源用户：小米　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/XiaoMi/mace">https://github.com/XiaoMi/mace</a>   </p>
<p>４、简介：Mobile AI Compute Engine (MACE) 是一个专为移动端异构计算设备优化的深度学习前向预测框架。
MACE覆盖了常见的移动端计算设备（CPU，GPU和DSP），并且提供了完整的工具链和文档，用户借助MACE能够很方便地在移动端部署深度学习模型。MACE已经在小米内部广泛使用并且被充分验证具有业界领先的性能和稳定性。</p>
<p>5、MACE的基本框架：</p>
<p><img alt="" src="../img/ch17/mace-arch.png" /></p>
<p><strong>MACE Model</strong></p>
<p>MACE定义了自有的模型格式（类似于Caffe2），通过MACE提供的工具可以将Caffe和TensorFlow的模型 转为MACE模型。</p>
<p><strong>MACE Interpreter</strong></p>
<p>MACE Interpreter主要负责解析运行神经网络图（DAG）并管理网络中的Tensors。</p>
<p><strong>Runtime</strong></p>
<p>CPU/GPU/DSP Runtime对应于各个计算设备的算子实现。</p>
<p>6、MACE使用的基本流程</p>
<p><img alt="" src="../img/ch17/mace-work-flow-zh.png" /></p>
<p><strong>1. 配置模型部署文件(.yml)</strong></p>
<p>模型部署文件详细描述了需要部署的模型以及生成库的信息，MACE根据该文件最终生成对应的库文件。</p>
<p><strong>2.编译MACE库</strong></p>
<p>编译MACE的静态库或者动态库。</p>
<p><strong>3.转换模型</strong></p>
<p>将TensorFlow 或者 Caffe的模型转为MACE的模型。</p>
<p><strong>4.1. 部署</strong></p>
<p>根据不同使用目的集成Build阶段生成的库文件，然后调用MACE相应的接口执行模型。</p>
<p><strong>4.2. 命令行运行</strong></p>
<p>MACE提供了命令行工具，可以在命令行运行模型，可以用来测试模型运行时间，内存占用和正确性。</p>
<p><strong>4.3. Benchmark</strong></p>
<p>MACE提供了命令行benchmark工具，可以细粒度的查看模型中所涉及的所有算子的运行时间。</p>
<p>7、MACE在哪些角度进行了优化?</p>
<p><strong>MACE</strong> 专为移动端异构计算平台优化的神经网络计算框架。主要从以下的角度做了专门的优化：</p>
<ul>
<li>性能</li>
<li>代码经过NEON指令，OpenCL以及Hexagon HVX专门优化，并且采用
    <a href="https://arxiv.org/abs/1509.09308">Winograd算法</a>来进行卷积操作的加速。
    此外，还对启动速度进行了专门的优化。</li>
<li>
<p>功耗</p>
</li>
<li>
<p>支持芯片的功耗管理，例如ARM的big.LITTLE调度，以及高通Adreno GPU功耗选项。</p>
</li>
<li>系统响应</li>
<li>支持自动拆解长时间的OpenCL计算任务，来保证UI渲染任务能够做到较好的抢占调度，
    从而保证系统UI的相应和用户体验。</li>
<li>内存占用</li>
<li>通过运用内存依赖分析技术，以及内存复用，减少内存的占用。另外，保持尽量少的外部
    依赖，保证代码尺寸精简。</li>
<li>
<p>模型加密与保护</p>
</li>
<li>
<p>模型保护是重要设计目标之一。支持将模型转换成C++代码，以及关键常量字符混淆，增加逆向的难度。</p>
</li>
<li>硬件支持范围</li>
<li>支持高通，联发科，以及松果等系列芯片的CPU，GPU与DSP(目前仅支持Hexagon)计算加速。</li>
<li>同时支持在具有POSIX接口的系统的CPU上运行。</li>
</ul>
<p>8、性能对比：</p>
<p>MACE 支持 TensorFlow 和 Caffe 模型，提供转换工具，可以将训练好的模型转换成专有的模型数据文件，同时还可以选择将模型转换成C++代码，支持生成动态库或者静态库，提高模型保密性。</p>
<p><img alt="" src="../img/ch17/maca_com.jpg" /></p>
<h3 id="17107-feathercnn">17.10.7 FeatherCNN<a class="headerlink" href="#17107-feathercnn" title="Permanent link">&para;</a></h3>
<p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：腾讯AI　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/FeatherCNN">https://github.com/Tencent/FeatherCNN</a></p>
<p>４、功能特点：</p>
<p><strong>FeatherCNN 是由腾讯 AI 平台部研发的基于 ARM 架构的高效 CNN 推理库，该项目支持 Caffe 模型，且具有高性能、易部署、轻量级三大特性。</strong></p>
<p><strong>该项目具体特性如下：</strong></p>
<ul>
<li>
<p>高性能：无论是在移动设备（iOS / Android），嵌入式设备（Linux）还是基于 ARM 的服务器（Linux）上，FeatherCNN 均能发挥最先进的推理计算性能；</p>
</li>
<li>
<p>易部署：FeatherCNN 的所有内容都包含在一个代码库中，以消除第三方依赖关系。因此，它便于在移动平台上部署。FeatherCNN 自身的模型格式与 Caffe 模型完全兼容。</p>
</li>
<li>
<p>轻量级：编译后的 FeatherCNN 库的体积仅为数百 KB。</p>
</li>
</ul>
<h3 id="17108-tensorflow-lite">17.10.8 TensorFlow Lite<a class="headerlink" href="#17108-tensorflow-lite" title="Permanent link">&para;</a></h3>
<p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：谷歌　　　</p>
<p>３、GitHub地址：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite</a></p>
<p>４、简介：</p>
<p>Google 表示 Lite 版本 TensorFlow 是 TensorFlow Mobile 的一个延伸版本。此前，通过TensorFlow Mobile API，TensorFlow已经支持手机上的模型嵌入式部署。TensorFlow Lite应该被视为TensorFlow Mobile的升级版。</p>
<p>TensorFlow Lite可以与Android 8.1中发布的神经网络API完美配合，即便在没有硬件加速时也能调用CPU处理，确保模型在不同设备上的运行。 而Android端版本演进的控制权是掌握在谷歌手中的，从长期看，TensorFlow Lite会得到Android系统层面上的支持。</p>
<p>5、架构：</p>
<p><img alt="" src="../img/ch17/tflite_artc.JPEG" /></p>
<p>其组件包括：</p>
<ul>
<li>TensorFlow 模型（TensorFlow Model）：保存在磁盘中的训练模型。</li>
<li>TensorFlow Lite 转化器（TensorFlow Lite Converter）：将模型转换成 TensorFlow Lite 文件格式的项目。</li>
<li>TensorFlow Lite 模型文件（TensorFlow Lite Model File）：基于 FlatBuffers，适配最大速度和最小规模的模型。</li>
</ul>
<p>6、移动端开发步骤：</p>
<p>Android Studio 3.0, SDK Version API26, NDK Version 14</p>
<p>步骤：
1. 将此项目导入到Android Studio：
    <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo</a></p>
<ol>
<li>
<p>下载移动端的模型（model）和标签数据（lables）：
    <a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip">https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip</a></p>
</li>
<li>
<p>下载完成解压mobilenet_v1_224_android_quant_2017_11_08.zip文件得到一个xxx.tflite和labes.txt文件，分别是模型和标签文件，并且把这两个文件复制到assets文件夹下。</p>
</li>
<li>
<p>构建app，run……</p>
</li>
</ol>
<p>17.7.9 TensorFlow Lite和TensorFlow Mobile的区别？</p>
<ul>
<li>TensorFlow Lite是TensorFlow Mobile的进化版。</li>
<li>在大多数情况下，TensorFlow Lite拥有跟小的二进制大小，更少的依赖以及更好的性能。</li>
<li>相比TensorFlow Mobile是对完整TensorFlow的裁减，TensorFlow Lite基本就是重新实现了。从内部实现来说，在TensorFlow内核最基本的OP，Context等数据结构，都是新的。从外在表现来说，模型文件从PB格式改成了FlatBuffers格式，TensorFlow的size有大幅度优化，降至300K，然后提供一个converter将普通TensorFlow模型转化成TensorFlow Lite需要的格式。因此，无论从哪方面看，TensorFlow Lite都是一个新的实现方案。</li>
</ul>
<h3 id="17109-pocketflow">17.10.9 PocketFlow<a class="headerlink" href="#17109-pocketflow" title="Permanent link">&para;</a></h3>
<p>１、开源时间：2018年9月　　　</p>
<p>２、开源用户：腾讯　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/PocketFlow">https://github.com/Tencent/PocketFlow</a></p>
<p>４、简介：</p>
<p>全球首个自动模型压缩框架</p>
<p>一款面向移动端AI开发者的自动模型压缩框架，集成了当前主流的模型压缩与训练算法，结合自研超参数优化组件实现了全程自动化托管式的模型压缩与加速。开发者无需了解具体算法细节，即可快速地将AI技术部署到移动端产品上，实现了自动托管式模型压缩与加速，实现用户数据的本地高效处理。</p>
<p>5、框架介绍</p>
<p>PocketFlow 框架主要由两部分组件构成，分别是模型压缩/加速算法组件和超参数优化组件，具体结构如下图所示。</p>
<p><img alt="" src="../img/ch17/framework_design.png" /></p>
<p>​   开发者将未压缩的原始模型作为 PocketFlow 框架的输入，同时指定期望的性能指标，例如模型的压缩和/或加速倍数；在每一轮迭代过程中，超参数优化组件选取一组超参数取值组合，之后模型压缩/加速算法组件基于该超参数取值组合，对原始模型进行压缩，得到一个压缩后的候选模型；基于对候选模型进行性能评估的结果，超参数优化组件调整自身的模型参数，并选取一组新的超参数取值组合，以开始下一轮迭代过程；当迭代终止时，PocketFlow 选取最优的超参数取值组合以及对应的候选模型，作为最终输出，返回给开发者用作移动端的模型部署。</p>
<p>6、PocketFlow如何实现模型压缩与加速？</p>
<p>​   具体地，PocketFlow 通过下列各个算法组件的有效结合，实现了精度损失更小、自动化程度更高的深度学习模型的压缩与加速：</p>
<ul>
<li>
<p>a) 通道剪枝（channel pruning）组件：在CNN网络中，通过对特征图中的通道维度进行剪枝，可以同时降低模型大小和计算复杂度，并且压缩后的模型可以直接基于现有的深度学习框架进行部署。在CIFAR-10图像分类任务中，通过对  ResNet-56 模型进行通道剪枝，可以实现2.5倍加速下分类精度损失0.4%，3.3倍加速下精度损失0.7%。</p>
</li>
<li>
<p>b) 权重稀疏化（weight sparsification）组件：通过对网络权重引入稀疏性约束，可以大幅度降低网络权重中的非零元素个数；压缩后模型的网络权重可以以稀疏矩阵的形式进行存储和传输，从而实现模型压缩。对于  MobileNet 图像分类模型，在删去50%网络权重后，在 ImageNet 数据集上的 Top-1 分类精度损失仅为0.6%。</p>
</li>
<li>
<p>c) 权重量化（weight quantization）组件：通过对网络权重引入量化约束，可以降低用于表示每个网络权重所需的比特数；团队同时提供了对于均匀和非均匀两大类量化算法的支持，可以充分利用  ARM 和 FPGA 等设备的硬件优化，以提升移动端的计算效率，并为未来的神经网络芯片设计提供软件支持。以用于 ImageNet  图像分类任务的 ResNet-18 模型为例，在8比特定点量化下可以实现精度无损的4倍压缩。</p>
</li>
<li>
<p>d)网络蒸馏（network distillation）组件：对于上述各种模型压缩组件，通过将未压缩的原始模型的输出作为额外的监督信息，指导压缩后模型的训练，在压缩/加速倍数不变的前提下均可以获得0.5%-2.0%不等的精度提升。</p>
</li>
<li>
<p>e) 多GPU训练（multi-GPU training）组件：深度学习模型训练过程对计算资源要求较高，单个GPU难以在短时间内完成模型训练，因此团队提供了对于多机多卡分布式训练的全面支持，以加快使用者的开发流程。无论是基于  ImageNet 数据的Resnet-50图像分类模型还是基于 WMT14 数据的 Transformer  机器翻译模型，均可以在一个小时内训练完毕。[1] </p>
</li>
<li>
<p>f) 超参数优化（hyper-parameter optimization）组件：多数开发者对模型压缩算法往往不甚了解，但超参数取值对最终结果往往有着巨大的影响，因此团队引入了超参数优化组件，采用了包括强化学习等算法以及  AI Lab 自研的 AutoML  自动超参数优化框架来根据具体性能需求，确定最优超参数取值组合。例如，对于通道剪枝算法，超参数优化组件可以自动地根据原始模型中各层的冗余程度，对各层采用不同的剪枝比例，在保证满足模型整体压缩倍数的前提下，实现压缩后模型识别精度的最大化。</p>
</li>
</ul>
<p><img alt="" src="../img/ch17/packflow1.jpg" /></p>
<p>7、PocketFlow 性能</p>
<p>​   通过引入超参数优化组件，不仅避免了高门槛、繁琐的人工调参工作，同时也使得  PocketFlow 在各个压缩算法上全面超过了人工调参的效果。以图像分类任务为例，在 CIFAR-10 和 ImageNet  等数据集上，PocketFlow 对 ResNet 和 MobileNet 等多种 CNN 网络结构进行有效的模型压缩与加速。</p>
<p>​   在  CIFAR-10 数据集上，PocketFlow 以 ResNet-56  作为基准模型进行通道剪枝，并加入了超参数优化和网络蒸馏等训练策略，实现了 2.5 倍加速下分类精度损失 0.4%，3.3 倍加速下精度损失  0.7%，且显著优于未压缩的 ResNet-44 模型； 在 ImageNet 数据集上，PocketFlow 可以对原本已经十分精简的  MobileNet 模型继续进行权重稀疏化，以更小的模型尺寸取得相似的分类精度；与 Inception-V1、ResNet-18  等模型相比，模型大小仅为后者的约 20~40%，但分类精度基本一致（甚至更高）。</p>
<p><img alt="" src="../img/ch17/packflow2.jpg" /></p>
<p><img alt="" src="../img/ch17/packflow3.jpg" /></p>
<p>相比于费时费力的人工调参，PocketFlow 框架中的 AutoML 自动超参数优化组件仅需 10 
余次迭代就能达到与人工调参类似的性能，在经过 100 次迭代后搜索得到的超参数组合可以降低约 0.6% 
的精度损失；通过使用超参数优化组件自动地确定网络中各层权重的量化比特数，PocketFlow 在对用于 ImageNet 图像分类任务的 
ResNet-18 模型进行压缩时，取得了一致性的性能提升；当平均量化比特数为 4 比特时，超参数优化组件的引入可以将分类精度从 63.6% 
提升至 68.1%（原始模型的分类精度为 70.3%）。</p>
<p><img alt="" src="../img/ch17/packflow4.jpg" /></p>
<p><img alt="" src="../img/ch17/packflow5.jpg" /></p>
<p><strong>参考文献</strong></p>
<p>[1]  Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao,  Qingyao Wu, Junzhou Huang, Jinhui Zhu,「Discrimination-aware Channel  Pruning for Deep Neural Networks", In Proc. of the 32<sup>nd</sup> Annual  Conference on Neural Information Processing Systems, NIPS '18, Montreal,  Canada, December 2018.</p>
<p>[2] Jiaxiang  Wu, Weidong Huang, Junzhou Huang, Tong Zhang,「Error Compensated  Quantized SGD and its Applications to Large-scale Distributed  Optimization」, In Proc. of the 35<sup>th</sup> International Conference on Machine  Learning, ICML’18, Stockholm, Sweden, July 2018.</p>
<h3 id="171010">17.10.10 其他几款支持移动端深度学习的开源框架<a class="headerlink" href="#171010" title="Permanent link">&para;</a></h3>
<p><a href="https://blog.csdn.net/zchang81/article/details/74280019">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="171011-mdlncnn-tflite">17.10.11 MDL、NCNN和 TFLite比较<a class="headerlink" href="#171011-mdlncnn-tflite" title="Permanent link">&para;</a></h3>
<p>百度-MDL框架、腾讯-NCNN框架和谷歌TFLite框架比较。</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">MDL</th>
<th align="center">NCNN</th>
<th align="center">TFLite</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">代码质量</td>
<td align="center">中</td>
<td align="center">高</td>
<td align="center">很高</td>
</tr>
<tr>
<td align="center">跨平台</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">支持caffe模型</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">×</td>
</tr>
<tr>
<td align="center">支持TensorFlow模型</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">CPU NEON指令优化</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
</tr>
<tr>
<td align="center">GPU加速</td>
<td align="center">√</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
</tbody>
</table>
<p>相同点：</p>
<ul>
<li>只含推理（inference）功能，使用的模型文件需要通过离线的方式训练得到。</li>
<li>最终生成的库尺寸较小，均小于500kB。</li>
<li>为了提升执行速度，都使用了ARM NEON指令进行加速。</li>
<li>跨平台，iOS和Android系统都支持。</li>
</ul>
<p>不同点：</p>
<ul>
<li>MDL和NCNN均是只支持Caffe框架生成的模型文件，而TfLite则毫无意外的只支持自家大哥TensorFlow框架生成的模型文件。</li>
<li>MDL支持利用iOS系统的Matal框架进行GPU加速，能够显著提升在iPhone上的运行速度，达到准实时的效果。而NCNN和TFLite还没有这个功能。</li>
</ul>
<h2 id="1711">17.11 移动端开源框架部署<a class="headerlink" href="#1711" title="Permanent link">&para;</a></h2>
<h3 id="1781-ncnn">17.8.1 以NCNN为例<a class="headerlink" href="#1781-ncnn" title="Permanent link">&para;</a></h3>
<p>部署步骤   </p>
<h3 id="1782-qnnpack">17.8.2 以QNNPACK为例<a class="headerlink" href="#1782-qnnpack" title="Permanent link">&para;</a></h3>
<p>部署步骤     </p>
<h3 id="1784-androidmace">17.8.4 在Android手机上使用MACE实现图像分类<a class="headerlink" href="#1784-androidmace" title="Permanent link">&para;</a></h3>
<h3 id="1783-androidpaddlemobile">17.8.3 在Android手机上使用PaddleMobile实现图像分类<a class="headerlink" href="#1783-androidpaddlemobile" title="Permanent link">&para;</a></h3>
<p><strong>编译paddle-mobile库</strong></p>
<p>1）编译Android能够使用的CPP库：编译Android的paddle-mobile库，可选择使用Docker编译和Ubuntu交叉编译，这里介绍使用Ubuntu交叉编译paddle-mobile库。</p>
<p><em>注</em>：在Android项目，Java代码调用CPP代码，CPP的函数需要遵循一定的命名规范，比如Java_包名_类名_对应的Java的方法名。</p>
<p>​   目前官方提供了5个可以给Java调用的函数，该代码在：paddle-mobile/src/jni/paddle_mobile_jni.cpp，如果想要让这些函数能够在自己的包名下的类调用，就要修改CPP的函数名称修改如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">JNIEXPORT</span> <span class="n">jboolean</span> <span class="n">JNICALL</span> <span class="nf">Java_com_baidu_paddle_PML_load</span><span class="o">(</span><span class="n">JNIEnv</span> <span class="o">*</span><span class="n">env</span><span class="o">,</span> 
    <span class="n">jclass</span> <span class="n">thiz</span><span class="o">,</span>
    <span class="n">jstring</span> <span class="n">modelPath</span><span class="o">)</span> <span class="o">{</span> 
        <span class="n">ANDROIDLOGI</span><span class="o">(</span><span class="s">&quot;load invoked&quot;</span><span class="o">);</span> 
        <span class="n">bool</span> <span class="n">optimize</span> <span class="o">=</span> <span class="kc">true</span><span class="o">;</span> 
        <span class="k">return</span> <span class="n">getPaddleMobileInstance</span><span class="o">()-&gt;</span><span class="n">Load</span><span class="o">(</span><span class="n">jstring2cppstring</span><span class="o">(</span><span class="n">env</span><span class="o">,</span> <span class="n">modelPath</span><span class="o">),</span> <span class="n">optimize</span><span class="o">);</span> <span class="o">}</span>
</pre></div>
</td></tr></table>

<p>​   笔者项目的包名为<code>com.example.paddlemobile1</code>，在这个包下有一个<code>ImageRecognition.java</code>的程序来对应这个CPP程序，那么修改<code>load</code>函数如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">JNIEXPORT</span> <span class="n">jboolean</span> <span class="n">JNICALL</span> <span class="nf">Java_com_example_paddlemobile1_ImageRecognition_load</span><span class="o">(</span><span class="n">JNIEnv</span> <span class="o">*</span><span class="n">env</span><span class="o">,</span>
                                                          <span class="n">jclass</span> <span class="n">thiz</span><span class="o">,</span>
                                                          <span class="n">jstring</span> <span class="n">modelPath</span><span class="o">)</span> <span class="o">{</span>
  <span class="n">ANDROIDLOGI</span><span class="o">(</span><span class="s">&quot;load invoked&quot;</span><span class="o">);</span>
  <span class="n">bool</span> <span class="n">optimize</span> <span class="o">=</span> <span class="kc">true</span><span class="o">;</span>
  <span class="k">return</span> <span class="n">getPaddleMobileInstance</span><span class="o">()-&gt;</span><span class="n">Load</span><span class="o">(</span><span class="n">jstring2cppstring</span><span class="o">(</span><span class="n">env</span><span class="o">,</span> <span class="n">modelPath</span><span class="o">),</span>
                                         <span class="n">optimize</span><span class="o">);</span>
<span class="o">}</span>
</pre></div>
</td></tr></table>

<p><strong>使用Ubuntu交叉编译paddle-mobile库</strong></p>
<p>1、下载和解压NDK。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>wget https://dl.google.com/android/repository/android-ndk-r17b-linux-x86_64.zip
unzip android-ndk-r17b-linux-x86_64.zip
</pre></div>
</td></tr></table>

<p>2、设置NDK环境变量，目录是NDK的解压目录。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>export NDK_ROOT=&quot;/home/test/paddlepaddle/android-ndk-r17b&quot;
</pre></div>
</td></tr></table>

<p>设置好之后，可以使用以下的命令查看配置情况。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>root@test:/home/test/paddlepaddle# echo $NDK_ROOT
/home/test/paddlepaddle/android-ndk-r17b
</pre></div>
</td></tr></table>

<p>3、安装cmake，需要安装较高版本的，笔者的cmake版本是3.11.2。</p>
<p>下载cmake源码</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>wget https://cmake.org/files/v3.11/cmake-3.11.2.tar.gz
</pre></div>
</td></tr></table>

<p>解压cmake源码</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>tar -zxvf cmake-3.11.2.tar.gz
</pre></div>
</td></tr></table>

<p>进入到cmake源码根目录，并执行bootstrap。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>cd cmake-3.11.2
./bootstrap
</pre></div>
</td></tr></table>

<p>最后执行以下两条命令开始安装cmake。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>make
make install
</pre></div>
</td></tr></table>

<p>安装完成之后，可以使用cmake --version是否安装成功.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>root@test:/home/test/paddlepaddle# cmake --version
cmake version 3.11.2

CMake suite maintained and supported by Kitware (kitware.com/cmake).
</pre></div>
</td></tr></table>

<p>4、克隆paddle-mobile源码。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>git clone https://github.com/PaddlePaddle/paddle-mobile.git
</pre></div>
</td></tr></table>

<p>5、进入到paddle-mobile的tools目录下，执行编译。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>cd paddle-mobile/tools/
sh build.sh android
</pre></div>
</td></tr></table>

<p>（可选）如果想编译针对某一个网络编译更小的库时，可以在命令后面加上相应的参数，如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>sh build.sh android googlenet
</pre></div>
</td></tr></table>

<p>6、最后会在paddle-mobile/build/release/arm-v7a/build目录下生产paddle-mobile库。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">root</span><span class="nd">@test</span><span class="o">:/</span><span class="n">home</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">paddlepaddle</span><span class="o">/</span><span class="n">paddle</span><span class="o">-</span><span class="n">mobile</span><span class="o">/</span><span class="n">build</span><span class="o">/</span><span class="n">release</span><span class="o">/</span><span class="n">arm</span><span class="o">-</span><span class="n">v7a</span><span class="o">/</span><span class="n">build</span><span class="err">#</span> <span class="n">ls</span>
<span class="n">libpaddle</span><span class="o">-</span><span class="n">mobile</span><span class="o">.</span><span class="na">so</span>
</pre></div>
</td></tr></table>

<p>libpaddle-mobile.so就是我们在开发Android项目的时候使用到的paddle-mobile库。</p>
<p><strong>创建Android项目</strong></p>
<p>1、首先使用Android Studio创建一个普通的Android项目，包名为<code>com.example.paddlemobile1</code></p>
<p>2、在main目录下创建l两个assets/paddle_models文件夹，这个文件夹存放PaddleFluid训练好的预测模型。PaddleMobile支持量化模型，使用模型量化可以把模型缩小至原来的四分之一，如果使用量化模型，那加载模型的接口也有修改一下，使用以下的接口加载模型：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">boolean</span> <span class="nf">loadQualified</span><span class="o">(</span><span class="n">String</span> <span class="n">modelDir</span><span class="o">);</span>
</pre></div>
</td></tr></table>

<p>3、在<code>main</code>目录下创建一个<code>jniLibs</code>文件夹，这个文件夹是存放CPP编译库的，在本项目中就存放上一部分编译的<code>libpaddle-mobile.so</code></p>
<p>4、在Android项目的配置文件夹中加上权限声明，因为我们要使用到读取相册和使用相机，所以加上以下的权限声明：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&lt;</span><span class="n">uses</span><span class="o">-</span><span class="n">permission</span> <span class="n">android</span><span class="o">:</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;android.permission.CAMERA&quot;</span> <span class="o">/&gt;</span>
<span class="o">&lt;</span><span class="n">uses</span><span class="o">-</span><span class="n">permission</span> <span class="n">android</span><span class="o">:</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;android.permission.WRITE_EXTERNAL_STORAGE&quot;</span> <span class="o">/&gt;</span>
<span class="o">&lt;</span><span class="n">uses</span><span class="o">-</span><span class="n">permission</span> <span class="n">android</span><span class="o">:</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;android.permission.READ_EXTERNAL_STORAGE&quot;</span> <span class="o">/&gt;</span>
</pre></div>
</td></tr></table>

<p>5、修改<code>activity_main.xml</code>界面，修改成如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="o">&lt;?</span><span class="n">xml</span> <span class="n">version</span><span class="o">=</span><span class="s">&quot;1.0&quot;</span> <span class="n">encoding</span><span class="o">=</span><span class="s">&quot;utf-8&quot;</span><span class="o">?&gt;</span>
<span class="o">&lt;</span><span class="n">RelativeLayout</span> <span class="n">xmlns</span><span class="o">:</span><span class="n">android</span><span class="o">=</span><span class="s">&quot;http://schemas.android.com/apk/res/android&quot;</span>
    <span class="n">xmlns</span><span class="o">:</span><span class="n">app</span><span class="o">=</span><span class="s">&quot;http://schemas.android.com/apk/res-auto&quot;</span>
    <span class="n">xmlns</span><span class="o">:</span><span class="n">tools</span><span class="o">=</span><span class="s">&quot;http://schemas.android.com/tools&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_width</span><span class="o">=</span><span class="s">&quot;match_parent&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_height</span><span class="o">=</span><span class="s">&quot;match_parent&quot;</span>
    <span class="n">tools</span><span class="o">:</span><span class="n">context</span><span class="o">=</span><span class="s">&quot;.MainActivity&quot;</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">LinearLayout</span>
    <span class="n">android</span><span class="o">:</span><span class="n">id</span><span class="o">=</span><span class="s">&quot;@+id/btn_ll&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_alignParentBottom</span><span class="o">=</span><span class="s">&quot;true&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_width</span><span class="o">=</span><span class="s">&quot;match_parent&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_height</span><span class="o">=</span><span class="s">&quot;wrap_content&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">orientation</span><span class="o">=</span><span class="s">&quot;horizontal&quot;</span><span class="o">&gt;</span>

    <span class="o">&lt;</span><span class="n">Button</span>
        <span class="n">android</span><span class="o">:</span><span class="n">id</span><span class="o">=</span><span class="s">&quot;@+id/use_photo&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">layout_weight</span><span class="o">=</span><span class="s">&quot;1&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">layout_width</span><span class="o">=</span><span class="s">&quot;0dp&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">layout_height</span><span class="o">=</span><span class="s">&quot;wrap_content&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">text</span><span class="o">=</span><span class="s">&quot;相册&quot;</span> <span class="o">/&gt;</span>

    <span class="o">&lt;</span><span class="n">Button</span>
        <span class="n">android</span><span class="o">:</span><span class="n">id</span><span class="o">=</span><span class="s">&quot;@+id/start_camera&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">layout_weight</span><span class="o">=</span><span class="s">&quot;1&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">layout_width</span><span class="o">=</span><span class="s">&quot;0dp&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">layout_height</span><span class="o">=</span><span class="s">&quot;wrap_content&quot;</span>
        <span class="n">android</span><span class="o">:</span><span class="n">text</span><span class="o">=</span><span class="s">&quot;拍照&quot;</span> <span class="o">/&gt;</span>
<span class="o">&lt;/</span><span class="n">LinearLayout</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">TextView</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_above</span><span class="o">=</span><span class="s">&quot;@id/btn_ll&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">id</span><span class="o">=</span><span class="s">&quot;@+id/result_text&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">textSize</span><span class="o">=</span><span class="s">&quot;16sp&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_width</span><span class="o">=</span><span class="s">&quot;match_parent&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">hint</span><span class="o">=</span><span class="s">&quot;预测结果会在这里显示&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_height</span><span class="o">=</span><span class="s">&quot;100dp&quot;</span> <span class="o">/&gt;</span>

<span class="o">&lt;</span><span class="n">ImageView</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_alignParentTop</span><span class="o">=</span><span class="s">&quot;true&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_above</span><span class="o">=</span><span class="s">&quot;@id/result_text&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">id</span><span class="o">=</span><span class="s">&quot;@+id/show_image&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_width</span><span class="o">=</span><span class="s">&quot;match_parent&quot;</span>
    <span class="n">android</span><span class="o">:</span><span class="n">layout_height</span><span class="o">=</span><span class="s">&quot;match_parent&quot;</span> <span class="o">/&gt;</span>
<span class="o">&lt;/</span><span class="n">RelativeLayout</span><span class="o">&gt;</span>
</pre></div>
</td></tr></table>

<p>6、创建一个<code>ImageRecognition.java</code>的Java程序，这个程序的作用就是调用<code>paddle-mobile/src/jni/paddle_mobile_jni.cpp</code>的函数，对应的是里面的函数。目前支持一下几个接口。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">package</span> <span class="nn">com.example.paddlemobile1</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">ImageRecognition</span> <span class="o">{</span>
    <span class="c1">// set thread num</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">void</span> <span class="nf">setThread</span><span class="o">(</span><span class="kt">int</span> <span class="n">threadCount</span><span class="o">);</span>

<span class="c1">//Load seperated parameters</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">boolean</span> <span class="nf">load</span><span class="o">(</span><span class="n">String</span> <span class="n">modelDir</span><span class="o">);</span>

<span class="c1">// load qualified model</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">boolean</span> <span class="nf">loadQualified</span><span class="o">(</span><span class="n">String</span> <span class="n">modelDir</span><span class="o">);</span>

<span class="c1">// Load combined parameters</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">boolean</span> <span class="nf">loadCombined</span><span class="o">(</span><span class="n">String</span> <span class="n">modelPath</span><span class="o">,</span> <span class="n">String</span> <span class="n">paramPath</span><span class="o">);</span>

<span class="c1">// load qualified model</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">boolean</span> <span class="nf">loadCombinedQualified</span><span class="o">(</span><span class="n">String</span> <span class="n">modelPath</span><span class="o">,</span> <span class="n">String</span> <span class="n">paramPath</span><span class="o">);</span>

<span class="c1">// object detection</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">float</span><span class="o">[]</span> <span class="nf">predictImage</span><span class="o">(</span><span class="kt">float</span><span class="o">[]</span> <span class="n">buf</span><span class="o">,</span> <span class="kt">int</span><span class="o">[]</span><span class="n">ddims</span><span class="o">);</span>

<span class="c1">// predict yuv image</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">float</span><span class="o">[]</span> <span class="nf">predictYuv</span><span class="o">(</span><span class="kt">byte</span><span class="o">[]</span> <span class="n">buf</span><span class="o">,</span> <span class="kt">int</span> <span class="n">imgWidth</span><span class="o">,</span> <span class="kt">int</span> <span class="n">imgHeight</span><span class="o">,</span> <span class="kt">int</span><span class="o">[]</span> <span class="n">ddims</span><span class="o">,</span> <span class="kt">float</span><span class="o">[]</span><span class="n">meanValues</span><span class="o">);</span>

<span class="c1">// clear model</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">native</span> <span class="kt">void</span> <span class="nf">clear</span><span class="o">();</span>
<span class="o">}</span>
</pre></div>
</td></tr></table>

<p>7、然后编写一个<code>PhotoUtil.java</code>的工具类。</p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">package</span> <span class="nn">com.example.paddlemobile1</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">android.app.Activity</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.content.Context</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.content.Intent</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.database.Cursor</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.graphics.Bitmap</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.graphics.BitmapFactory</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.net.Uri</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.os.Build</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.provider.MediaStore</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.support.v4.content.FileProvider</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.util.Log</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.io.File</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.io.IOException</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.Arrays</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">PhotoUtil</span> <span class="o">{</span>
<span class="c1">// start camera</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="n">Uri</span> <span class="nf">start_camera</span><span class="o">(</span><span class="n">Activity</span> <span class="n">activity</span><span class="o">,</span> <span class="kt">int</span> <span class="n">requestCode</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">Uri</span> <span class="n">imageUri</span><span class="o">;</span>
    <span class="c1">// save image in cache path</span>
    <span class="n">File</span> <span class="n">outputImage</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="n">activity</span><span class="o">.</span><span class="na">getExternalCacheDir</span><span class="o">(),</span> <span class="s">&quot;out_image.jpg&quot;</span><span class="o">);</span>
    <span class="k">try</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">outputImage</span><span class="o">.</span><span class="na">exists</span><span class="o">())</span> <span class="o">{</span>
            <span class="n">outputImage</span><span class="o">.</span><span class="na">delete</span><span class="o">();</span>
        <span class="o">}</span>
        <span class="n">outputImage</span><span class="o">.</span><span class="na">createNewFile</span><span class="o">();</span>
    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
    <span class="o">}</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">Build</span><span class="o">.</span><span class="na">VERSION</span><span class="o">.</span><span class="na">SDK_INT</span> <span class="o">&gt;=</span> <span class="mi">24</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// compatible with Android 7.0 or over</span>
        <span class="n">imageUri</span> <span class="o">=</span> <span class="n">FileProvider</span><span class="o">.</span><span class="na">getUriForFile</span><span class="o">(</span><span class="n">activity</span><span class="o">,</span>
                <span class="s">&quot;com.example.paddlemobile1&quot;</span><span class="o">,</span> <span class="n">outputImage</span><span class="o">);</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="n">imageUri</span> <span class="o">=</span> <span class="n">Uri</span><span class="o">.</span><span class="na">fromFile</span><span class="o">(</span><span class="n">outputImage</span><span class="o">);</span>
    <span class="o">}</span>
    <span class="c1">// set system camera Action</span>
    <span class="n">Intent</span> <span class="n">intent</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Intent</span><span class="o">(</span><span class="n">MediaStore</span><span class="o">.</span><span class="na">ACTION_IMAGE_CAPTURE</span><span class="o">);</span>
    <span class="c1">// set save photo path</span>
    <span class="n">intent</span><span class="o">.</span><span class="na">putExtra</span><span class="o">(</span><span class="n">MediaStore</span><span class="o">.</span><span class="na">EXTRA_OUTPUT</span><span class="o">,</span> <span class="n">imageUri</span><span class="o">);</span>
    <span class="c1">// set photo quality, min is 0, max is 1</span>
    <span class="n">intent</span><span class="o">.</span><span class="na">putExtra</span><span class="o">(</span><span class="n">MediaStore</span><span class="o">.</span><span class="na">EXTRA_VIDEO_QUALITY</span><span class="o">,</span> <span class="mi">0</span><span class="o">);</span>
    <span class="n">activity</span><span class="o">.</span><span class="na">startActivityForResult</span><span class="o">(</span><span class="n">intent</span><span class="o">,</span> <span class="n">requestCode</span><span class="o">);</span>
    <span class="k">return</span> <span class="n">imageUri</span><span class="o">;</span>
<span class="o">}</span>

<span class="c1">// get picture in photo</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">use_photo</span><span class="o">(</span><span class="n">Activity</span> <span class="n">activity</span><span class="o">,</span> <span class="kt">int</span> <span class="n">requestCode</span><span class="o">){</span>
    <span class="n">Intent</span> <span class="n">intent</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Intent</span><span class="o">(</span><span class="n">Intent</span><span class="o">.</span><span class="na">ACTION_PICK</span><span class="o">);</span>
    <span class="n">intent</span><span class="o">.</span><span class="na">setType</span><span class="o">(</span><span class="s">&quot;image/*&quot;</span><span class="o">);</span>
    <span class="n">activity</span><span class="o">.</span><span class="na">startActivityForResult</span><span class="o">(</span><span class="n">intent</span><span class="o">,</span> <span class="n">requestCode</span><span class="o">);</span>
<span class="o">}</span>

<span class="c1">// get photo from Uri</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="n">String</span> <span class="nf">get_path_from_URI</span><span class="o">(</span><span class="n">Context</span> <span class="n">context</span><span class="o">,</span> <span class="n">Uri</span> <span class="n">uri</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">String</span> <span class="n">result</span><span class="o">;</span>
    <span class="n">Cursor</span> <span class="n">cursor</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getContentResolver</span><span class="o">().</span><span class="na">query</span><span class="o">(</span><span class="n">uri</span><span class="o">,</span> <span class="kc">null</span><span class="o">,</span> <span class="kc">null</span><span class="o">,</span> <span class="kc">null</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">cursor</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">uri</span><span class="o">.</span><span class="na">getPath</span><span class="o">();</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="n">cursor</span><span class="o">.</span><span class="na">moveToFirst</span><span class="o">();</span>
        <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">cursor</span><span class="o">.</span><span class="na">getColumnIndex</span><span class="o">(</span><span class="n">MediaStore</span><span class="o">.</span><span class="na">Images</span><span class="o">.</span><span class="na">ImageColumns</span><span class="o">.</span><span class="na">DATA</span><span class="o">);</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">cursor</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="n">idx</span><span class="o">);</span>
        <span class="n">cursor</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="n">result</span><span class="o">;</span>
<span class="o">}</span>

<span class="c1">// Compress the image to the size of the training image，and change RGB</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kt">float</span><span class="o">[]</span> <span class="nf">getScaledMatrix</span><span class="o">(</span><span class="n">Bitmap</span> <span class="n">bitmap</span><span class="o">,</span> <span class="kt">int</span> <span class="n">desWidth</span><span class="o">,</span>
                               <span class="kt">int</span> <span class="n">desHeight</span><span class="o">)</span> <span class="o">{</span>
    <span class="kt">float</span><span class="o">[]</span> <span class="n">dataBuf</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">float</span><span class="o">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">desWidth</span> <span class="o">*</span> <span class="n">desHeight</span><span class="o">];</span>
    <span class="kt">int</span> <span class="n">rIndex</span><span class="o">;</span>
    <span class="kt">int</span> <span class="n">gIndex</span><span class="o">;</span>
    <span class="kt">int</span> <span class="n">bIndex</span><span class="o">;</span>
    <span class="kt">int</span><span class="o">[]</span> <span class="n">pixels</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="o">[</span><span class="n">desWidth</span> <span class="o">*</span> <span class="n">desHeight</span><span class="o">];</span>
    <span class="n">Bitmap</span> <span class="n">bm</span> <span class="o">=</span> <span class="n">Bitmap</span><span class="o">.</span><span class="na">createScaledBitmap</span><span class="o">(</span><span class="n">bitmap</span><span class="o">,</span> <span class="n">desWidth</span><span class="o">,</span> <span class="n">desHeight</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>
    <span class="n">bm</span><span class="o">.</span><span class="na">getPixels</span><span class="o">(</span><span class="n">pixels</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="n">desWidth</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="n">desWidth</span><span class="o">,</span> <span class="n">desHeight</span><span class="o">);</span>
    <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
    <span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
    <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">pixels</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
        <span class="kt">int</span> <span class="n">clr</span> <span class="o">=</span> <span class="n">pixels</span><span class="o">[</span><span class="n">i</span><span class="o">];</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="n">desHeight</span><span class="o">;</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">desWidth</span><span class="o">;</span>
        <span class="n">rIndex</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">desWidth</span> <span class="o">+</span> <span class="n">k</span><span class="o">;</span>
        <span class="n">gIndex</span> <span class="o">=</span> <span class="n">rIndex</span> <span class="o">+</span> <span class="n">desHeight</span> <span class="o">*</span> <span class="n">desWidth</span><span class="o">;</span>
        <span class="n">bIndex</span> <span class="o">=</span> <span class="n">gIndex</span> <span class="o">+</span> <span class="n">desHeight</span> <span class="o">*</span> <span class="n">desWidth</span><span class="o">;</span>
        <span class="n">dataBuf</span><span class="o">[</span><span class="n">rIndex</span><span class="o">]</span> <span class="o">=</span> <span class="o">(</span><span class="kt">float</span><span class="o">)</span> <span class="o">((</span><span class="n">clr</span> <span class="o">&amp;</span> <span class="mh">0x00ff0000</span><span class="o">)</span> <span class="o">&gt;&gt;</span> <span class="mi">16</span><span class="o">)</span> <span class="o">-</span> <span class="mi">148</span><span class="o">;</span>
        <span class="n">dataBuf</span><span class="o">[</span><span class="n">gIndex</span><span class="o">]</span> <span class="o">=</span> <span class="o">(</span><span class="kt">float</span><span class="o">)</span> <span class="o">((</span><span class="n">clr</span> <span class="o">&amp;</span> <span class="mh">0x0000ff00</span><span class="o">)</span> <span class="o">&gt;&gt;</span> <span class="mi">8</span><span class="o">)</span> <span class="o">-</span> <span class="mi">148</span><span class="o">;</span>
        <span class="n">dataBuf</span><span class="o">[</span><span class="n">bIndex</span><span class="o">]</span> <span class="o">=</span> <span class="o">(</span><span class="kt">float</span><span class="o">)</span> <span class="o">((</span><span class="n">clr</span> <span class="o">&amp;</span> <span class="mh">0x000000ff</span><span class="o">))</span> <span class="o">-</span> <span class="mi">148</span><span class="o">;</span>

    <span class="o">}</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">bm</span><span class="o">.</span><span class="na">isRecycled</span><span class="o">())</span> <span class="o">{</span>
        <span class="n">bm</span><span class="o">.</span><span class="na">recycle</span><span class="o">();</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="n">dataBuf</span><span class="o">;</span>
<span class="o">}</span>

<span class="c1">// compress picture</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="n">Bitmap</span> <span class="nf">getScaleBitmap</span><span class="o">(</span><span class="n">String</span> <span class="n">filePath</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">BitmapFactory</span><span class="o">.</span><span class="na">Options</span> <span class="n">opt</span> <span class="o">=</span> <span class="k">new</span> <span class="n">BitmapFactory</span><span class="o">.</span><span class="na">Options</span><span class="o">();</span>
    <span class="n">opt</span><span class="o">.</span><span class="na">inJustDecodeBounds</span> <span class="o">=</span> <span class="kc">true</span><span class="o">;</span>
    <span class="n">BitmapFactory</span><span class="o">.</span><span class="na">decodeFile</span><span class="o">(</span><span class="n">filePath</span><span class="o">,</span> <span class="n">opt</span><span class="o">);</span>

    <span class="kt">int</span> <span class="n">bmpWidth</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="na">outWidth</span><span class="o">;</span>
    <span class="kt">int</span> <span class="n">bmpHeight</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="na">outHeight</span><span class="o">;</span>

    <span class="kt">int</span> <span class="n">maxSize</span> <span class="o">=</span> <span class="mi">500</span><span class="o">;</span>

    <span class="c1">// compress picture with inSampleSize</span>
    <span class="n">opt</span><span class="o">.</span><span class="na">inSampleSize</span> <span class="o">=</span> <span class="mi">1</span><span class="o">;</span>
    <span class="k">while</span> <span class="o">(</span><span class="kc">true</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">bmpWidth</span> <span class="o">/</span> <span class="n">opt</span><span class="o">.</span><span class="na">inSampleSize</span> <span class="o">&lt;</span> <span class="n">maxSize</span> <span class="o">||</span> <span class="n">bmpHeight</span> <span class="o">/</span> <span class="n">opt</span><span class="o">.</span><span class="na">inSampleSize</span> <span class="o">&lt;</span> <span class="n">maxSize</span><span class="o">)</span> <span class="o">{</span>
            <span class="k">break</span><span class="o">;</span>
        <span class="o">}</span>
        <span class="n">opt</span><span class="o">.</span><span class="na">inSampleSize</span> <span class="o">*=</span> <span class="mi">2</span><span class="o">;</span>
    <span class="o">}</span>
    <span class="n">opt</span><span class="o">.</span><span class="na">inJustDecodeBounds</span> <span class="o">=</span> <span class="kc">false</span><span class="o">;</span>
    <span class="k">return</span> <span class="n">BitmapFactory</span><span class="o">.</span><span class="na">decodeFile</span><span class="o">(</span><span class="n">filePath</span><span class="o">,</span> <span class="n">opt</span><span class="o">);</span>
<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</td></tr></table>
- start_camera()方法是启动相机并返回图片的URI。
- use_photo()方法是打开相册，获取到的图片URI在回到函数中获取。
- get_path_from_URI()方法是把图片的URI转换成绝对路径。
- getScaledMatrix()方法是把图片压缩成跟训练时的大小，并转换成预测需要用的数据格式浮点数组。
- getScaleBitmap()方法是对图片进行等比例压缩，减少内存的支出。</p>
<p>8、最后修改<code>MainActivity.java</code>，修改如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">package</span> <span class="nn">com.example.paddlemobile1</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">android.Manifest</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.annotation.SuppressLint</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.app.Activity</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.content.Context</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.content.Intent</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.content.pm.PackageManager</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.graphics.Bitmap</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.net.Uri</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.os.Bundle</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.os.Environment</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.support.annotation.NonNull</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.support.annotation.Nullable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.support.v4.app.ActivityCompat</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.support.v4.content.ContextCompat</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.support.v7.app.AppCompatActivity</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.util.Log</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.view.View</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.widget.Button</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.widget.ImageView</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.widget.TextView</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">android.widget.Toast</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">com.bumptech.glide.Glide</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">com.bumptech.glide.load.engine.DiskCacheStrategy</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">com.bumptech.glide.request.RequestOptions</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.io.File</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.io.FileOutputStream</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.io.InputStream</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.Arrays</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">MainActivity</span> <span class="kd">extends</span> <span class="n">AppCompatActivity</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">TAG</span> <span class="o">=</span> <span class="n">MainActivity</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">();</span>
    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">USE_PHOTO</span> <span class="o">=</span> <span class="mi">1001</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">START_CAMERA</span> <span class="o">=</span> <span class="mi">1002</span><span class="o">;</span>
    <span class="kd">private</span> <span class="n">Uri</span> <span class="n">image_uri</span><span class="o">;</span>
    <span class="kd">private</span> <span class="n">ImageView</span> <span class="n">show_image</span><span class="o">;</span>
    <span class="kd">private</span> <span class="n">TextView</span> <span class="n">result_text</span><span class="o">;</span>
    <span class="kd">private</span> <span class="n">String</span> <span class="n">assets_path</span> <span class="o">=</span> <span class="s">&quot;paddle_models&quot;</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">boolean</span> <span class="n">load_result</span> <span class="o">=</span> <span class="kc">false</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kt">int</span><span class="o">[]</span> <span class="n">ddims</span> <span class="o">=</span> <span class="o">{</span><span class="mi">1</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">224</span><span class="o">,</span> <span class="mi">224</span><span class="o">};</span>
<span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">String</span><span class="o">[]</span> <span class="n">PADDLE_MODEL</span> <span class="o">=</span> <span class="o">{</span>
        <span class="s">&quot;lenet&quot;</span><span class="o">,</span>
        <span class="s">&quot;alexnet&quot;</span><span class="o">,</span>
        <span class="s">&quot;vgg16&quot;</span><span class="o">,</span>
        <span class="s">&quot;resnet&quot;</span><span class="o">,</span>
        <span class="s">&quot;googlenet&quot;</span><span class="o">,</span>
        <span class="s">&quot;mobilenet_v1&quot;</span><span class="o">,</span>
        <span class="s">&quot;mobilenet_v2&quot;</span><span class="o">,</span>
        <span class="s">&quot;inception_v1&quot;</span><span class="o">,</span>
        <span class="s">&quot;inception_v2&quot;</span><span class="o">,</span>
        <span class="s">&quot;squeezenet&quot;</span>
<span class="o">};</span>

<span class="c1">// load paddle-mobile api</span>
<span class="kd">static</span> <span class="o">{</span>
    <span class="k">try</span> <span class="o">{</span>
        <span class="n">System</span><span class="o">.</span><span class="na">loadLibrary</span><span class="o">(</span><span class="s">&quot;paddle-mobile&quot;</span><span class="o">);</span>

    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">SecurityException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>

    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">UnsatisfiedLinkError</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>

    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">NullPointerException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>

    <span class="o">}</span>

<span class="o">}</span>
</pre></div>
</td></tr></table>

<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="nd">@Override</span>
<span class="kd">protected</span> <span class="kt">void</span> <span class="nf">onCreate</span><span class="o">(</span><span class="n">Bundle</span> <span class="n">savedInstanceState</span><span class="o">)</span> <span class="o">{</span>
    <span class="kd">super</span><span class="o">.</span><span class="na">onCreate</span><span class="o">(</span><span class="n">savedInstanceState</span><span class="o">);</span>
    <span class="n">setContentView</span><span class="o">(</span><span class="n">R</span><span class="o">.</span><span class="na">layout</span><span class="o">.</span><span class="na">activity_main</span><span class="o">);</span>

    <span class="n">init</span><span class="o">();</span>
<span class="o">}</span>

<span class="c1">// initialize view</span>
<span class="kd">private</span> <span class="kt">void</span> <span class="nf">init</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">request_permissions</span><span class="o">();</span>
    <span class="n">show_image</span> <span class="o">=</span> <span class="o">(</span><span class="n">ImageView</span><span class="o">)</span> <span class="n">findViewById</span><span class="o">(</span><span class="n">R</span><span class="o">.</span><span class="na">id</span><span class="o">.</span><span class="na">show_image</span><span class="o">);</span>
    <span class="n">result_text</span> <span class="o">=</span> <span class="o">(</span><span class="n">TextView</span><span class="o">)</span> <span class="n">findViewById</span><span class="o">(</span><span class="n">R</span><span class="o">.</span><span class="na">id</span><span class="o">.</span><span class="na">result_text</span><span class="o">);</span>
    <span class="n">Button</span> <span class="n">use_photo</span> <span class="o">=</span> <span class="o">(</span><span class="n">Button</span><span class="o">)</span> <span class="n">findViewById</span><span class="o">(</span><span class="n">R</span><span class="o">.</span><span class="na">id</span><span class="o">.</span><span class="na">use_photo</span><span class="o">);</span>
    <span class="n">Button</span> <span class="n">start_photo</span> <span class="o">=</span> <span class="o">(</span><span class="n">Button</span><span class="o">)</span> <span class="n">findViewById</span><span class="o">(</span><span class="n">R</span><span class="o">.</span><span class="na">id</span><span class="o">.</span><span class="na">start_camera</span><span class="o">);</span>

    <span class="c1">// use photo click</span>
    <span class="n">use_photo</span><span class="o">.</span><span class="na">setOnClickListener</span><span class="o">(</span><span class="k">new</span> <span class="n">View</span><span class="o">.</span><span class="na">OnClickListener</span><span class="o">()</span> <span class="o">{</span>
        <span class="nd">@Override</span>
        <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onClick</span><span class="o">(</span><span class="n">View</span> <span class="n">view</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">PhotoUtil</span><span class="o">.</span><span class="na">use_photo</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">,</span> <span class="n">USE_PHOTO</span><span class="o">);</span>
            <span class="c1">//                load_model();</span>
            <span class="o">}</span>
        <span class="o">});</span>
</pre></div>
</td></tr></table>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>    <span class="c1">// start camera click</span>
    <span class="n">start_photo</span><span class="o">.</span><span class="na">setOnClickListener</span><span class="o">(</span><span class="k">new</span> <span class="n">View</span><span class="o">.</span><span class="na">OnClickListener</span><span class="o">()</span> <span class="o">{</span>
        <span class="nd">@Override</span>
        <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onClick</span><span class="o">(</span><span class="n">View</span> <span class="n">view</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">image_uri</span> <span class="o">=</span> <span class="n">PhotoUtil</span><span class="o">.</span><span class="na">start_camera</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">,</span> <span class="n">START_CAMERA</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">});</span>

    <span class="c1">// copy file from assets to sdcard</span>
    <span class="n">String</span> <span class="n">sdcard_path</span> <span class="o">=</span> <span class="n">Environment</span><span class="o">.</span><span class="na">getExternalStorageDirectory</span><span class="o">()</span>
            <span class="o">+</span> <span class="n">File</span><span class="o">.</span><span class="na">separator</span> <span class="o">+</span> <span class="n">assets_path</span><span class="o">;</span>
    <span class="n">copy_file_from_asset</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">assets_path</span><span class="o">,</span> <span class="n">sdcard_path</span><span class="o">);</span>

    <span class="c1">// load model</span>
    <span class="n">load_model</span><span class="o">();</span>
<span class="o">}</span>

<span class="c1">// load infer model</span>
<span class="kd">private</span> <span class="kt">void</span> <span class="nf">load_model</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">String</span> <span class="n">model_path</span> <span class="o">=</span> <span class="n">Environment</span><span class="o">.</span><span class="na">getExternalStorageDirectory</span><span class="o">()</span>
            <span class="o">+</span> <span class="n">File</span><span class="o">.</span><span class="na">separator</span> <span class="o">+</span> <span class="n">assets_path</span> <span class="o">+</span> <span class="n">File</span><span class="o">.</span><span class="na">separator</span> <span class="o">+</span> <span class="n">PADDLE_MODEL</span><span class="o">[</span><span class="mi">4</span><span class="o">];</span>
    <span class="n">Log</span><span class="o">.</span><span class="na">d</span><span class="o">(</span><span class="n">TAG</span><span class="o">,</span> <span class="n">model_path</span><span class="o">);</span>
    <span class="n">load_result</span> <span class="o">=</span> <span class="n">ImageRecognition</span><span class="o">.</span><span class="na">load</span><span class="o">(</span><span class="n">model_path</span><span class="o">);</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">load_result</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">Log</span><span class="o">.</span><span class="na">d</span><span class="o">(</span><span class="n">TAG</span><span class="o">,</span> <span class="s">&quot;model load success&quot;</span><span class="o">);</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="n">Log</span><span class="o">.</span><span class="na">d</span><span class="o">(</span><span class="n">TAG</span><span class="o">,</span> <span class="s">&quot;model load fail&quot;</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// clear infer model</span>
<span class="kd">private</span> <span class="kt">void</span> <span class="nf">clear_model</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">ImageRecognition</span><span class="o">.</span><span class="na">clear</span><span class="o">();</span>
    <span class="n">Log</span><span class="o">.</span><span class="na">d</span><span class="o">(</span><span class="n">TAG</span><span class="o">,</span> <span class="s">&quot;model is clear&quot;</span><span class="o">);</span>
<span class="o">}</span>

<span class="c1">// copy file from asset to sdcard</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">copy_file_from_asset</span><span class="o">(</span><span class="n">Context</span> <span class="n">context</span><span class="o">,</span> <span class="n">String</span> <span class="n">oldPath</span><span class="o">,</span> <span class="n">String</span> <span class="n">newPath</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">try</span> <span class="o">{</span>
        <span class="n">String</span><span class="o">[]</span> <span class="n">fileNames</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getAssets</span><span class="o">().</span><span class="na">list</span><span class="o">(</span><span class="n">oldPath</span><span class="o">);</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">fileNames</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
            <span class="c1">// directory</span>
            <span class="n">File</span> <span class="n">file</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="n">newPath</span><span class="o">);</span>
            <span class="k">if</span> <span class="o">(!</span><span class="n">file</span><span class="o">.</span><span class="na">exists</span><span class="o">())</span> <span class="o">{</span>
                <span class="n">file</span><span class="o">.</span><span class="na">mkdirs</span><span class="o">();</span>
            <span class="o">}</span>
            <span class="c1">// copy recursivelyC</span>
            <span class="k">for</span> <span class="o">(</span><span class="n">String</span> <span class="n">fileName</span> <span class="o">:</span> <span class="n">fileNames</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">copy_file_from_asset</span><span class="o">(</span><span class="n">context</span><span class="o">,</span> <span class="n">oldPath</span> <span class="o">+</span> <span class="s">&quot;/&quot;</span> <span class="o">+</span> <span class="n">fileName</span><span class="o">,</span> <span class="n">newPath</span> <span class="o">+</span> <span class="s">&quot;/&quot;</span> <span class="o">+</span> <span class="n">fileName</span><span class="o">);</span>
            <span class="o">}</span>
            <span class="n">Log</span><span class="o">.</span><span class="na">d</span><span class="o">(</span><span class="n">TAG</span><span class="o">,</span> <span class="s">&quot;copy files finish&quot;</span><span class="o">);</span>
        <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
            <span class="c1">// file</span>
            <span class="n">File</span> <span class="n">file</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="n">newPath</span><span class="o">);</span>
            <span class="c1">// if file exists will never copy</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">file</span><span class="o">.</span><span class="na">exists</span><span class="o">())</span> <span class="o">{</span>
                <span class="k">return</span><span class="o">;</span>
            <span class="o">}</span>

            <span class="c1">// copy file to new path</span>
            <span class="n">InputStream</span> <span class="n">is</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getAssets</span><span class="o">().</span><span class="na">open</span><span class="o">(</span><span class="n">oldPath</span><span class="o">);</span>
            <span class="n">FileOutputStream</span> <span class="n">fos</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileOutputStream</span><span class="o">(</span><span class="n">file</span><span class="o">);</span>
            <span class="kt">byte</span><span class="o">[]</span> <span class="n">buffer</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">byte</span><span class="o">[</span><span class="mi">1024</span><span class="o">];</span>
            <span class="kt">int</span> <span class="n">byteCount</span><span class="o">;</span>
            <span class="k">while</span> <span class="o">((</span><span class="n">byteCount</span> <span class="o">=</span> <span class="n">is</span><span class="o">.</span><span class="na">read</span><span class="o">(</span><span class="n">buffer</span><span class="o">))</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">fos</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">buffer</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="n">byteCount</span><span class="o">);</span>
            <span class="o">}</span>
            <span class="n">fos</span><span class="o">.</span><span class="na">flush</span><span class="o">();</span>
            <span class="n">is</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
            <span class="n">fos</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
        <span class="o">}</span>
    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="nd">@Override</span>
<span class="kd">protected</span> <span class="kt">void</span> <span class="nf">onActivityResult</span><span class="o">(</span><span class="kt">int</span> <span class="n">requestCode</span><span class="o">,</span> <span class="kt">int</span> <span class="n">resultCode</span><span class="o">,</span> <span class="nd">@Nullable</span> <span class="n">Intent</span> <span class="n">data</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">String</span> <span class="n">image_path</span><span class="o">;</span>
    <span class="n">RequestOptions</span> <span class="n">options</span> <span class="o">=</span> <span class="k">new</span> <span class="n">RequestOptions</span><span class="o">().</span><span class="na">skipMemoryCache</span><span class="o">(</span><span class="kc">true</span><span class="o">).</span><span class="na">diskCacheStrategy</span><span class="o">(</span><span class="n">DiskCacheStrategy</span><span class="o">.</span><span class="na">NONE</span><span class="o">);</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">resultCode</span> <span class="o">==</span> <span class="n">Activity</span><span class="o">.</span><span class="na">RESULT_OK</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">switch</span> <span class="o">(</span><span class="n">requestCode</span><span class="o">)</span> <span class="o">{</span>
            <span class="k">case</span> <span class="n">USE_PHOTO</span><span class="o">:</span>
                <span class="k">if</span> <span class="o">(</span><span class="n">data</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
                    <span class="n">Log</span><span class="o">.</span><span class="na">w</span><span class="o">(</span><span class="n">TAG</span><span class="o">,</span> <span class="s">&quot;user photo data is null&quot;</span><span class="o">);</span>
                    <span class="k">return</span><span class="o">;</span>
                <span class="o">}</span>
                <span class="n">image_uri</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="na">getData</span><span class="o">();</span>
                <span class="n">Glide</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">).</span><span class="na">load</span><span class="o">(</span><span class="n">image_uri</span><span class="o">).</span><span class="na">apply</span><span class="o">(</span><span class="n">options</span><span class="o">).</span><span class="na">into</span><span class="o">(</span><span class="n">show_image</span><span class="o">);</span>
                <span class="c1">// get image path from uri</span>
                <span class="n">image_path</span> <span class="o">=</span> <span class="n">PhotoUtil</span><span class="o">.</span><span class="na">get_path_from_URI</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">,</span> <span class="n">image_uri</span><span class="o">);</span>
                <span class="c1">// show result</span>
                <span class="n">result_text</span><span class="o">.</span><span class="na">setText</span><span class="o">(</span><span class="n">image_path</span><span class="o">);</span>
                <span class="c1">// predict image</span>
                <span class="n">predict_image</span><span class="o">(</span><span class="n">PhotoUtil</span><span class="o">.</span><span class="na">get_path_from_URI</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">,</span> <span class="n">image_uri</span><span class="o">));</span>
                <span class="k">break</span><span class="o">;</span>
            <span class="k">case</span> <span class="n">START_CAMERA</span><span class="o">:</span>
                <span class="c1">// show photo</span>
                <span class="n">Glide</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">).</span><span class="na">load</span><span class="o">(</span><span class="n">image_uri</span><span class="o">).</span><span class="na">apply</span><span class="o">(</span><span class="n">options</span><span class="o">).</span><span class="na">into</span><span class="o">(</span><span class="n">show_image</span><span class="o">);</span>
                <span class="c1">// get image path from uri</span>
                <span class="n">image_path</span> <span class="o">=</span> <span class="n">PhotoUtil</span><span class="o">.</span><span class="na">get_path_from_URI</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">,</span> <span class="n">image_uri</span><span class="o">);</span>
                <span class="c1">// show result</span>
                <span class="n">result_text</span><span class="o">.</span><span class="na">setText</span><span class="o">(</span><span class="n">image_path</span><span class="o">);</span>
                <span class="c1">// predict image</span>
                <span class="n">predict_image</span><span class="o">(</span><span class="n">PhotoUtil</span><span class="o">.</span><span class="na">get_path_from_URI</span><span class="o">(</span><span class="n">MainActivity</span><span class="o">.</span><span class="na">this</span><span class="o">,</span> <span class="n">image_uri</span><span class="o">));</span>
                <span class="k">break</span><span class="o">;</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="nd">@SuppressLint</span><span class="o">(</span><span class="s">&quot;SetTextI18n&quot;</span><span class="o">)</span>
<span class="kd">private</span> <span class="kt">void</span> <span class="nf">predict_image</span><span class="o">(</span><span class="n">String</span> <span class="n">image_path</span><span class="o">)</span> <span class="o">{</span>
    <span class="c1">// picture to float array</span>
    <span class="n">Bitmap</span> <span class="n">bmp</span> <span class="o">=</span> <span class="n">PhotoUtil</span><span class="o">.</span><span class="na">getScaleBitmap</span><span class="o">(</span><span class="n">image_path</span><span class="o">);</span>
    <span class="kt">float</span><span class="o">[]</span> <span class="n">inputData</span> <span class="o">=</span> <span class="n">PhotoUtil</span><span class="o">.</span><span class="na">getScaledMatrix</span><span class="o">(</span><span class="n">bmp</span><span class="o">,</span> <span class="n">ddims</span><span class="o">[</span><span class="mi">2</span><span class="o">],</span> <span class="n">ddims</span><span class="o">[</span><span class="mi">3</span><span class="o">]);</span>
    <span class="k">try</span> <span class="o">{</span>
        <span class="kt">long</span> <span class="n">start</span> <span class="o">=</span> <span class="n">System</span><span class="o">.</span><span class="na">currentTimeMillis</span><span class="o">();</span>
        <span class="c1">// get predict result</span>
        <span class="kt">float</span><span class="o">[]</span> <span class="n">result</span> <span class="o">=</span> <span class="n">ImageRecognition</span><span class="o">.</span><span class="na">predictImage</span><span class="o">(</span><span class="n">inputData</span><span class="o">,</span> <span class="n">ddims</span><span class="o">);</span>
        <span class="n">Log</span><span class="o">.</span><span class="na">d</span><span class="o">(</span><span class="n">TAG</span><span class="o">,</span> <span class="s">&quot;origin predict result:&quot;</span> <span class="o">+</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">toString</span><span class="o">(</span><span class="n">result</span><span class="o">));</span>
        <span class="kt">long</span> <span class="n">end</span> <span class="o">=</span> <span class="n">System</span><span class="o">.</span><span class="na">currentTimeMillis</span><span class="o">();</span>
        <span class="kt">long</span> <span class="n">time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="o">;</span>
        <span class="n">Log</span><span class="o">.</span><span class="na">d</span><span class="o">(</span><span class="s">&quot;result length&quot;</span><span class="o">,</span> <span class="n">String</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">result</span><span class="o">.</span><span class="na">length</span><span class="o">));</span>
        <span class="c1">// show predict result and time</span>
        <span class="kt">int</span> <span class="n">r</span> <span class="o">=</span> <span class="n">get_max_result</span><span class="o">(</span><span class="n">result</span><span class="o">);</span>
        <span class="n">String</span> <span class="n">show_text</span> <span class="o">=</span> <span class="s">&quot;result：&quot;</span> <span class="o">+</span> <span class="n">r</span> <span class="o">+</span> <span class="s">&quot;\nprobability：&quot;</span> <span class="o">+</span> <span class="n">result</span><span class="o">[</span><span class="n">r</span><span class="o">]</span> <span class="o">+</span> <span class="s">&quot;\ntime：&quot;</span> <span class="o">+</span> <span class="n">time</span> <span class="o">+</span> <span class="s">&quot;ms&quot;</span><span class="o">;</span>
        <span class="n">result_text</span><span class="o">.</span><span class="na">setText</span><span class="o">(</span><span class="n">show_text</span><span class="o">);</span>
    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="kd">private</span> <span class="kt">int</span> <span class="nf">get_max_result</span><span class="o">(</span><span class="kt">float</span><span class="o">[]</span> <span class="n">result</span><span class="o">)</span> <span class="o">{</span>
    <span class="kt">float</span> <span class="n">probability</span> <span class="o">=</span> <span class="n">result</span><span class="o">[</span><span class="mi">0</span><span class="o">];</span>
    <span class="kt">int</span> <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
    <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">result</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">probability</span> <span class="o">&lt;</span> <span class="n">result</span><span class="o">[</span><span class="n">i</span><span class="o">])</span> <span class="o">{</span>
            <span class="n">probability</span> <span class="o">=</span> <span class="n">result</span><span class="o">[</span><span class="n">i</span><span class="o">];</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">i</span><span class="o">;</span>
        <span class="o">}</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="n">r</span><span class="o">;</span>
<span class="o">}</span>

<span class="c1">// request permissions</span>
<span class="kd">private</span> <span class="kt">void</span> <span class="nf">request_permissions</span><span class="o">()</span> <span class="o">{</span>

    <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">permissionList</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">ContextCompat</span><span class="o">.</span><span class="na">checkSelfPermission</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">Manifest</span><span class="o">.</span><span class="na">permission</span><span class="o">.</span><span class="na">CAMERA</span><span class="o">)</span> <span class="o">!=</span> <span class="n">PackageManager</span><span class="o">.</span><span class="na">PERMISSION_GRANTED</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">permissionList</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">Manifest</span><span class="o">.</span><span class="na">permission</span><span class="o">.</span><span class="na">CAMERA</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="k">if</span> <span class="o">(</span><span class="n">ContextCompat</span><span class="o">.</span><span class="na">checkSelfPermission</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">Manifest</span><span class="o">.</span><span class="na">permission</span><span class="o">.</span><span class="na">WRITE_EXTERNAL_STORAGE</span><span class="o">)</span> <span class="o">!=</span> <span class="n">PackageManager</span><span class="o">.</span><span class="na">PERMISSION_GRANTED</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">permissionList</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">Manifest</span><span class="o">.</span><span class="na">permission</span><span class="o">.</span><span class="na">WRITE_EXTERNAL_STORAGE</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="k">if</span> <span class="o">(</span><span class="n">ContextCompat</span><span class="o">.</span><span class="na">checkSelfPermission</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">Manifest</span><span class="o">.</span><span class="na">permission</span><span class="o">.</span><span class="na">READ_EXTERNAL_STORAGE</span><span class="o">)</span> <span class="o">!=</span> <span class="n">PackageManager</span><span class="o">.</span><span class="na">PERMISSION_GRANTED</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">permissionList</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">Manifest</span><span class="o">.</span><span class="na">permission</span><span class="o">.</span><span class="na">READ_EXTERNAL_STORAGE</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="c1">// if list is not empty will request permissions</span>
    <span class="k">if</span> <span class="o">(!</span><span class="n">permissionList</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">())</span> <span class="o">{</span>
        <span class="n">ActivityCompat</span><span class="o">.</span><span class="na">requestPermissions</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">permissionList</span><span class="o">.</span><span class="na">toArray</span><span class="o">(</span><span class="k">new</span> <span class="n">String</span><span class="o">[</span><span class="n">permissionList</span><span class="o">.</span><span class="na">size</span><span class="o">()]),</span> <span class="mi">1</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="nd">@Override</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">onRequestPermissionsResult</span><span class="o">(</span><span class="kt">int</span> <span class="n">requestCode</span><span class="o">,</span> <span class="nd">@NonNull</span> <span class="n">String</span><span class="o">[]</span> <span class="n">permissions</span><span class="o">,</span> <span class="nd">@NonNull</span> <span class="kt">int</span><span class="o">[]</span> <span class="n">grantResults</span><span class="o">)</span> <span class="o">{</span>
    <span class="kd">super</span><span class="o">.</span><span class="na">onRequestPermissionsResult</span><span class="o">(</span><span class="n">requestCode</span><span class="o">,</span> <span class="n">permissions</span><span class="o">,</span> <span class="n">grantResults</span><span class="o">);</span>
    <span class="k">switch</span> <span class="o">(</span><span class="n">requestCode</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">case</span> <span class="mi">1</span><span class="o">:</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">grantResults</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
                <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">grantResults</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>

                    <span class="kt">int</span> <span class="n">grantResult</span> <span class="o">=</span> <span class="n">grantResults</span><span class="o">[</span><span class="n">i</span><span class="o">];</span>
                    <span class="k">if</span> <span class="o">(</span><span class="n">grantResult</span> <span class="o">==</span> <span class="n">PackageManager</span><span class="o">.</span><span class="na">PERMISSION_DENIED</span><span class="o">)</span> <span class="o">{</span>
                        <span class="n">String</span> <span class="n">s</span> <span class="o">=</span> <span class="n">permissions</span><span class="o">[</span><span class="n">i</span><span class="o">];</span>
                        <span class="n">Toast</span><span class="o">.</span><span class="na">makeText</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">s</span> <span class="o">+</span> <span class="s">&quot; permission was denied&quot;</span><span class="o">,</span> <span class="n">Toast</span><span class="o">.</span><span class="na">LENGTH_SHORT</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
                    <span class="o">}</span>
                <span class="o">}</span>
            <span class="o">}</span>
            <span class="k">break</span><span class="o">;</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="nd">@Override</span>
<span class="kd">protected</span> <span class="kt">void</span> <span class="nf">onDestroy</span><span class="o">()</span> <span class="o">{</span>
    <span class="c1">// clear model before destroy app</span>
    <span class="n">clear_model</span><span class="o">();</span>
    <span class="kd">super</span><span class="o">.</span><span class="na">onDestroy</span><span class="o">();</span>
<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</td></tr></table>
- load_model()方法是加载预测模型的。
- clear_model()方法是清空预测模型的。
- copy_file_from_asset()方法是把预测模型复制到内存卡上。
- predict_image()方法是预测图片的。
- get_max_result()方法是获取概率最大的预测结果。
- request_permissions()方法是动态请求权限的。</p>
<p>因为使用到图像加载框架Glide，所以要在<code>build.gradle</code>加入以下的引用。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>implementation &#39;com.github.bumptech.glide:glide:4.3.1&#39;
</pre></div>
</td></tr></table>

<p>8、最后运行项目，选择图片预测就会得到结果。</p>
<h2 id="179_1">17.9 移动端开源框架部署疑难<a class="headerlink" href="#179_1" title="Permanent link">&para;</a></h2>
<p>增加常见的几个问题</p>
<p>知识蒸馏（Distillation）相关论文阅读（1）——Distilling the Knowledge in a Neural Network（以及代码复现）</p>
<h2 id="_2">参考文献<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
                
                  
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://hai5g.cn/aiwiki/qa500/ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署/";
      this.page.identifier =
        "/qa500/ch17_模型压缩、加速及移动端部署/第十七章_模型压缩、加速及移动端部署/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//AI-Wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../ch16_自然语言处理(NLP)/第十六章_NLP/" title="第十六章_NLP" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                第十六章_NLP
              </span>
            </div>
          </a>
        
        
          <a href="../../ch18_后端架构选型、离线及实时计算/第十八章_后端架构选型、离线及实时计算/" title="第十八章_后端架构选型、离线及实时计算" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                第十八章_后端架构选型、离线及实时计算
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 AI Wiki Team
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.39abc4af.js"></script>
      
        
        
          
          <script src="../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
        <script src="https://cdn.jsdelivr.net/gh/ethantw/Han@3.3.0/dist/han.min.js"></script>
      
        <script src="../../../_static/js/extra.js?v=10"></script>
      
        <script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>